---
title: "An intuitive Introduction to R"
author: "Okan Sarioglu"
date: "`r Sys.Date()`"
site: bookdown::bookdown_site
---

# Welcome ! {.unnumbered}

Welcome to this R course! In this course I will teach you the workflow from raw data to your very first analyses in R. When I was a beginner myself, I tried a lot of courses like this, but there were two points, which I think slowed down my learning process, I do want to make different with this course:

-   The courses taught way too much unnecessary stuff for the beginning. It does not matter to know several variations of a command for example, the intuition is way more important.

This course was originally a tutorial for my juniors at my university and I asked myself the question: What do they need to know, to conduct data analyses themselves with R? Well, they have to get an intuition of the workflow. So I decided to design a course, that instead of showing unnecessary variations of functions aim to show how to analyse a question of interest.

-   The courses were not reproducible! That is not the directly the fault of the courses. There are tools to make R scripts reproducible, however they are not beginner friendly, so the authors of those courses are facing a trade-off: Teaching unnecessary complicated stuff at to beginners, or to leave it out and make the course not reproducible.

This course is different! In this course, you can download R-scripts and load them directly on your own device and follow the course by executing the codes for yourself to have the original experience of working with R.

## About this Course {.unnumbered}

### Prerequisities {.unnumbered}

-   `R` and RStudio. You should download the most current version of R and RStudio. How to do that easily is described [here](https://posit.co/download/rstudio-desktop/ "Link to download R").

### What is `R` and `RStudio` ? {.unnumbered}

`R` is a programming language specifically developed for statistical analysis. `RStudio` is the standard graphical interface to work with R.

### Why `R` ? {.unnumbered}

-   Free of cost and open-source
-   Functionalities for all steps of research process from Data Collection to Data Analysis
-   Programming language specifically developed for statistical analysis
-   Very active Community:
-   e.g. `R` community on [StackOverflow](https://stackoverflow.blog/2017/10/10/impressive-growth-r/)
-   e.g. *#rstats* on twitter

### What expects you and what not {.unnumbered}

In this course you will learn:

-   To get familiar with `R` and its basic language
-   Core commands from the tidyverse package
-   Data Wrangling, Data Bridging, Data Munging, Data Manipulation
-   An efficient Workflow
-   A brief introduction into basic Data Analysis and Exploratory Data Analysis.

You will **not** learn:

-   Advanced R usage (Webscraping, Quantitative Text Analysis etc...)

### Overview of the course structure: {.unnumbered}

**1. The R environment**

-   Basic Functionality (Calculations, Vectors, Matrices, Lists)
-   Object classes
-   Accessing, Subsetting and Naming Objects

**2. Data Manipulation**

-   Pipelines or Piping
-   The tidyverse - Dplyr
-   Loading and Storing Data
-   Ordering your Data: Renaming, Re-Ordering, Subsetting and Selecting
-   Transforming Variables
-   Merging Data
-   Missing Values

**3. Exploratory Data Analysis/Descriptives**

-   Standard Descriptive Statistics (Mean, Median, SD,....)
-   Contingency Tables
-   Correlations
-   Working with EDA packages

**4. Data Visualization**

-   The Tidyverse - ggplot 2
-   Constructing Plots
-   Plotting anything

**5. Data Analysis**

-   Linear Regression

-   Model Fit

-   Hypothesis Testing with R

-   Multivariate Regression

-   Categorical Variables

**6. R Programming**

-   For loops
-   Apply function
-   Functions

## About Me {.unnumbered}

My name is Okan and I am a Data Scientist. To be more precise, my profession is Data Science, but originally I graduated in Political Science. I know, what kind of transition is that? From Politics to Data? Well, my university was specialized in empirical research and Political Science was no exception. To explain political phenomena, I analysed large data sets and it was the most fun part of my studies. So I decided to become a Data Scientist and I could not be happier about that decision!

Be part of this Course! Please report errors, bugs and problems with the code. If you have any ideas how to improve this course please contact me on GitHub or via E-Mail. In general, let us stay in touch, follow me on GitHub and LinkedIn and if you like this course share your experience and recommend it to others!

<!--chapter:end:index.Rmd-->

# Fundamentals

In this Chapter you will learn the user interface and basic functions in R. The user interface, as well as the basic functions for mathematical calculations. Further, you will be introduced to one of the most important logical operations: the if else function. The goal of this chapter is to give you an overview of RStudio and to give you a feeling and intuition for R by introducing you to mathematical concepts you already know. Do not forget, nobody is perfect, everyone has problems in the beginning and they can be frustrating, but that is most normal thing, when learning a new programming language. Have fun!

## Getting familiar with RStudio and establishing a Workflow

### The user interface

RStudio has four main components and each of them has a purpose:

![**Figure 1: Standard RStudio Interface**](images/example1.jpeg){width="4750"}

-   **On the top left** you can see the **Source** window. In this window you write and run your code. The console opens after you choose, which type of Script you want to use:

-   The standard **R Script** (`Ctrl + Shift + N`): In this file you can just write your code and run it. You can also make commenting lines with putting a `#` in front of it.

-   The **R Markdown File**: In contrast to the standard **R Script** not everything written in a **R Markdown File** is automatically considered as Code (if not written after an `#`). A **R Markdown File** has options to design an HTML Output and a PDF Output. This can increase your efficiency in terms of working with partners. Further, you can write your code in **chunks** and have plenty options to work with those **chunks**. In QM and AQM you will exclusively use **R Markdown** and over time you will see the advantages of R Markdown.

-   **On the top right** you can see the **Environment**. Here you have an overview over all the objects currently loaded in your environment. You will learn more about objects later in the course.

-   **On the bottom left** you have the **Console**: This is where the results appear once you execute your R-code. You can also directly type R-code into the console and execute it. However, we cannot save this code which is why we usually work in the Editor.

-   **On the bottom right** you can see the **Output**: Plots (if you use the standard R Script) and other things will appear here, don't worry too much about it for the moment.

### How to design it according to your preferences (optional)

-   You can change the appearance of `RStudio`: `Tools > Global Options > Appearance`. Here you can change the zoom of the Console, the font size of the your letters and the style of your code. Further, you can change RStudio to a dark theme. Play around with it and find out how it is the most comfortable for you and of course you can change it over time.

-   You can change the **Pane Layout** meaning where the four components of RStudio should be: `RStudio`: `Tools > Global Options > Pane Layout`.

-   You should use key shortcuts. There are pre-installed short-cuts of RStudio, which are really helpful. You should get familiar with them. `Tools > Key Shortcut Helps` or directly with `Ctrl + Shift + K`. You can add your own Key Shortcuts and we will do that in this course.

## Lets get started: R as a fancy calculator

### Mathmetical Operations in R

You can use R for basic calculations:

```{r calculations}

#You can use hashtags to comment

1 + 1 #Addition

1 - 1 #Substraction

1 * 1 #Multiplication 

1 / 1 #Division

2^(1 / 2) #Mixed Terms

```

You can use R also for TRUE/FALSE statements for logical statements via the comparison operators:

-   `>` greater than
-   `<` smaller than
-   `==` equal
-   `!=` not equal
-   `>=` greater than or equal to
-   `<=` less than or equal to

```{r logical operators}

1 < 3 #TRUE

5 >= 8 #FALSE

11 != 10 #TRUE

22 == 22 #TRUE

7 < 3 #FALSE

5 <= 2+3 #TRUE

```

You can also use logical operators: - `&` element-wise AND operator. It returns TRUE if both elements are true - `|` element-wise OR operator. It returns TRUE if one of the statements is TRUE - `!` Logical NOT operator. It returns FALSE if statement is TRUE

```{r logical statements}

5 & 4 < 8 #TRUE

5 | 4 < 8 #TRUE

!5 > 2 #FALSE

```

### Using Commands

For more advanced operations you can and should use functions. Functios are mostly a word with brackets. Within a function, there are so called arguments. These arguments specify the function and give it the information it needs + optional information.

e.g.

-   `sqrt(x)` taking the square root, `x` is any number.
-   `exp(x)` the constant e, `x` is any number.
-   `mean(x)` for the mean, `x` is any number.
-   `median(x)` for the median, `x` is any number.

```{r functions, eval=FALSE}
sqrt(x = 36) #square root

exp(x = 0) # exponential of 1

print("U can stay under my umbrella") #with this command you can print what you want 
```

I explicitly choose easy examples, but sometimes commands can be complicated, because they demand special inputs. To get help, our first step should be to ask R itself:

-   You can put a question mark in front of a function and execute it. In your output under the tab "Help" an explanation with examples will pop up and explain the function.

-   You can also call the help() function and put the function you want to learn about without brackets in the help() function.

```{r help}
?exp() #questionmark
help(exp) #help command
```

### Assigning objects and printing them

Most of the time you will store your results in objects.

-   You can do so by using the `<-` operator. Afterwards you can work with the objects. Let us assign numbers to out objects.

-   The objects will be saved and you can see them in your environment.

```{r objects}
Pizza <- 7.50 #pizza object

Cola <- 3.50 #cola object

Pizza + Cola #addition of objects
```

We can then go on work with the content of the objects.

-   For beginners, let us add the object Pizza and the object Cola together.

-   We can also save the result of those object in a new object and work with this object and this can go on forever technically.

```{r}
Offer <- Pizza + Cola #assigning addition 

Offer #printing the object

Offer^2 #square the term with ^2
```

### Vectors

In R a vector contains more than one information.

-   You use the `c()` command, and divide the information with a `,`. Let us compare food prices:

```{r interacting objects}

food <- c("Pizza", "Kebab", "Curry", 
          "Fish", "Burrito") #food vector

print(food) #printing it 

prices <- c(7.50, 6.00, 8.50, 3.00, 11.00) #price vector

print(prices) #printing it

cola_prices <- c(3.50, 3, 4, 2.50, 3) #cola prices vector

print(cola_prices) #printing it
```

Now we can calculate the prices for a decent meal in one step by adding the two vectors together. The vector prices_combined will give us the prices for a meal plus a cola:

```{r interacting objects 2}
prices_combined <- prices + cola_prices #prices combined

print(prices_combined) #printing it
```

### Object Classes

Objects can contain information of different *data types*:

|               |               |                                              |
|---------------|---------------|----------------------------------------------|
| **Numeric**   | Numbers       | `c(1, 2.4, 3.14, 4)`                         |
| **Character** | Text          | `c("1", "blue", "fun", "monster")`           |
| **Logical**   | True or false | `c(TRUE, FALSE, TRUE, FALSE)`                |
| **Factor**    | Category      | `c("Strongly disagree", "Agree", "Neutral")` |

For data analysis commands sometimes require special object classes. With the `class()` command we can find out the class. And with `as.numeric` for example we can change classes by assigning it to itself, by it is common to assign it to a new object:

```{r classes}
#Let us find out the classes 
class(prices) #numeric
class(food) #character
class(cola_prices) #numeric
```

We can also change the classes of variables. To do so, we can use `as.factor()`, `as.numeric()`, `as.character()` and so forth. You can do that for every class. Let us change the cola_prices to a vector.

-   To do so, we change call `as.character()` and put the object in it. Then we assign it to another object called `cola_prices_character`. This object will have the class `"character"`.

```{r changing classes}
#We want the cola_prices vector to be a character 
cola_prices_character <- as.character(cola_prices)

#Checking it
class(cola_prices_character)
print(cola_prices_character)
```

### Matrices

#### Making Matrices

There are different ways of building a matrix. Let us start by just binding the vectors as columns together. You can do that `cbind()` if you want to bind columns together. `rbind()` is therefore the command to bind rows together.

-   You have to call `cbind()` and include the vectors you want to bind together

-   The same with `rbind()`

```{r matrix}
price_index <- cbind(food, 
                     prices,
                     cola_prices) #We bind it together

print(price_index) #We print it 

#Let's do the same by binding the rows together

price_index2 <- rbind(food, 
                     prices,
                     cola_prices) #We bind it together

print(price_index2) #We print it 
```

We can also generate a matrix by simulating it. There are a lot of things to care about, simply because a lot is possible:

-   You first call the matrix() command.

-   The first argument is an interval of numbers. These are our total observations if you want.

-   The second and third argument are our number of rows `nrow()` and our number of columns `ncol()`. If you multiply them, they have to result in the number of observations you defined before. We have 20 numbers and 4 multiplied by 5 is 20, thus this is fine.

-   Lastly you have to define if the numbers should be included from left to right, thus by row or if they should be ordered from top to bottom. We go through both examples and then it should become clear.

-   The `dim()` command is helpful, because it shows us to inspect the dimensions.

```{r simulating matrix}
# Create a matrix
matrix_example <- matrix(1:20, nrow = 4, ncol = 5, byrow = T) #

# Checking it
print(matrix_example)
# Checking the dimensions
dim(matrix_example)

# What happens if byrow is set to FALSE?
matrix_example2 <- matrix(1:20, nrow = 4, ncol = 5, byrow = F)

# Checking it 
print(matrix_example2)
dim(matrix_example2)
```

#### Working with Matrices

We want to work with matrices. The first tool to learn is how to inspect the matrices:

-   First, you call the matrix. In our example, matrix_example with square brackets.

-   In these brackets you can call single rows by entering the number of the row you want to inspect, and then you put a comma behind it to signal R that you want to have the row.

-   If you put a number behind the comma, you tell R to give you the column with the number.

-   If you want a single number then you have to define a row and a column.

```{r working with matrices}
#Let us get used to work with objects
row <- 1 
column <- 1 

#Printing it
print(object1 <- matrix_example[row, ]) #printing the first row
print(object2 <- matrix_example[, column]) #printing the first column 
print(object3 <- matrix_example[row, column]) #printing first row and column

print(matrix_example) #printing the matrix
```

```{r matrix info}
#More Information 

nrow(matrix_example) #How many rows

ncol(matrix_example) #How many columns

dim(matrix_example) #Overall dimensions
```

### Data Frames

The next type of data storage are data frames. These are the standard storage objects for data in R. The reason is simple, matrices are only able to contain one type of variables (numeric, character, etc...).

-   In this example, we have a dataset with the variable country (character), the capital (character), the population in mio (numeric), and a if the country is in europe (logical)

```{r dfs}
# making an example df
df_example <- data.frame(
    country = c("Austria", "England", 
                "Brazil", "Germany"), 
    capital = c("Vienna", "London", 
                "Brasilia", "Berlin"), 
    pop = c(9.04, 55.98, 215.3, 83.8),
    europe = c(TRUE, FALSE, TRUE, TRUE)
  )

# Checking it
print(df_example)
```

-   If you want to work with data frames you can call columns by calling the name of the data frame putting a dollar sign behind it and then calling the name of the column, you want to inspect:

```{r getting columns}
df_example$country  
```

You see that you get the vector of the column. We can go further and work more with data frames

-   Let us call a single observation in the data frame: You call the column you want to inspect the same way as before, this time you also put square brackets behind and call the number of the observation in the column.

-   We want to have "Brazil". "Brazil" is the third element of the `df_example$country`, thus we include 3 in the square brackets:

```{r inspect data frames}
df_example$country[3]
```

The last thing to do with data frames is to get columns based on conditions. In the next part of this chapter we will get a method to do so, but we can do so as well with the data frame. Imagine you want to have a vector of `df$country`, but only with countries that have a `df$pop` bigger than 60. Meaning a population bigger than 60.

-   To do so we call the `df$country` column and put square brackets behind it

-   Further we need to call in the square brackets the condition. Thus, the variable `df_example$pop` and set it to bigger than 60. Et voila, we get the columns of `df_example$country` bigger than 60.

```{r dfs conditions}
df_example$country[df_example$pop > 60]
```

## `ifelse` statements and the `ifelse()` function

One of the most frequently used and therefore most important logical operators in programming in general are `ifelse` commands. You probably know them from Excel, but every programming language includes them, because of their usefulness. A quick reminder of their logic.

### If else statement with only one condition

First, you define an if statement. The if statement is a logical statement for example bigger, smaller than X. The logical statement is your test expression. With this you tell the program to test the condition for an object, in excel a cell or whatever object in your programming language can be tested. The program then checks if the condition is TRUE or FALSE. Until this point every if else is the same and now we will look at some variations:

![***Figure 2: Logic of if-else statements***](images/r-if-else-statement_2.png){fig-align="center" width="350"}

Figure 2 shows the logic of an if else statement with one condition. We say that if the test expression is true something should happen. If not nothing happens, because nothing was defined.

-   We want R to judge our grades in school. For this reason we define an object called grade and assign 1.7 to it.

-   In the next step we call if and open a bracket. We include the test expression in the bracket, which shall be if grade, our grade is smaller than 2

-   Then we open curly brackets to define what should happen if the test expression is TRUE. Meaning what should happen if the grade is better than 2. We define print("Good Job").

-   Here is the general logic of a if else statement:

    `if (test expression) {`

    `Body of if`

    `}`

```{r if else statements}

grade <- 1.7 

if(grade < 2) {
  print("Good Job")
} # You write down if(test expression), and then the {body expression}, thus the body expression in fancy brackets.

grade <- 2.5 

if(grade < 2) {
  print("Good Job")
} #Since the condition is not met, nothing happens

```

As you can see if the grade is bigger than 2 nothing happens. Otherwise "Good Job" is printed as intended.

### if statements with else condition

An if command on its own is quite useless. Things get interesting, when we also have a body for the else command. What happens now, is that instead of nothing being printed when the test expression is FALSE. Then the body of else is printed.

![**Figure 3: Standard RStudio Interface**](images/r-if-statement.png){fig-align="center" width="350"}

Now we just add a **body of else** into the equation, in R that means we need to define it:

-   We take the if else statement of the chunk before and now we add an `else` and open curly brackets, where we define the body of `else`.

```{r more if else statements}

grade <- 3.3 #assigning a grade

if (grade <= 2) {
  print("Good Job")
} else {
  print("Life goes on")
} 

grade <- 1.3 #assigning a grade

if (grade <= 2) {
  print("Good Job")
} else {
  print("Life goes on")
}
```

We see that in any case a result is printed.

### The `ifelse()` command

What you see above is the manual way of coding an if else condition. But there is also an ifelse() function in R, where you do not have different colors and fancy brackets everywhere.

-   you call ifelse()

-   The first argument is your test expression

-   The second the body of if

-   The third the body of else

`ifelse(test expression, body of if, body of else)`

```{r ifelse command}
ifelse(grade <=2, "Good Job", "Life goes on") #ifelse command
```

### if else ladders/ if else with multiple conditions

The world is a complex place. Sometimes things are not black and white. Thus we have to be prepared for different scenarios. To make it less melodramatic, if else statements are also possible with several bodies of else. Let us take the example that we want to have a statement of R regarding our grade for different number intervals. What R does is to check the first condition, afterward he checks the second test expression and so forth until he finds a hit and prints it. If no expression is found, you have to define a last else expression.

-   You add else if with curly brackets where the test expression is included.

-   Lastly, you define an else with curly brackets for the case there is not test expression

```{r if else multiple}

grade <- 3.3 #Assigning a grade

if (grade == 1.0) {
  print("Amazing") 
} else if (grade > 1.0 & grade <= 2.0) {
  print("Good Job")
} else if (grade > 2.0 & grade <= 3.0) {
  print("OK")
} else if (grade > 3.0 & grade <= 4.0) {
  print("Life goes on")
} else {
  print("No expression found")
}
  
grade <- 1.7 #Assigning a grade

if (grade == 1.0) {
  print("Amazing") 
} else if (grade > 1.0 & grade <= 2.0) {
  print("Good Job")
} else if (grade > 2.0 & grade <= 3.0) {
  print("OK")
} else if (grade > 3.0 & grade <= 4.0) {
  print("Life goes on")
} else {
  print("No expression found")
}

grade <- 5.0 #Assigning a grade

if (grade == 1.0) {
  print("Amazing") 
} else if (grade > 1.0 & grade <= 2.0) {
  print("Good Job")
} else if (grade > 2.0 & grade <= 3.0) {
  print("OK")
} else if (grade > 3.0 & grade <= 4.0) {
  print("Life goes on")
} else {
  print("No expression found")
}
```

Again we can do so-called **nested ifelse function**, using the `ifelse()` command:

-   Instead of defining an else condition, we define another `ifelse()` command and do as long as we need to.

-   The last `ifelse()` is then the only one with a clear else condition.

```{r ifelse nested}

grade <- 1.7 #Assigning a grade

ifelse(grade == 1.0, "Amazing", 
       ifelse(grade > 1 & grade <= 2, "Good Job", 
              ifelse(grade > 2 & grade <= 3, "OK", 
                     ifelse(grade > 3 & grade <=4, "Life goes on", 
                            "No expression found"
                            )
                     )
              )
       )

grade <- 3.3 #Assigning a grade

ifelse(grade == 1.0, "Amazing", 
       ifelse(grade > 1 & grade <= 2, "Good Job", 
              ifelse(grade > 2 & grade <= 3, "OK", 
                     ifelse(grade > 3 &
                            grade <=4, "Life goes on", 
                            "No expression found")
                     )
              )
       )

# The same logic: ifelse(test expression, body expression if, ifelse(test expression 2, body expression if 2)) etc..
```

## Outlook

This section was a brief introduction to the fundamentals of R. It was kept simple to give you a feeling for a R. These are the absolute basics, which are needed to understand everything, which will be built based on it.

## Exercise Section

### Exercise 1: Making your first Vector

Create a vector called `my_vector` with the values 1,2,3 and check is class.

```{r ch1 exercise 1, eval=FALSE}
#create the vector 
my_vector <- 

#check the class

```

### Exercise 2: Making your first matrix

Create a Matrix called `student`. This should contain information about the `name`, `age` and `major`. Make three vectors wiht three entries and bind them together to a the matrix `student`. Print the matrix.

```{r ch2 exercise 2, eval = FALSE}
#Create the vectors 

name <- 
age <- 
major <- 

#Create the matrix
student <- 
  
#Print the matrix

```

### Exercise 3: `ifelse` function

Write an `ifelse` statement that checks if a given number is positive or negative. If the number is positive, print "Number is positive", otherwise print "Number is negative". Feel free to decide if you want to use the `ifelse` function or the `ifelse` condition.

```{r ch3 exercise 3, eval = FALSE}

#Assigning the number to the object "number"
number <- -4



```

### Exercise 4: `ifelse` ladders

Write an if-else ladder that categorizes a student's grade based on their score. The grading criteria are as follows:

Score \>= 90: "A" Score \>= 80 and \< 90: "B" Score \>= 70 and \< 80: "C" Score \>= 60 and \< 70: "D" Score \< 60: "F"

```{r ch4 exercise 4, eval = FALSE}

```

<!--chapter:end:02-fundamentals.Rmd-->

# Data Manipulation

In this chapter we load and transform data. That is a standard step and probably the step taking the most time, when analyzing data. Preparing data is necessary, since data analysis requires data in a certain format. Which format depends on the model. I will introduce you to dplyr, the standard package for manipulating data. At first, it seems a bit complicated, but I guarantee you, if you do data analyses in R, you will have to work with it a lot and you will become quite fluent in it. The goal of this chapter is to teach you how to manipulate data so that we can bring it into the format we need it to analyse it properly. Have fun!

![](images/meme2.jpg){fig-align="center" width="300"}

## Packages

This far, we've only covered so-called "base R functions" or "built-in functions", but R has an active community and sometimes further operations are needed, so we use **packages**. These are including further functions, which we will use heavily in the following section.

### The `tidyverse` package

One of the most influential and widely used package in R is the `tidyverse` package. This package includes several other packages, which are key for data manipulation e.g. `dplyr`, `ggplot2`, `stringr`, `readr`, `tidyr`.

## Working with packages

### Installing packages

To install packages you use the very creative `install.packages()` command in R. Note that it is necessary to directly install a package in R. This step is only required once:

-   Call the `install.packages()` command.

-   You put the name of the package in quotation marks into the function.

`install.packages("pacman")`

### Loading your packages

While you only need to install a package once, you need to load it every time in your script, when you open it. You can do that with the `library()` function in R:

-   Call the `library()` function.

-   Put the name of

`library(pacman)`

It is always important to have an efficient workflow in R. Traditional R users, load all packages they need at the beginning of their page. Logically, so they just need to go back to the top of the script and need to load it every time they open the script. But there are way more elegant and pragmatic ways to do that.

One way is the `pacman` package:

-   First, if you use the name of package and put a "::" behind it you tell R to go into the package and to specifically get one command of the package, in our case the `p_load` command.

-   Second, the p_load command, loads the packages in the brackets and checks if they are installed, if not, it automatically installs and loads them.

```{r installing pacman if needed, echo = FALSE}
if (!require("pacman")) install.packages("pacman")
```

```{r pacman}
pacman::p_load("tidyverse", "psych", "gapminder") #loading packages
```

## The Data we will work with: The European Social Survey (ESS)

For the following Data Manipulation Part, we will use the European Social Survey Round 10 with the topic "Democracy, Digital social contacts". It is a high-quality survey conducted in 31 European countries. Round 10 was conducted in 2020 and is the most recent ESS. We will use it, since survey data is quite popular among students and further at some point everyone needs to work with it. But do not worry if you do not like survey data, we will also cover other prominent Datasets.

You can freely download it via the [website](https://www.europeansocialsurvey.org/) of the ESS. From there you need to go to the Data Portal and than you can download the Round you want, in the format you want. As already mentioned, use the `.dta` or `.csv` format.

+--------------+--------------------------------------------------------+-------------------------------------------------------------------------------------------------------+
| **Variable** | **Description**                                        | Scales                                                                                                |
+:============:+:======================================================:+:=====================================================================================================:+
| **idnt**     | Respondent's identification number                     | unique number from 1-9000                                                                             |
+--------------+--------------------------------------------------------+-------------------------------------------------------------------------------------------------------+
| **year**     | The year when the survey was conducted                 | only 2020                                                                                             |
+--------------+--------------------------------------------------------+-------------------------------------------------------------------------------------------------------+
| **cntry**    | Country                                                | BE, BG, CH, CZ, EE, FI, FR,GB, GR, HR, HU, IE, IS, IT, LT,NL, NO, PT, SI, SK                          |
+--------------+--------------------------------------------------------+-------------------------------------------------------------------------------------------------------+
| **agea**     | Age of the Respondent, calculated                      | Number of Age = 15-90                                                                                 |
|              |                                                        |                                                                                                       |
|              |                                                        | 999 = Not available                                                                                   |
+--------------+--------------------------------------------------------+-------------------------------------------------------------------------------------------------------+
| **gndr**     | Gender                                                 | 1 = Male;                                                                                             |
|              |                                                        |                                                                                                       |
|              |                                                        | 2 = Female;                                                                                           |
|              |                                                        |                                                                                                       |
|              |                                                        | 9 = No answer                                                                                         |
+--------------+--------------------------------------------------------+-------------------------------------------------------------------------------------------------------+
| **happy**    | How happy are you                                      | 0 (Extremly unhappy) - 10 (Extremly happy);                                                           |
|              |                                                        |                                                                                                       |
|              |                                                        | 77 = Refusal;                                                                                         |
|              |                                                        |                                                                                                       |
|              |                                                        | 88 = Don't Know;                                                                                      |
|              |                                                        |                                                                                                       |
|              |                                                        | 99 = No answer                                                                                        |
+--------------+--------------------------------------------------------+-------------------------------------------------------------------------------------------------------+
| **eisced**   | Highest level of education, ES - ISCED                 | 0 = Not possible to harmonise into ES-ISCED;                                                          |
|              |                                                        |                                                                                                       |
|              |                                                        | 1 (ES-ISCED I , less than lower secondary) - 7 (ES-ISCED V2, higher tertiary education, =\> MA level; |
|              |                                                        |                                                                                                       |
|              |                                                        | 55 = Other;                                                                                           |
|              |                                                        |                                                                                                       |
|              |                                                        | 77 = Refusal;                                                                                         |
|              |                                                        |                                                                                                       |
|              |                                                        | 88 = Don't know;                                                                                      |
|              |                                                        |                                                                                                       |
|              |                                                        | 99 = No answer                                                                                        |
+--------------+--------------------------------------------------------+-------------------------------------------------------------------------------------------------------+
| **netusoft** | Internet use, how often                                | 1 (Never) - 5 (Every day);                                                                            |
|              |                                                        |                                                                                                       |
|              |                                                        | 7 = Refusal;                                                                                          |
|              |                                                        |                                                                                                       |
|              |                                                        | 8 = Don't know;                                                                                       |
|              |                                                        |                                                                                                       |
|              |                                                        | 9 = No answer                                                                                         |
+--------------+--------------------------------------------------------+-------------------------------------------------------------------------------------------------------+
| **trstprl**  | Most people can be trusted or you can't be too careful | 0 (You can't be too careful) - 10 (Most people can be trusted);                                       |
|              |                                                        |                                                                                                       |
|              |                                                        | 77 = Refusal;                                                                                         |
|              |                                                        |                                                                                                       |
|              |                                                        | 88 = Don't Know;                                                                                      |
|              |                                                        |                                                                                                       |
|              |                                                        | 99 = No answer                                                                                        |
+--------------+--------------------------------------------------------+-------------------------------------------------------------------------------------------------------+
| **lrscale**  | Left-Right Placement                                   | 0 (Left) - 10 (Right);                                                                                |
|              |                                                        |                                                                                                       |
|              |                                                        | 77 = Refusal;                                                                                         |
|              |                                                        |                                                                                                       |
|              |                                                        | 88 = Don't know;                                                                                      |
|              |                                                        |                                                                                                       |
|              |                                                        | 99 = No answer                                                                                        |
+--------------+--------------------------------------------------------+-------------------------------------------------------------------------------------------------------+

*Note: In the following, I will simulate the ESS with the same names and same range values. Which means if you read in the actual ESS, you should be able to run the code as well without problems!*

*I do so for reproducibility reasons. When downloading the script, users should be able to run the whole script with one click. This is because users, who are new to R might have problems with setting working directories, since working directories are prone to errors, especially at the beginning.*

```{r simulating ess, echo = FALSE}
set.seed(123)

ess <- data.frame(
  idnt = 1:9000,
  year = 2020,
  cntry = rep(c("BE", "BG", "CH", "CZ", "EE", "FI", "FR","GB", 
                "GR", "HR", "HU", "IE", "IS", "IT", "LT","NL", 
                "NO", "PT", "SI", "SK"), each = 450),
  agea = sample(15:90, 9000, replace = TRUE),
  gndr = sample(1:2, 9000, replace = TRUE), 
  happy = sample(1:10, 9000, replace = TRUE),   
  eisced = sample(1:7, 9000, replace = TRUE), 
  netusoft = sample(1:5, 9000, replace = TRUE),#Internet use    
  trstprl = sample(1:10, 9000, replace = TRUE),
  lrscale = sample(1:10, 9000, replace = TRUE)   
)

missing_indices_agea <- sample(1:9000, 45)
ess$agea[missing_indices_agea] <- 999

missing_indices_gndr <- sample(1:9000, 215)
ess$gndr[missing_indices_gndr] <- 9

missing_indices_happy <- sample(1:9000, 145)
ess$happy[missing_indices_happy] <- sample(c(77, 88, 99), 145, 
                                         replace = TRUE)

missing_indices_eisced <- sample(1:9000, 355)
ess$eisced[missing_indices_eisced] <- sample(c(55, 77, 88, 99), 
                                             355, 
                                             replace = TRUE)

missing_indices_netusoft <- sample(1:9000, 228)
ess$netusoft[missing_indices_netusoft] <- sample(c(7, 8, 9), 
                                                 228, 
                                                 replace = TRUE)

missing_indices_trstprl <- sample(1:9000, 277)
ess$trstprl[missing_indices_trstprl] <- sample(c(77, 88, 99),
                                               277, 
                                               replace = TRUE)

missing_indices_lrscale <- sample(1:9000, 308)
ess$lrscale[missing_indices_lrscale] <- sample(c(77, 88, 99),
                                               308, 
                                               replace = TRUE)
```

### Load the data

#### How to load data

The first thing to do, when cleaning data is to load the data into R. The first thing to ensure is to know, where R can take the data from and load it into its own environment. You can call data from your local devise or remote through e.g. an API:

-   **Remote:** This means the dataset is laying around on some external Cloud or Server and you have the possibility to load it into R, without downloading it on your local devise. Depending on the dataset there are different ways, I will show you later how to get data from the World Bank.

-   **Local:** To get data from your devise, you have to tell R, where to find it, meaning you tell him the path. Go to your file and click on it. The path will be shown in the address bar, just copy it and paste it in R with the responding command. The command is defined by the type of the file as you can see in the table below. Here is an example how it could look wit a .csv file

    `data <- read.csv("C:/Users/YourUsername/Documents/data.csv")`

This table presents some of the most frequent data types, which you can download and load into R. There are of course more data types, and to find out if you can read them into R, you can just google the type and look for the command. Mostly you will find a package with a command. And mostly, those commands start with "read".

+---------------------------+----------------+----------------------------------------------+-------------------------+
| File                      | File Extension | Package                                      | Command                 |
+===========================+================+==============================================+=========================+
| **Stata**                 | .dta           | haven                                        | `read_dta()`            |
+---------------------------+----------------+----------------------------------------------+-------------------------+
| **CSV-Files**             | .csv           | readr (is included in the tidyverse package) | `read_csv()`            |
+---------------------------+----------------+----------------------------------------------+-------------------------+
| **Excel-Files**           | .xlsx; .xls    | readxl                                       | `read.rds()`            |
+---------------------------+----------------+----------------------------------------------+-------------------------+
| **RData (also RDS Data)** | .RData;.rds    | base R functions, no package required        | `load()` , `read.rds()` |
+---------------------------+----------------+----------------------------------------------+-------------------------+

: Examples of Different Data Types and how to load them into R

#### Working Directories and R-Projects

If you have more data and files you want to load into R, it is not recommended to copy always the path of every file. Instead it is common to work with working directories or R-Projects.

-   There is always a working directory you can find out, in which working directory you are currently at by using the command `getwd()`, just run it in your console and R will give you the path it currently is working at. Let us assume you saved all your data in a folder called "Intro_to_R_course", then you can set the working directory separately with this command (do not forget to change the names in the paths):

    `setwd("C:/Users/YourUsername/Intro_to_R_course")`

-   After you set the working directory you can run again `getwd()` and the path now has to be changed to the content of the `setwd()`. The advantage is now that if the working directory is set, you can load in the data without specifying the path. If your data is in the path of the working directory you can load like that:

    `data <- read.csv("data.csv")`

-   You can also create an R-Project. If you click on `file > New Project > New Directory > New Project` you can determine the working directory and create a folder in it. In this folder, there will be an R-Project file, if you open this file, a blank R environment will appear. Now, you can create files in this folder. And every time you enter R through the R-Project file, you do not have to set any working directories. Because if the R-Projects opens R, it automatically sets the working directory to where you created the folder.

-   I would not recommend to work with R-Projects in the beginning, but when you work with collaborators and want to make your code reproducible for others, then you will have to work with R-Projects one day. Again, this also the reason, I just simulate all the data, so you can work with the code without worrying about any working directories.

### One last thing: Pipelines

Sometimes codes have several dimensions, which could make it quite complicated

`leave_house(get_dressed(get_out_of_the_bed(wake_up(me))))`

Well, as you can see there are too many dimensions and with tidyverse you can basically split it up into so-called **Pipelines**, for them you use a **Pipe** `%>%` :

`me %>%` `wake_up() %>%` `get_out_of_the_bed() %>%` `get_dressed() %>%` `leave_house`

It is the same code, in R this code would do the same. But the advantage is that it is way more intuitive and makes the code clear. Here an example:

-   First, I create a vector with three random numbers named `q`.

-   Then I take the mean, then the exponential and lastly the square root. I could just wrap the codes around each other like this: `sqrt(exp(mean(q)))`.

-   But I could also use pipes, where I clearly see that first the mean, then the exponential and then the square root is taken: `q %>% mean() %>% exp() %>% sqrt()`. Run both codes, they are producing the same result.

```{r piping}
#Making a vector with three random numbers
q <- c(6,3,8)

#Taking first the mean, second the exponential and lastly the square root
sqrt(exp(mean(q)))

###With a Pipe 
q %>% 
  mean() %>%
  exp() %>%
  sqrt()
```

This was an easy and short example, but as you will see at the end of this chapter, the code can get really fast really messy and to have a clean code, we will use pipes. Furthermore, tidyverse users use pipes all the times, so even if you do not use them, you have to understand them.

## Let's wrangle the data: The `dplyr` package

![](images/meme3.jpg){fig-align="center" width="435"}

The `dplyr` package is THE standard package, when it comes to data manipulation (next to Base R of course). It has essential functions, and helpful further functions. If you are able to understand the flexibility of these functions you can easily handle every data set.

### The `filter()` command

The first function I introduce you is the `filter()` function. Within the filter() function we can define certain conditions to cut our Data Set to.

-   We need the filter() function and then a variable we want to filter based on. In my example, I only want to keep all observations, which have "HU" in their cntry - variable. Substantially this means, I cut down to all observations from Hungary. I use the == operator since I want to have all observations where the condition is true.

    Here is a quick reminder of logical operators in R:

    +--------------------------+----------------------------------------------------------------------------+
    | Logical Operator         | Meaning                                                                    |
    +==========================+============================================================================+
    | ==                       | equals                                                                     |
    +--------------------------+----------------------------------------------------------------------------+
    | \<                       | smaller than                                                               |
    +--------------------------+----------------------------------------------------------------------------+
    | \>                       | greater than                                                               |
    +--------------------------+----------------------------------------------------------------------------+
    | \<=                      | less than or equal to                                                      |
    +--------------------------+----------------------------------------------------------------------------+
    | \>=                      | greater than or equal to                                                   |
    +--------------------------+----------------------------------------------------------------------------+
    | ! (e.g. !=; \>!; \<!...) | not equal, not greater than, not smaller than                              |
    +--------------------------+----------------------------------------------------------------------------+
    | &                        | element-wise AND operator. It returns TRUE if both elements are true       |
    +--------------------------+----------------------------------------------------------------------------+
    | \|                       | element-wise OR operator. It returns TRUE if one of the statements is TRUE |
    +--------------------------+----------------------------------------------------------------------------+

#### Filtering for only one condition

-   We start by defining a new object, let us call it `d1`.

-   Then we take the data we want to filter, in our case `ess`.

-   We define a pipe, write down filter and define a condition, in our case that only cases where `cntry` is equal to the iso2c code of Hungary, `"HU"`.

-   In the next code, we do the same and filter for cases that are equal or smaller than 40. Thus we get a dataset with observations who are 40 or younger.

*Note: Since the cntry variable is a character variable, we have to put the condition in square brackets. The variable agea is a numeric variable, therefore we only need the number.*

```{r filter one condition}
#filtering for cases only in Hungary
d1 <- ess %>% 
  filter(cntry == "HU")

#checking it
head(d1) 

#We only want participants younger than 40
d2 <- ess %>%  
  filter(agea <= 40)

#checking it
head(d2) 
```

#### Filtering for multiple condition

```{r filter multiple condition}
#filtering for cases in Hungary and France
d1 <- ess %>% 
  filter(cntry %in% c("HU", "FR"))

#Checking it
head(d1)  

#filtering for cases under 40 in Hungary and France
d2 <- ess %>%
  filter(cntry %in% c("HU", "FR") &
           agea <= 40)

#Checking it
head(d2)

#d2 <- ess %>% #Our dataset
#  filter(cntry %in% c("HU", "FR"), 
#           agea <= 40) #filtering for cases under 40 in Hungary and France with a comma
#head(d2)
```

### The `select()` function:

We obviously do not care about all variables a dataset can offer (mostly). To select the variables we need, we can use the `select()` function. This of course depends on our research question. Let us say we want to select the year, the country, the happy variable, age, gender, income and education.

#### Selecting and deleting single Rows

-   We only have to pass their column names to `select()`. That's it.

```{r select}
#Selecting relevant variables
d1 <- ess %>%
   select(year, cntry, happy, agea, gndr, eisced) 

#Checking it
head(d1) 
```

#### Deleting Rows

-   To delete row, you just put a minus in front of the column, you want to delete. That's it, the rest stays the same.

```{r minus select}
#We delete columns by simply putting a comma before it
d2 <- d1 %>% 
  select(-agea)

#Checking it
head(d2)
```

#### Combining `select()` with the `filter()` function

One huge advantage of piping is, that we can clear our data in one step or at least big steps. The only thing you have to be aware of is what you put there. Remember the last command, is always the first to be executed.

-   In this example, we select the 7 variables and then filter it for all observations under 40.

-   The two commands are separated by a pipe.

```{r filter and select}
#Combining codes
d1 <- ess %>%
  filter(agea < 40) %>%
  select(year, cntry, happy, agea, gndr, eisced)

#Checking it
head(d1)
```

### The `arrange()` function

If we want our data to be in a certain order, we arrange it with this function.

#### Arranging in ascending order

-   The arrange() function is called, and afterwards we call the variable we want to arrange based on. The default function of arrange, orders the data always in ascending order.

```{r arrange}
#Adding arrange()
d1 <- ess %>%
  filter(agea < 40) %>%
  select(year, cntry, happy, agea, gndr, eisced) %>% 
  arrange(agea) 

#Checking it
head(d1)
```

#### Arranging in descending order

-   If we want to order them in ascending order, we have to call `desc()` inside the `arrange()` and put the name of the variable inside `desc()`.

```{r arrange descending}
#arranging in descending order 
d1 <- ess %>%
  filter(agea < 40) %>%
  select(year, cntry, happy, agea, gndr, eisced) %>% 
  arrange(desc(agea))

#Checking it
head(d1)
```

### The `rename()` and `relocate()` function

Two functions to make our dataset structured more useful are `rename()` and `relocate()`, well they do what they basically named after:

#### Renaming Variables: `rename()`

-   Rename() follows a simple logic, you call the function and write down the new name, thus the name you want to assign, then you put an equal sign, and put in the old name, thus the current name of the column.

Note: If you have a variable, which is binary, thus has two discrete categories, you name it after the category which corresponds to the higher value. If male is 0 and female is 1, you name it after the higher category 1, therefore the variable is named female.

```{r rename}
#Renaming variables
d1 <- ess %>%
  filter(agea < 40) %>%
  select(year, cntry, happy, agea, gndr, eisced) %>% 
  arrange(desc(agea)) %>%
  rename(country = cntry, 
         age = agea, 
         education = eisced, 
         female = gndr) 

#Checking it
head(d1)
```

#### Relocating Variables: `relocate()`

-   You call relocate and determine the order of the columns.

-   You can also rearrange single columns, you call the name of the column, which you want to rarrange. and then you either call .before and a column or .after and a column. And the column you want to rearrange is placed before or after the column you want to place. You separate both arguments with a comma.

```{r relocate}
#relocating variables
d1 <- ess %>%
  filter(agea < 40) %>%
  select(year, cntry, happy, agea, gndr, eisced) %>% 
  arrange(desc(agea)) %>%
  rename(country = cntry, 
         age = agea, 
         education = eisced, 
         female = gndr) %>% 
  relocate(education, age, female, country, happy, year) #determine the order

#Checking it
head(d1)

#relocate after 
d2 <- ess %>%
  filter(agea < 40) %>%
  select(year, cntry, happy, agea, gndr, eisced) %>% 
  arrange(desc(agea)) %>%
  rename(country = cntry, 
         age = agea, 
         education = eisced, 
         female = gndr) %>%
  relocate(country, .after = age) 

#Checking it
head(d2)

#relocating before 
d3 <- ess %>%
  filter(agea < 40) %>%
  select(year, cntry, happy, agea, gndr, eisced) %>% 
  arrange(desc(agea)) %>%
  rename(country = cntry, 
         age = agea, 
         education = eisced, 
         female = gndr) %>%
  relocate(country, .before = age) 

#Checking it
head(d3)
```

### The `mutate()` function

The next function is the powerful `mutate()` command. For the start, just think about mutate as a Variable with which you can transform or mutate variables to other variables as you please.

-   You call mutate() and first you define the name of a new column with a name that is not existent in your dataset. You could also use the name of an existing column, but be careful! Then the new values are overwriting the old ones and we do not want that necessarily, therefore I recommend to always define new columns.

-   After defining the new name you put an equal sign after it and define the calculation. In our case we just multiply happy, so all values in the happy column by 10

```{r mutate}
#mutating variables
d1 <- ess %>%
  mutate(happy_10 = happy*10) 

#checking it
head(d1)
```

We can also use mutate for more than calculations. We can also define new columns by mutating existing columns into different classes:

-   The variable new_variable is just a random mathematical operation including two variables.

-   The second call in mutate() changes the class of the gndr variable to a character and saves it as such in the dataset.

```{r mutate 2}
#more mutating
d2 <- ess %>% 
  mutate(new_variable = happy*10/eisced+67, 
         female_char = as.character(gndr)) %>% 
  select(female_char, new_variable)

#Checking it
head(d2)
```

What makes mutate() so powerful is that you can use other functions in it to define your variables as you want. Image you have the happy variable. Quick reminder, this variable contains the answers to the question "How happy are you?" in the questionnaire of the European Social Survey. It is scaled on a 0 (not happy at all) to 10(very happy).

Let us say we want to change that. We want to make a variable with only three categories (unhappy, neutral, happy). We decide that all values from 0-4 should be classifies as unhappy, 5 should be classified as neutral and everything above 5 as happy. How can we do that in R?

There are different ways and I will show you some of them:

#### Recoding with `mutate()` using `recode().`

You can use the `recode()` function:

-   First, we define a new column name, in our example happy_cat.

-   Then we call `recode()` and inside of it we call the variable we want to transform, in our case happy.

-   We put call the category we want to transform in these brackets ``` `` ```. We put an equal sign after it and define the new value we want to assign to the category.

*Note: We can assign different values, do not worry about the "NA_real\_" value I will come back to that later*

```{r recode}
#Get an overview of the variable
table(ess$happy)

#recoding variables
d1 <- ess %>% 
  mutate(
    gndr_fac = as.factor(gndr), #always check the class
    happy_cat = dplyr::recode(happy,
                    `1` = 0,
                    `2` = 0,
                    `3` = 0,
                    `4` = 0,
                    `5` = 1,
                    `6` = 2,
                    `7` = 2,
                    `8` = 2,
                    `9` = 2,
                   `10` = 2,
                   `77` = NA_real_, 
                   `88` = NA_real_,
                   `99` = NA_real_),
    female = dplyr::recode(gndr_fac, 
                    `1` = "Male", 
                    `2` = "Female",
                    `9` = NA_character_))

#Let us check how it worked out 
table(d1$happy_cat)
table(d1$female)
```

#### Recoding with `mutate()` using `case_when()`

As you see, the `recode()` command is quite extensive. The **tidyverse** offers a way more intuitive command, the `case_when()` function. The function case_when is a generalized ifelse function. Which means we can use logical operators. The recode() function is way too extensive, it gives you full control over the data, but we do not that much control:

-   We again define happy_cat.

-   We call case_when() and inside we call our variable we want to transform.

-   We define a logical statements, in our case that happy smaller than 5, meaning that we tell R that all values under 5 should be transformed.

-   We call the wave \~ and tell R what value should substitute all values which are TRUE for the logical statement.

*Note: What is not explicitly stated in the case_when() function will be coded as NA.*

```{r case_when}
#recoding with case_when
d1 <- ess %>% 
  mutate(gndr_fac = as.factor(gndr),
    happy_cat = case_when(
    happy < 5 ~ 0,
    happy == 5 ~ 1,
    happy > 5 ~ 2),
    female = case_when(
    gndr == 1 ~ "Male",
    gndr == 2 ~ "Female"
    ))

#Checking it
table(d1$female)
table(d1$happy_cat)

```

#### Recoding with `mutate()` using `ifelse()`

Do you remember the `ifelse()` function? As already mentioned, the `case_when()` command is a generalized `ifelse()` function. If you want to keep it old school, we can also recode with the `ifelse()` function:

```{r ifelse}
#recoding with ifelse function
d1 <- ess %>% 
  mutate(gndr_fac = as.factor(gndr),
         happy_cat = ifelse(happy < 5, 0, 
                            ifelse(happy == 5, 1, 
                                   ifelse(happy > 5, 2, NA
                                          ))),
         female = ifelse(gndr_fac == 1, "Male",
                         ifelse(gndr_fac == 2, "Female", NA))
  )

#Check it
table(d1$happy_cat)
table(d1$female)
```

### Handling Missing Values/Incomplete Data

As you saw right now, not all data in a dataset is complete. Of course not, there are several sources, which can lead to incomplete/missing data. Can you think of reasons why?

In Data Sciences we need to deal with missing values directly. If we look into the codebook, the ESS declares different types of missing values with high numbers: 7(7) means "Refusal", so the respondent refused to answer, 8(8) means "dont know", and 9(9) "No answer".

*Note that the ESS does so, since some researchers are interested in missing values, and why they happen, so they can investigate it.*

For us, this is a problem, because we cannot run an analysis with missing values. There are two options:

1.  Using statistics to artificially fill them out, this called multiple imputation techniques, but this requires advanced data science knowledge so I do not recommend that for beginners.
2.  Just delete incomplete observations to have a dataset without missing values

The ESS assigns values to missing values. First, we have to tell R that we do want those values to be missing values otherwise it biases our analyses. In the following, we have to recode the variables and tell R, that the missing values are declared as such:

-   I already showed you how to do so. In the following I will use `case_when()`

We will do the second one, and I already showed how to recode useless values to NAs so this should be clear by now. Remember, the `ifelse()` and `recode()` explicitly need input to turn values into NAs.

```{r workflow with drop_na}
#Creating missing values and showing a mutating workflow
d1 <- ess %>%
  filter(agea >=40) %>% 
  select(year, cntry, netusoft, agea, eisced, gndr, happy) %>% 
  arrange(desc(agea)) %>% 
  rename(
    internet_use = netusoft,
    age = agea, 
    education = eisced, 
    female = gndr) %>% 
  mutate( 
    internet_use = case_when( 
      internet_use < 5 ~ NA_real_, 
      TRUE ~ internet_use), 
    age = case_when(
      age == 999 ~ NA_real_,
      TRUE ~ age), 
    education = case_when(
      education %in% c(55, 77, 88, 99) ~ NA_real_,
      TRUE ~ education), 
    female = case_when(
      female == 1 ~ 0, 
      female == 2 ~ 1, 
      female == 9 ~ NA_real_, 
      TRUE ~ female),
    happy = case_when(
      happy %in% c(77, 88, 99) ~ NA_real_,
      TRUE ~ happy)
    ) %>%
  arrange(age)

#Checking it
head(d1) 
```

When you print the dataset, you see that now there are some values named "NA". That stands for not available.

-   If we want to delete NAs, we can use the tidyverse way by simply piping to `drop_na()`

-   The base R way would be to put the dataset name `na.omit()`.

We assign both to a new data frame. We will see in the result that all rows are deleted which include NAs. This is one of the reasons it is important to cut down to variables we only need, so that only incomplete observations of our variables we need are deleted. So always remember, first transforming, than dropping.

```{r missing values}
#Checking it
head(d1) 

#dropping NAs 
d2 <- d1 %>% 
  drop_na() 

#Checking if there are NAs
colSums(is.na(d2))

#dropping NAs
d3 <- na.omit(d1) 

#Checking if there are NAs
colSums(is.na(d2))
```

We see that there are no missing values left in our dataset, and our number of observations are reduced.

Now we have all ingredients to make our dataset. Do you remember our research question? We want to find out if people, who tend to not trust science are less willing to get vaccinated. We want to do that for all people over 40.

### The `group_by()` and `summarize()` functions

Two of the most useful commands in R for summary statistics are `group_by()` and `summarize()`.

`group_by()` helps us to, when we have categorical variables with several observations and we want to calculate a metric e.g. for this group. The dataset we have loaded, the ESS for example asked 9000 respondents about their level of happiness. Image you are interested in the average level of happiness of men and women. Here the `group_by()` functions defines the groups we want to aggregate e.g. gender.

#### With one grouping variable and one metric

`summarize()` defines the metric we want to search. For example we want to calculate the mean of the level of happiness of men and women. We also could just calculate the median for example. Let us have a look:

-   First, we call the `group_by()` function and define the group we want to aggregate, thus the group we are interested in. In our example, we want to aggregate based on the sexes, therefore we need the to define that. Before that we have to delete NAs or transform the variable to the categories we are interested in, therefore we first call mutate() and transform the variable.

-   Second, we call summarize() and define the name of the new column, let us call it average_happiness() and then we call the metric of the variable we are interested in. In our example, we were interested in the average happiness, so we have to call mean() and the happy variable:

```{r group_by and summarise}
#group_by and summarize
d1 <- ess %>% 
  mutate(
    gndr_fac = as.factor(gndr),
    female = case_when(
    gndr_fac == 1 ~ "Male", 
    gndr_fac == 2 ~ "Female",
    gndr_fac == 9 ~ NA_character_
  )) %>%
  drop_na() %>%
  group_by(female) %>% 
  dplyr::summarize(average_happiness = mean(happy))

#Checking it
head(d1)
```

Et voil, we get a dataset with two observations, because we have only two groups. The second row is the average_happiness row we defined in the summarize() function.

#### With more grouping variables and metrics

The `group_by()` and `summarize()` functions are of course way more flexible, for example, we can define more groups. What about that, you are interested in the level of happiness of females, and males in the countries conducted by the ESS. You just put the countries and then the gender variable in the `group_by()`. Further we might be interested in more metrics, no problem, let us just define more columns with `summarize()`. Again, you have to first clean the data by transforming the class and deleting missing values.

```{r group_by and summarise 2}
#grouping and summarize
d1 <- ess %>% 
  mutate(
    country = cntry,
    gndr_fac = as.factor(gndr),
    female = case_when(
    gndr_fac == 1 ~ "Male", 
    gndr_fac == 2 ~ "Female",
    gndr_fac %in% c(77, 88, 99) ~ NA_character_),
    age = case_when(
      agea == 999 ~ NA_real_,
      TRUE ~ agea)
    ) %>%
  drop_na() %>%
  group_by(country, female) %>% 
  dplyr::summarize(average_happiness = mean(happy), 
            median_happiness = median(happy), 
            average_age = mean(age), 
            meadian_age = median(age)
          )

#Check it out
head(d1)
```

## Merging Datasets

### Introduction to merging with `dplyr` and preparing data

Sometimes it could be the case that you need variables, which are not in one dataset *a priori* available, but in another dataset. For this case you load both datasets and **merge** them together. **This only works if there is a similiar data structure, so know your data !**

As an example, I will show how to do that with **World Bank Data**. From this data we can gather nearly all important economic indicators for countries since the 1970s. But mostly we need to merge them to datasets we are interested in. We will merge the **World Bank Data** with the **ESS** data. So we can analyze variables, which were not collected in the same dataset.

There are several ways of getting World Bank Data, but I will show you the most efficient. There is the package `WDI` with which you can get data through an API (Application Programming Interface). Long story short, we do not need to download anything and get the data directly with code:

First we define, which countries should be included:

```{r getting countries}
countries <- c("BE", "BG", "CH", "EE", "FR","GB") 
```

Afterwards we define, which variables we want. You do that by using the official indicator, thus the variable you want. You can find the indicators on the website of the [world bank data](https://data.worldbank.org/indicator). Click on the variables you want, then click on the details, and there you find the indicator. I will use GDP per capita, Fuel exports, CO2 emissions (kt).

```{r getting indicators}
indicators = c("NY.GDP.PCAP.CD", "TX.VAL.FUEL.ZS.UN", "EN.ATM.CO2E.KT")
```

Now we are ready to use the API.

-   To do so we call the WDI function.

-   We define the argument country to only get countries we are interested in.

-   We also define the indicators.

-   Lastly, with the "`start =`" argument we define the starting year, so data which goes back to that date is loaded and with "`end =`" is analagos to define where the time should stop. Thus, both arguments define the time span we want to inspect

```{r world bank data}
#wb <- WDI( 
#  country = countries, #We include our countries 
#  indicator = indicators, #We include our variables 
#  start = 2020, #start date 
#  end = 2020) #end date 

#This takes some time, especially if you have more countries, more indicators and a longer time span.

#Checking it
#head(wb)

#Simulating the data 
wb <- data.frame(
  iso2c = c("BE", "BG", "CH", "EE", "FR", "GB"), 
  NY.GDP.PCAP.CD = c(45587.97, 10148.34, 85897.78, 23565.18, 39179.74, 40217.01), 
  TX.VAL.FUEL.ZS.UN = c(5.021, 4.644, 0.6111, 4.863, 1.886, 7.062), 
  EN.ATM.CO2E.KT = c(85364.10, 34138.10, 34916.10, 7097.52, 267154.70, 308650.30)
)

#Checking it
head(wb)
```

Let us transform the dataset (Only variables we need, arranging it alphabetically, renaming it and rounding one variable to make the numbers more intuitive):

```{r cleaning wb}
#Cleaning the wb data
wb <- wb %>%
  select(iso2c, NY.GDP.PCAP.CD, TX.VAL.FUEL.ZS.UN, EN.ATM.CO2E.KT) %>%
  arrange(iso2c) %>%
  rename(gdp_per_cap = NY.GDP.PCAP.CD,
         fuel_exp = TX.VAL.FUEL.ZS.UN,
         co2 = EN.ATM.CO2E.KT
         ) %>% 
  mutate(fuel_exp = round(fuel_exp, 2))

#Checking it
head(wb)
```

Now, we cut down and prepare our ESS data by selecting the countries we are interested in, renaming the country variable (I'll explain later why), we group by the country (iso2c) and the year (year) to get the average happiness by country. Lastly, we round the value to get only two decimals.

```{r making happy_agg, message=FALSE}
#preparing ess
d1 <- ess %>%
  filter(cntry == c("BE", "BG", "CZ", "EE", "FI")) %>%
  rename(iso2c = cntry) %>%
  group_by(iso2c, year) %>% 
  summarise(happy_agg = round(mean(happy), 2))
  
#Checking it
head(d1) 
```

### `left_join()` and `right_join()` with one identifier

To merge data there are important functions from the **dplyr** package: The `left_join()` and the `right_join()` function. Since they are a bit complicated to understand, we will go through them and in the end, you can choose the one you prefer.

-   `left_join()`: You want to keep all observations in the first table, including matching observations in the second table. You merging the data from the right table to left table and get a datset with the same number of rows as the left table:

![](images/left_join_with_one_identifier.PNG){fig-align="center" width="800"}

-   `right_join()`: You want to keep all observations in the second table, including matching observations in the first table. You join from the left table to the right table, this time the table takes on the number of observations of the table which is right joined.

![](images/right_join_with_one_identifier.PNG){fig-align="center" width="800"}

Well, in the end of the day it is a matter of programming socialisation and taste, which one do you prefer. I will show you both.

To merge two datasets, you need at least one common variable. One variable will be your unique identifier. Since every country is unique in the dataset that is our unique identifier i.e. the variable we give R to tell him how to merge the datasets:

-   First, we define a new object, where we will save the dataset called merged_data.

-   Second, we call left_join()

-   Then we set our left table and our right table.

-   We define the by = argument and put the unique identifier in quotation marks, meaning the variable name

```{r left_join, eval = FALSE}
#left_join
merged_data <- left_join(d1, wb, 
                         by = "iso2c")

#Checking it
head(merged_data)
```

For right_join() you do the exact same, but remember you get a different result.

```{r right_join}
#right_join
merged_data2 <- right_join(d1, wb, 
                           by = "iso2c")

#Checking it
head(merged_data2)
```

### `left_join()` and `right_join()` with two identifiers

Sometimes, you have multiple dimensions. For example, what if we also include the year? Then every country-year observation is our unique identifier. Why? Because one observation was collected in country X to time point Y. That is why you should always know your data and your research goal, because accordingly you have to write your code.

Let us get again World Bank Data and clean it:

```{r,  world bank 2}
#Getting the Data
#wb <- WDI( 
#  country = c("BE", "BG"), #We include our countries 
#  indicator = indicators, #We include our variables 
#  start = 2019, #start date 
#  end = 2020) #end date 

#Simulating the data 
wb <- data.frame(
  iso3c = c("BEL", "BEL", "BGR", "BGR"),
  iso2c = c("BE", "BE","BG", "BG"), 
  year = c(2019, 2020, 2019, 2020),
  NY.GDP.PCAP.CD = c(46641.72, 45587.97, 9874.336, 10148.34), 
  TX.VAL.FUEL.ZS.UN = c(7.38, 5.02, 9.53, 4.64), 
  EN.ATM.CO2E.KT = c(92989.4, 85364.10, 39159.9, 34138.10)
)

#Cleaning the Data
wb <- wb %>% 
  select(-iso3c) %>%
  arrange(iso2c) %>%
  rename(gdp_per_cap = NY.GDP.PCAP.CD,
         fuel_exp = TX.VAL.FUEL.ZS.UN,
         co2 = EN.ATM.CO2E.KT
         ) %>% 
  mutate(fuel_exp = round(fuel_exp, 2))

#Checking the Data
head(wb)
```

Now we just simulate some data we want to merge:

```{r, happy_agg for two years}
#Getting the Data
d1 <- data.frame(
  iso2c = c("BE", "BE", "BG", "BG", "CZ", "CZ"), 
  year = c(2019, 2020, 2019, 2020, 2019, 2020), 
  happy_agg = c(5.95, 6.76, 6.56, 7.54, 6.27, 6.88)
)

#Checking the Data
head(d1)
```

How does it look like when we have two variables and the combination out of those is our unique identifier?

-   `left_join()` with two identifiers:

![](images/left_join_with_two_identifiers.PNG){fig-align="center" width="800"}

-   As you can see the dataset again takes on the number of observations of our left table
-   To implement it, we have to change by argument. We define a vector, where we put our two identifiers in quotation marks:

```{r left_join multiple}
#Merging the Data with left_join()
merged_data3 <- left_join(d1, wb,
                          by = c("iso2c", "year"))
#Checking it
head(merged_data3)
```

-   `right_join()` with two identifiers:

![](images/right_join_with_two_identifier.PNG){fig-align="center" width="800"}

-   Again we do the same with `right_join()`:

```{r, right_join multiple}
merged_data4 <- right_join(d1, wb,
                          by = c("iso2c", "year"))
head(merged_data4)
```

*Note: However, this chapter only touched the basics and for merging alone there are several further commands, like inner_join(), anti_join(), semi_join()...etc. But when you encounter problems with the two functions I showed you will run into them eventually.*

## Outlook

This Chapter introduced you to the basic functions of the dplyr package. You are now able to transform variables according to your needs. Further, you learned how to use pipes to work efficient code. Loading data, transforming data, and preparing datasets for the analysis. There are a lot of techniques, and in the end of the day data wrangling is the most extensive part, because every analysis is individual and requires an individual preparation. The more individual the analysis, the more individual the preparation.

-   I recommend the standard book for data science in R in this chapter, since it has a strong emphasis on data manipulation: ["R for Data Science"](https://r4ds.had.co.nz/) by Hadley Wickham & Garrett Grolemund.

## Exercise Section

### Exercise 1: Let's wrangle kid

You are interested in discrimination and the perception of the judicial. More specifically, you want to know if people, who fell discriminated evaluate courts differently. Below you see a table with all variables you want to include in your analysis:

+--------------+--------------------------------------------------------+-------------------------------------------------------------------------------------------------------+
| **Variable** | **Description**                                        | Scales                                                                                                |
+:============:+========================================================+=======================================================================================================+
| **idnt**     | Respondent's identification number                     | unique number from 1-9000                                                                             |
+--------------+--------------------------------------------------------+-------------------------------------------------------------------------------------------------------+
| **year**     | The year when the survey was conducted                 | only 2020                                                                                             |
+--------------+--------------------------------------------------------+-------------------------------------------------------------------------------------------------------+
| **cntry**    | Country                                                | BE, BG, CH, CZ, EE, FI, FR,GB, GR, HR, HU, IE, IS, IT, LT,NL, NO, PT, SI, SK                          |
+--------------+--------------------------------------------------------+-------------------------------------------------------------------------------------------------------+
| **agea**     | Age of the Respondent, calculated                      | Number of Age = 15-90                                                                                 |
|              |                                                        |                                                                                                       |
|              |                                                        | 999 = Not available                                                                                   |
+--------------+--------------------------------------------------------+-------------------------------------------------------------------------------------------------------+
| **gndr**     | Gender                                                 | 1 = Male;                                                                                             |
|              |                                                        |                                                                                                       |
|              |                                                        | 2 = Female;                                                                                           |
|              |                                                        |                                                                                                       |
|              |                                                        | 9 = No answer                                                                                         |
+--------------+--------------------------------------------------------+-------------------------------------------------------------------------------------------------------+
| **happy**    | How happy are you                                      | 0 (Extremly unhappy) - 10 (Extremly happy);                                                           |
|              |                                                        |                                                                                                       |
|              |                                                        | 77 = Refusal;                                                                                         |
|              |                                                        |                                                                                                       |
|              |                                                        | 88 = Don't Know;                                                                                      |
|              |                                                        |                                                                                                       |
|              |                                                        | 99 = No answer                                                                                        |
+--------------+--------------------------------------------------------+-------------------------------------------------------------------------------------------------------+
| **eisced**   | Highest level of education, ES - ISCED                 | 0 = Not possible to harmonise into ES-ISCED;                                                          |
|              |                                                        |                                                                                                       |
|              |                                                        | 1 (ES-ISCED I , less than lower secondary) - 7 (ES-ISCED V2, higher tertiary education, =\> MA level; |
|              |                                                        |                                                                                                       |
|              |                                                        | 55 = Other;                                                                                           |
|              |                                                        |                                                                                                       |
|              |                                                        | 77 = Refusal;                                                                                         |
|              |                                                        |                                                                                                       |
|              |                                                        | 88 = Don't know;                                                                                      |
|              |                                                        |                                                                                                       |
|              |                                                        | 99 = No answer                                                                                        |
+--------------+--------------------------------------------------------+-------------------------------------------------------------------------------------------------------+
| **netusoft** | Internet use, how often                                | 1 (Never) - 5 (Every day);                                                                            |
|              |                                                        |                                                                                                       |
|              |                                                        | 7 = Refusal;                                                                                          |
|              |                                                        |                                                                                                       |
|              |                                                        | 8 = Don't know;                                                                                       |
|              |                                                        |                                                                                                       |
|              |                                                        | 9 = No answer                                                                                         |
+--------------+--------------------------------------------------------+-------------------------------------------------------------------------------------------------------+
| **trstprl**  | Most people can be trusted or you can't be too careful | 0 (You can't be too careful) - 10 (Most people can be trusted);                                       |
|              |                                                        |                                                                                                       |
|              |                                                        | 77 = Refusal;                                                                                         |
|              |                                                        |                                                                                                       |
|              |                                                        | 88 = Don't Know;                                                                                      |
|              |                                                        |                                                                                                       |
|              |                                                        | 99 = No answer                                                                                        |
+--------------+--------------------------------------------------------+-------------------------------------------------------------------------------------------------------+
| **lrscale**  | Left-Right Placement                                   | 0 (Left) - 10 (Right);                                                                                |
|              |                                                        |                                                                                                       |
|              |                                                        | 77 = Refusal;                                                                                         |
|              |                                                        |                                                                                                       |
|              |                                                        | 88 = Don't know;                                                                                      |
|              |                                                        |                                                                                                       |
|              |                                                        | 99 = No answer                                                                                        |
+--------------+--------------------------------------------------------+-------------------------------------------------------------------------------------------------------+

a.  Wrangle the data, and assign it to an object called **ess**.
b.  Select the variables you need
c.  Filter for Austria, Belgium, Denmark, Georgia, Iceland and the Russian Federation
d.  Have a look at the codebook and code all irrelevant values as missing. If you have binary variables recode them from 1, 2 to 0 to 1\
e.  You want to build an extremism variable: You do so by subtracting 5 from the from the variable and squaring it afterwards. Call it extremism
f.  Rename the variables to more intuitive names, don't forget to name binary varaibles after the category which is on 1
g.  drop all missing values
h.  Check out your new dataset

```{r ch3 exercise 1a, eval=FALSE}

```

### Exercise 2: Merging Datasets

The `gapminder` package in R loads automatically the gapminder dataset. The gapminder project is an independent educational non-profit fighting global misconceptions, check out their website: <https://www.gapminder.org/> The gapminder dataset is already loaded.

a.  Get an overview of the gapminder dataset. There are different ways to do so, you can choose by yourself

```{r ch3 exercise 1b, eval=FALSE}

```

b.  Load World Bank Data from 1972 to 2007 and load the variable "Exports and Goods (% of GDP)".
c.  Merge the World Bank data to the gapminder data, so a dataset evolves with the number of observations of the gapminder data.

```{r ch3 exercise 1c, eval=FALSE}
  
```

d\. Clean the data by dropping all missing values

```{r ch3 exercise 1d, eval=FALSE}

```

<!--chapter:end:03-data-manipulation.Rmd-->

# Data Visualisation

In this chapter, I will introduce you to the most fun part in R, data visualisation (at least in my opinion). I will introduce you to the basic graphs and how to plot them in R. To do so, I introduce you the framework of `ggplot2`. This is a quite famous and powerful package and it became the standard package of data visualization in R. The goal of this chapter is to show you the basic plots everyone should know about, how you can program nice plots to include them in your papers and reports, and how to work with the `ggplot2` package.

```{r loading packages viz}
pacman::p_load("tidyverse", "babynames", "sf", "ggridges",
               "rnaturalearth", "rnaturalearthdata" ,"forcats" ,"tmap")
```

## Introduction to `ggplot2`

The `tidyverse` includes the most popular package for data visualization in R, `ggplot2`. With its relative straight forward code and its huge flexibility, and I mean HUGE FLEXIBILTY, it became the standard form of Data Visualization. It is aims to simplify data visualization by utilizing the "Grammar of Graphics" defined by Leland Wilkinson. While it may appear complicated at first, it just creates a frame and adds elements to it.

Let us start by looking at the code structure and creating the frame. The central code here is `ggplot()`:

```{r basic ggplot}
ggplot()
```

As we can see, we get an empty frame and in the following we will go through the standard forms of data visualizations by simply adding elements to this empty frame. But this is only the Peak of what is possible with Data Visualization in R. I strongly recommend to further work on this topic for two reasons, especially R is the perfect language to dive deeply in this topic. R is known for beautiful data visualizations and it is a reason for its popularity.

## Distributions: Histogram, Density Plots, and Boxplots

The first type of visualizations are displaying distributions. We should always get an overview of how our variables are distributed, because the distributions gives us valuable information about the data structure. For example a lot of statistical models assume certain distributions and to identify if we can test data with those models, we have to make sure that it does not violate the distribution assumption. Further, distributions make it easy to detect outliers or biases, since they are easy to spot with such visualizations.

### Histograms

#### Basic Histogram

Let us start with a normal Histogram. A histogram is an accurate graphical representation of the distribution of a numeric variable. It takes as input numeric variables only. The variable is cut into several bins, and the number of observation per bin is represented by the height of the bar.

Before making our first plot, let us simulate some data:

```{r, simulating data distributions}
#Setting Seed for reproducibility
set.seed(123)

#Simulating data 
data1 <- data.frame(
  type = c(rep("Variable 1", 1000)),
  value = c(rnorm(1000))
)

#Looking at the data 
glimpse(data1)

```

We now have a dataset for a random variable called "Variable 1" and this variable has 500 values assigned to it. We now want to know the distribution of these values and decide to plot a histogram.

Now we have data and we can go straight to business. For a histogram in `ggplot`, we need the `ggplot()` command. Afterward, we include our dataset, in our case `data`. We use a comma in the `ggplot()` command after the data and add a new command, called `aes()`. In this command we need to define **the x-axis** and **the y-axis**. Here we just need the x-axis, since a histogram logically plots the "count" thus how often one value appears in the dataset, `ggplot` does that automatically. Last thing remaining is to close the bracket of `aes()` and of the `ggplot()` command and to tell ggplot, what kind of visualization we want. Our answer comes with a "`+`" after the closed command and we add the command `geom_histogram()`.

```{r basic histogram}
ggplot(data1, aes(x = value)) + 
  geom_histogram()
```

And you just made your first histogram. But as you can see, it does not look nice. The reason is that we have to tell ggplot2 what we specifically want to change. And we can do so by defining the inside of the `geom_histogram()` function. I guess the first step is to make the bins visible and to change the color from gray to something nicer. We can do so by the defining the color for the borders of the bins, and the fill command to change the color of the bins in the `geom_historgram()` function. Let us set it to white to make it visible.

*Note: I could have defined any color, the only condition is to put it in quotation marks. Some colors such as white can be just written down, but you can always use any hexcode inside the quotation marks and it will work fine.*

```{r histogram with bins}
ggplot(data1, aes(x = value)) + 
  geom_histogram(color = "white", fill = "#69b3a2")
```

Looks better! But still, we have to think about that we want to publish this in an article or report. And for this purpose it is not sufficient. Next we should change the names of the labs, we can do so by adding a plus + again after the `geom_histogram()` command and using the `labs()` function. In this function we define the name of our x-axis and the y-axis. While we are at it, we can define the title in this function as well. What I like to do next is to scale the x-axis and to have ggplot display the values of each of the horizontal grid lines. Here an important mechanic is needed. The code `scale_x_continous()` helps us to rescale the x-axis. In general, the family of `scale_*` functions are powerful, because re-scaling the axis can (must not necessarily) change the visualization, thus these are powerful tools we should be aware of:

```{r histogram with rescaled x-axis, warning=FALSE, message=FALSE}
ggplot(data1, aes(x = value)) + 
  geom_histogram(color = "white", fill = "#69b3a2") + 
  labs( 
    x = "Value", 
    y = "Count", 
    title = "A Histogram") + 
  scale_x_continuous(breaks = seq(-4, 4, 1), 
                     limits = c(-4, 4))
```

I do not know about you, but I have a huge problem with the gray grid as a background. This is the default grid by ggplot2 and we can change that. Again, we need a "`+`", and then we can just add the function without any things in it. I decided for the `theme_bw()` function, which is my favorite theme, but I found a website, where you can have a look at the different themes, look [here](https://ggplot2.tidyverse.org/reference/ggtheme.html).

```{r histogram with theme, warning=FALSE}
ggplot(data1, aes(x = value)) + 
  geom_histogram(color = "white", fill = "#69b3a2") + 
  labs( 
    x = "Value", 
    y = "Count", 
    title = "A Histogram") + 
  scale_x_continuous(breaks = seq(-4, 4, 1), 
                     limits = c(-4, 4)) +
  theme_minimal()
```

Well, we did it. I think that this plot can be displayed in an article or report. Good job!

One elemental thing I want to talk about is the width of the size. Currently, the binwidth is at 0.3. We can adjust that by including binwidth in the geom_histogram() command:

```{r histogram with different binwidth, warning=FALSE}
#histogram bindwidth = 0.1
ggplot(data1, aes(x = value)) + 
  geom_histogram(color = "white", fill = "#69b3a2", 
                 binwidth = 0.1) + 
  labs( 
    x = "Value", 
    y = "Count", 
    title = "A Histogram with binwidth = 0.1") + 
  scale_x_continuous(breaks = seq(-4, 4, 1), 
                     limits = c(-4, 4)) +
  theme_minimal()

#histogram with bindwidth = 0.6
ggplot(data1, aes(x = value)) + 
  geom_histogram(color = "white", fill = "#69b3a2", 
                 binwidth = 0.6) + 
  labs( 
    x = "Value", 
    y = "Count", 
    title = "A Histogram with binwidth = 0.6") + 
  scale_x_continuous(breaks = seq(-4, 4, 1), 
                     limits = c(-4, 4)) +
  theme_minimal()
```

#### Multiple Histograms

In this part, I want to show you variations of the Histogram visualization plot. We will start with multiple distributions we probably want to display. To do so, we need a new variable we will call "Variable 2", with its own observations and add it to our dataset:

```{r adding Variable 2 to our dataset}
#Creating data
data2 <- data.frame(
  type = c(rep("Variable 2", 1000)), 
  value = c(rnorm(1000, mean = 4))
)

#rowbinding it with data1
data2 <- rbind(data1, data2)
```

We have two variables, each with their own distribution. We have to tell ggplot2 to distinguish the numbers by the different variables. We do so by modifying the inside of the `aes()` function. Our x-axis stays the same, right? We still want the values to be on the x-axis, so that parts stays the same. We define the fill within the `aes()` command to tell ggplot to fill the values of the two variables. Additionally, I will specify position = "identity" in the plot, this specification helps to adjust the position, when two histograms are overlapping, which will be the case.

*Note: I leave out the \`fill\` specification for the reason that the colors are defined by default for both graphs (but we can change that, I will show that later).*

```{r, two histograms in one}
ggplot(data2, aes(x=value, fill=type)) +
    geom_histogram(color="#e9ecef",
                   position = "identity") +
  theme_bw() 
```

As you can see, we get two plots colored by the type there are assigned to. We can now play around a bit. I want to introduce you the alpha specification. This makes colors more transparent. Again this a command should be used if objects are overlapping to have a clearer picture of the overlap.

Additionally, I will scale new colors, here the scale\_\* function family comes again into play. We will use the scale_fill_manual command, since we want to change the color of the fill specification in the aes() command:

```{r histogram two groups}
ggplot(data2, aes(x=value, fill=type)) +
  geom_histogram(color="#e9ecef", 
                 alpha = 0.6, 
                position = "identity") +
  scale_fill_manual(values = c("#8AA4D6", "#E89149")) +
  theme_bw() 
```

### Density Plots

A density plot is a representation of the distribution of a numeric variable. It uses a kernel density estimate to show the probability density function of the variable. It is basically a smoothed version of a histogram. Since the logic is the same, except that the `geom_histogram()` is changed with `geom_density()`.

#### Basic Density Plot

Let us start with a basic density plot:

```{r basic density}
ggplot(data1, aes(x = value)) + 
  geom_density()
```

Well, we now can do the exact same things as we did above: Fill the density plot with a color with `fill()`, make the fill color more transparent with `alpha()` and change the color of the line with `color()` in the `geom_density()` function. We can rescale the x-axis with `scale_x_continous`, and we can change the labels of the axis with `labs()`, and change the theme to `theme_minimal()`.

```{r density color}
ggplot(data1, aes(x = value, fill =)) + 
  geom_density(color = "white", 
               fill = "orange",
               alpha = 0.6) + 
  labs( 
    x = "Value", 
    y = "Count", 
    title = "A Density Plot") + 
  scale_x_continuous(breaks = seq(-4, 4, 1), 
                     limits = c(-4, 4)) +
  theme_minimal()
```

#### Multiple Density Plots

We could also do this with multiple density plots, remember that we always need the data structure to plot a graph. For this reason we again need `data3`. The rest stays again the same as with histograms:

*Note: I just copied the code from above, changed the geom_histogram() to geom_density() and then I just changed the colors, the alpha and the theme. That's it. And that is mostly how plotting works, just copy and paste from the internet, and adjust what you do not like.*

```{r density two groups}
ggplot(data2, aes(x=value, fill=type)) +
  geom_density(color="#0a0a0a", 
                 alpha = 0.9, 
                position = "identity") +
  scale_fill_manual(values = c("#FDE725FF", 
                               "#440154FF")) +
  theme_minimal() 
```

### Boxplots

#### Basic Boxplots

The last visualization form of distributions are Boxplots. Boxplots are a really interesting form of showing distributions with a lot of information. Let us have a look at their anatomy, before I show you how to program them:

![Anatomy of a Boxplot](images/Boxplot_graph.jpg){fig-align="center" width="647"}

-   **The black rectangle** represents the Interquartile Range (IQR), thus the difference between the 25th and 75th percentiles of the data

-   **The red line** in the black rectangle represents the median of the data.

-   **The end of the lines** show the value at the 0th percentile, respectively 100th percentile, thus the minimum and the maximum value of the IQR, not the data.

-   **The dots beyond the black lines** are potential outliers and the points at the ends are the minimum value, respectively maximum value in the data. We should be aware of them, because if we ignore them, they could bias our statistical models, but more to that in Chapter 6.

Let us implement a boxplot in R. Again the only thing that changes is that we use the standard ggplot() function and go on with the function geom_boxplot():

```{r basic boxplot}
ggplot(data1, aes(x = value)) + 
  geom_boxplot()
```

We can also make that graph pretty with the same techniques as above:

```{r boxplot example}
ggplot(data1, aes(x = value)) + 
  geom_boxplot() + 
    labs( 
    x = "Value", 
    y = "Count", 
    title = "A  Boxplot") + 
  scale_x_continuous(breaks = seq(-4, 4, 1), 
                     limits = c(-4, 4)) +
  theme_classic()
```

#### Multiple Boxplots

A huge advantage of Boxplots are that it is an easy way to compare the structure of distributions of different groups. Consider following example: We want to compare the income of people with migration background and people without migration background. Let us say we collected a sample of people with 2000 respondents, 1000 with and 1000 without migration background. We further collected the incomes of each respondent. Be aware that we now need to define the y-axis with income. Since we do not look anymore at the count of the distribution, but the distribution over another variable (here:income). Let us look at the plot:

```{r simulating data for second boxplot}
# Set seed for reproducibility
set.seed(123)

# Simulate income data
income_18_24 <- rnorm(1000, mean = 40000, sd = 11000)
income_25_34 <- rnorm(1000, mean = 55000, sd = 17500)
income_35_59 <- rnorm(1000, mean = 70000, sd = 25000)

# Combine into a data frame
data3 <- data.frame(
  income = c(income_18_24, income_25_34, income_35_59),
  age = factor(rep(c("18-24", "25-34", "35-59"), 
                                each = 1000))
)
```

```{r grouped boxplot}
ggplot(data3, aes(x = age, 
                 y = income, fill = age)) +
geom_boxplot()  
```

Before interpreting the plot, let us make it prettier: We change labels of the x-axis, y-axis and give the plot a title with the `labs()` function. I do not like the colors, we change them with the `scale_fill_manual()`. Again, we define `alpha = 0.5` and also `width = 0.5` of the boxes in `geom_boxplot()`. I also think, we do not need a legend, therefore we can remove it, and use the `theme()` function. This function is powerful, since its specification gives us a lot of possibilities to design the plot according to our wishes. We specify in the `theme()` function that `legend.position = "none"`, which means that we do not want the legend to be displayed at all:

```{r multiple boxplots}
# Create boxplot
ggplot(data3, aes(x = age, y = income, fill = age)) +
  geom_boxplot(alpha = 0.5, width = 0.5) +
  scale_fill_manual(values = c("#acf6c8", "#ecec53" ,"#D1BC8A")) +
  labs(
    title = "Comparison of Income Distribution by Age",
    x = "Age",
    y = "Income"
  ) +
  theme_minimal() +
  theme(legend.position = "none")
```

We have a lot of information here. First, we clearly see that the median of people with migration background is lower than the median income of people without migration background. But we further see, that the income distribution of respondents without migration background is more spread out over a higher range. We can see that by the longer lines of the boxplot of respondents without migration background. Also the IQR range of both variables are varying. The box of people without migration background is again smaller, which again is an indicator that respondents without migration background are more spread out. In comparison, we can see that respondents with migration background in the 50th -75th percentile earn as much as respondents without migration background in the 25th to 50th percentile.

I could go on the whole day, boxplots are very informative and a nice tool to inspect and compare distribution structures.

*Note: I used simulated data, therefore this data is fictional.*

## Ranking: Barplot

### Basic Barplot

The most famous, and easiest way of showing values of different groups is the Barplot. A barplot (or barchart) is one of the most common types of graphic. It shows the relationship between a numeric and a categoric variable. Each entity of the categoric variable is represented as a bar. The size of the bar represents its numeric value.

-   In ggplot, we only have to define the x-axis, and y-axis inside the ggplot() function, and add the function geom_bar(). Inside geom_bar() you have to add stat = "identity", for the simple reason, that we have to tell ggplot2 to display the numbers of the column "strength", otherwise it will give us an error.

```{r barplot data}
# Create data
data4 <- data.frame(
  name=c("King Kong","Godzilla","Superman",
         "Odin","Darth Vader") ,  
  strength=c(10,15,45,61,22)
  )

#Plotting it 
ggplot(data4, aes(x = name, y = strength)) + 
  geom_bar(stat = "identity")
```

Again, we can change the look of our plot. We start by changing the color by setting color within the `geom_bar()` function, we set a theme, let us do `theme_test()` this time and we change the names of the columns with the `labs()` function.

*Note: I can disable the name of the x-lab by simply adding empty quotation marks in the `labs()` function*

```{r basic barplot}
ggplot(data4, aes(x = name, y = strength)) + 
  geom_bar(stat = "identity", fill = "#AE388B") +
  labs(
    x = "", 
    y = "Strength", 
    title = "Strength of fictional Characters"
  ) + 
  theme_test()
```

There is also another possibility to use Barplots. We could use them to count categories. Like we would in a histogram with the difference that we now have not a range of numbers, where we count how many numbers for one variable. We have a groups and want to count how often those groups appear in our dataset. Let us assume we asked 20 kids what their favorite fictional character is among Superman, King Kong and Godzilla.

```{r barplot count data}
data5 <- data.frame(
  hero = c(rep("Superman", 10), 
           rep("King Kong", 3), 
           rep("Godzilla", 7)), 
  id = c(seq(1:20)), 
  female = c(rep("Female", 7), 
             rep("Male", 5), 
             rep("Female", 1), 
             rep("Female", 3), 
             rep("Male", 4))
) 
```

```{r barplot count}
ggplot(data5, aes(x = hero)) + 
  geom_bar(fill = "#AE388B") +
  labs(
    x = "", 
    y = "Count", 
    title = "What is your favourite fictional Character?"
  ) + 
  scale_y_continuous(breaks = seq(0,10,1)) +
  theme_test()
```

We could also turn around both Barplots to have a vertical Barplot. That is quite easy, we just have to add the coord_flip() function. This function swaps the x-axis and the y-axis.

Let us look at the plots:

```{r barplot flipped}
#Plot 1 
ggplot(data4, aes(x = name, y = strength)) + 
  geom_bar(stat = "identity", fill = "#AE388B") +
  labs(
    x = "", 
    y = "Strength", 
    title = "Strength of fictional Characters"
  ) + 
  theme_test() + 
  coord_flip()

#Plot 2 
ggplot(data5, aes(x = hero)) + 
  geom_bar(fill = "#AE388B") +
  labs(
    x = "", 
    y = "Count", 
    title = "What is your favourite fictional Character?"
  ) + 
  scale_y_continuous(breaks = seq(0,10,1)) +
  theme_test() + 
  coord_flip()
```

### Reordering them

To make a Barplot more intuitive, we can order it so the bar with the highest x-value is at the beginning and then it decreases or vice versa.

-   To do so, we use the `forcats` package

-   We take the code from above and wrap the x-value in the `fct_reorder()` command and determine the value it should be reorder based on, in our case the x-value is the name of the fictional characters and the value is the strength or the count:

Note: You could also do it in descending order by just wrapping a `desc()` around the value the variable should be reorder based on thus it would look like this: `fct_reorder(name, desc(strength))`.

```{r barplot ordered}
#Plot 1
ggplot(data4, aes(x = fct_reorder(name, strength), y = strength)) + 
  geom_bar(stat = "identity", fill = "#AE388B") +
  labs(
    x = "", 
    y = "Strength", 
    title = "Strength of fictional Characters"
  ) + 
  theme_test()

#Plot 2
ggplot(data4, aes(x = fct_reorder(name, strength), y = strength)) + 
  geom_bar(stat = "identity", fill = "#AE388B") +
  labs(
    x = "", 
    y = "Strength", 
    title = "Strength of fictional Characters"
  ) + 
  theme_test() + 
  coord_flip()
```

### Grouped and Stacked Barplots

We can go a step further with barplots and group them. Let us assume we asked respondents to tell us how healthy they feel on a scale from 0-10. But we want to separate respondents older than 40 and younger than 40. And we again separate the group between female and male respondents. Therefore we look at the average answer of 4 groups: Female, older 40, Male, older 40, Female younger 40 and Male younger 40. To see if there are gender differences within these groups. Let us get the data:

```{r data grouped barplot}
data6 <- data.frame(
  female = c("Female", "Male", "Female", "Male"), 
  age = c("Old", "Old", "Young", "Young"), 
  value = c(5, 2, 8, 7)
)
```

Now we got the data. We have to define 3 parameters within aes(). The x-axis is the age groups, the y-axis the average value, and we have to define fill = female, since this is our group we want to investigate within the age groups. Inside geom_bar(), we need two arguments stat = "identity" and position = dodge. Et voila we will get our first **grouped barplot.**

```{r grouped barplot dodged}
ggplot(data6, aes(x = age, y = value, fill = female)) + 
    geom_bar(position = "dodge", stat="identity") 
```

We could have also used the a **stacked barplot**. The difference is, that we have one bar for our x-axis group, in our example the age group, and then the amount of the second group, the gender, is stacked on top of each it other. You could also see that as a normal barplot, where the bar is colored depending on the percentual distribution of the other group. In the code the only thing changing is that we set the position argument in the geom_bar() code to position = "stack":

```{r grouped barplot stacked}
ggplot(data6, aes(x = age, y = value, fill = female)) + 
    geom_bar(position = "stack", stat="identity") 
```

Let us make them pretty with our well-known techniques, it is always the same story. But twonew thing are introduced

-   The argument `width = 0.35` is included to the `geom_bar()` so we can determine the width of the bars

-   I introduce you so-called color palettes. Instead of manually scaling the color, you can use built-in color palettes for different types of plots. For Barplot you can use the scale_fill_brewer, which includes different palettes and colors, which are automatically displayed. Have a look at the palettes of the command [here](https://r-graph-gallery.com/38-rcolorbrewers-palettes.html). That can be really helpful, if you have a lot of groups, so you do not have to think about different colors, which look good together.

```{r bar plot with color palettes}
#Plot 1
ggplot(data6, aes(x = age, y = value, fill = female)) + 
    geom_bar(position = "dodge", stat="identity", 
             width = 0.35) + 
  scale_fill_brewer(palette = "Accent") +
  scale_y_continuous(breaks = seq(0, 15, 1)) + 
  labs(
    x = "Age Cohort", 
    y = "Average Score Well-Being", 
    title = "Impact of Age on Well-Being by
    Gender"
  ) +
  theme_minimal() + 
  theme(legend.title=element_blank())

#Plot 2
ggplot(data6, aes(x = age, y = value, fill = female)) + 
    geom_bar(position = "stack", stat="identity", 
             width = 0.35) +
  scale_fill_brewer(palette = "Accent") +
  scale_y_continuous(breaks = seq(0, 15, 2)) + 
  labs(
    x = "Age Cohort", 
    y = "Average Score Well-Being", 
    title = "Impact of Age on Well-Being by Gender"
  ) +
  theme_minimal() + 
  theme(legend.title=element_blank())
```

## Evolution: Line Chart

A quite familiar plot is the line chart. A quite popular way of showing the evolution of a variable over a variable on the x-axis. We know them mostly from time series analyses, where a certain period is on the x-axis. Since such line charts with dates are well known, I will stick with them as an example. A line chart or line graph displays the evolution of one or several numeric variables. Data points are connected by straight line segments the measurement points are ordered (typically by their x-axis value) and joined with straight line segments.

### Basic Line Plot

In ggplot, we stick with the ggplot() function, define our x-axis and our y-axis. We add the function geom_line() to it.

```{r simulating data for line plot}
# Setting Seed
set.seed(500)
# create data
date <- 2000:2024
y <- cumsum(rnorm(25))
y2 <- cumsum(rnorm(25))
data7 <- data.frame(date,y, y2)
```

```{r basic line plot}
ggplot(data7, aes(x = date, y = y)) + 
  geom_line()
```

Normally we would go on and make the plot pretty. But there are additional aesthetics to a line plot.

-   First, we can change the line type. The line type can be straight as in the default layout, but I will change set it in the `geom_line()` command to line `type = "dashed"`. For an overview of all line types look [here](http://www.sthda.com/english/wiki/ggplot2-line-types-how-to-change-line-types-of-a-graph-in-r-software).

-   Second, I change the size of the line with setting `size = 1` in the `geom_line()` command.

-   The rest of the aesthetics are stay the same, re-scaling axes, coloring, and themes.

```{r dashed line plot}
ggplot(data7, aes(x = date, y = y)) + 
  geom_line(color = "#0F52BA", linetype = "dashed",
            linewidth = 1) + 
  scale_y_continuous(breaks = seq(-1, 6, 1), 
                     limits = c(-1, 6)) + 
  scale_x_continuous(breaks = seq(2000, 2024, 2)) + 
  labs(
    y = "",
    x = "Year", 
    title = "A Line Plot"
  ) +
  theme_bw()
```

### Multiple Line Chart

In the next step, we want to plot multiple lines in one plot. This is useful when we want to compare the evolution of variables for example over time. In `ggplot2` we only need to add another layer with a plus and add another `geom_line()` command. But now things get a bit complicated:

-   Inside the ggplot() command we only add our dataset with our dataset, nothing more.

-   In the first geom_line() command we add the aes() function and define x and y. Until now, we only wrote the aes() function inside the ggplot() function, but now we have to write it in the `geom_line()` function, since we add another geom_line() layer.

-   In the second geom_line() command we define the our next layer. This time the x-axis stays the same logically. But now we change y and set it to the second variable we want to inspect.

```{r multiple lines}
ggplot(data7) + 
  geom_line(aes(x = date, y = y)) +
  geom_line(aes(x = date, y = y2))
```

As always, we make the plot pretty in the next step. I will use the same code as above. But regarding the lines itself, we can separate the aesthetics separately:

-   We can set the line type, color and size differently for each layer. We just have to specify it inside the `geom_line()` command for the respective layer.

```{r multiple colored lines}
ggplot(data7) + 
  geom_line(aes(x = date, y = y), 
            linetype = "twodash", 
            size = 1, 
            color = "#365E32") +
  geom_line(aes(x = date, y = y2), 
            linetype = "longdash", 
            size = 1,
            color = "#FD9B63") +
  scale_y_continuous(breaks = seq(-5, 6, 1), 
                     limits = c(-5, 6)) + 
  scale_x_continuous(breaks = seq(2000, 2024, 2)) + 
  labs(
    y = "",
    x = "Year", 
    title = "A Line Plot"
  ) +
  theme_bw()
```

### Grouped Line Charts

Another possibility of using line charts is to look at the evolution of groups separately. I introduce you to the `babynames` dataset, which is a package in R, which loads automatically the dataset about the most popular babynames in the US from 1880 until 2017. Let us have a look at it:

```{r babynames}
###Looking at the dataset
head(babynames)
```

Well, let us say we are interested in the popularity of the names Michael, Abby, and Lisa. Let us cut down the dataset to these three names with the filter() function you learned in the previous chapter:

```{r cleaning babynames}
babynames_cut <- babynames %>%
  filter(name %in% c("Emma", "Kimberly", "Ruth")) %>%
  filter(sex == "F")
```

In the next step, let us plot the popularity of these three names over time.

-   We have to specify the x and y-axis and further add a `geom_line()` layer. So far, so normal. The next thing we do, is to tell ggplot2 that we want groups. We do so, in the `ggplot()` function by setting `group = name`. We should also set the `colors = name`, otherwise all lines will be black and we cannot distinguish, which line belongs to which group.

```{r basic line plot groups}
ggplot(babynames_cut, aes(x = year, y = n,
                      group = name,
                      color = name)) + 
  geom_line()
```

Well, that looks good, we can see that Ruth had its peak in the 20s, Kimberly in the 60s and Emma is currently on the rise. Let us design the plot with a theme, remove the legend title, add some meaningful lab names and add a color palette with `scale_color_brewer()`.

-   Regarding the labs, I will introduce you a way of re-naming the legend, by simply setting `color = "New Name"` in the `labs()` function

```{r colored line plot}
ggplot(babynames_cut, aes(x = year, y = n,
                      group = name,
                      color = name)) + 
  geom_line(size  = 1) + 
  scale_color_brewer(palette = "Set1") + 
  labs(
    x = "Year", 
    y = "Number of Babies named", 
    title = "Popularity of Babynames over time",
    color = "Name"
  ) +
  theme_minimal() 
```

## Correlation: Scatterplots

The last type of visualization are scatter plots. A Scatter plot displays the relationship between 2 numeric variables. Each dot represents an observation. Their position on the X (horizontal) and Y (vertical) axis represents the values of the 2 variables. It is a quite popular way in articles to investigate the relationship between two variables.

### Basic Scatterplot

We want to investigate the relationship between two variables. Let us assume we are the owner of a big choclate company. We want to find the out the relationship of our marketing spendings on the sales of our chocolate. We have the data for each quarter of the year and for years:

```{r data for scatterplot}
# Set the seed for reproducibility
set.seed(123)

# Simulate data
n <- 100
marketing_budget <- runif(n, min = 1000, max = 10000)
sales <- 2000 + 0.65 * marketing_budget + 
  rnorm(n, mean = 1400, sd = 750)
quarters <- rep(c("Q1", "Q2", "Q3", "Q4"), 25)

# Create a data frame
data_point <- data.frame(marketing_budget, sales, 
                         quarters)

#Give it a name
data_point$name <- "Chocolate Milk"
```

A scatter plot in R is made with the same logic as always.

-   First, we define our x and y-axis in the `ggplot()` command.

-   We add a comma and call the `geom_point()` function

```{r basic scatterplot}
ggplot(data_point, aes(x = marketing_budget, 
                       y = sales)) +
  geom_point()
```

Let us make the plot pretty and as always, we define a color for the dots in the layer, thus the geom_point() function, re-scale the axes (in this case I would just re-scale the x-axis), re-name the labels, give a title and define a theme.

```{r scatterplot milk}
ggplot(data_point, aes(x = marketing_budget, 
                       y = sales)) +
  geom_point(color = "#99582a") +
  scale_x_continuous(breaks = seq(0, 10000, 2000)) + 
  labs(
    x = "Marketing Budget", 
    y = "Sales per Unit", 
    title = "Chocolate Milk Sales and Marketing"
  ) + 
  theme_classic() 
```

### Scatter Plots with multiple Groups

Let us go on with our example. We do not only have one sort of chocolate but two. Chocolate milk and dark chocolate. Let us get the data for dark chocolate as well:

```{r more data for scatterplot}
# Set the seed for reproducibility
set.seed(123)

# Simulate data
n <- 100
marketing_budget <- runif(n, min = 1000, max = 10000)
sales <- 1500 + 0.3 * marketing_budget + rnorm(n, mean = 1400, sd = 750)
quarters <- rep(c("Q1", "Q2", "Q3", "Q4"), 25)

#Making a df 
df_dark <- data.frame(marketing_budget, sales, quarters)

#Give it a name
df_dark$name <- "Dark Chocolate"

#rowbind it with the other dataset 
data8 <- rbind(data_point, df_dark)
```

Now, we could run the same code as above, but we would not be able to distinguish, which dots belong to which chocolate.

-   That is the reason we need to specify in the `aes()` function the argument `color = name`. That will color the dots in the group they belong to.

-   I will manually give the colors, since I have to use brown colors for this example.

```{r chocolate milk and marketing}
ggplot(data8, aes(x = marketing_budget, 
                       y = sales, 
                       color = name)) +
  geom_point() +
  scale_color_manual(values = c("#e71d36",
                                "#260701"))+
  scale_x_continuous(breaks = seq(0, 10000, 2000)) + 
  labs(
    x = "Marketing Budget", 
    y = "Sales per Unit", 
    title = "Chocolate Milk Sales and Marketing",
    color = "Product"
  ) + 
  theme_classic() 
```

As we can see, in general marketing leads to higher sales of chocolate. Further we can see that Marketing has a higher effect on Chocolate milk than on Dark Chocolate.

Using colors is one way to differentiate between groups in scatter plots.

-   Another way is to use different shapes. The only thing we have to change the color argument with a the `shape` argument.

-   We can also adjust the size and I want to do that, since I want to make the forms more visible. Since this changes the design of the points, we have to set the argument `size = 2.5` inside the `geom_point()` function.

-   In the `labs()` function we change the argument color = "Product" to shape = "Product", because we now name the legend of the shape layer, and not the color layer.

Let us have a look:

```{r scatterplots with shapes}
ggplot(data8, aes(x = marketing_budget, 
                       y = sales, 
                       shape = name)) +
  geom_point(size = 2.5) +
  scale_x_continuous(breaks = seq(0, 10000, 2000)) + 
  labs(
    x = "Marketing Budget", 
    y = "Sales per Unit", 
    title = "Chocolate Milk Sales and Marketing",
    shape = "Product"
  ) + 
  theme_classic() 
```

There are different types of shapes and we can set them manually via numbers.

-   For this purpose we can use the `scale_shape_manual()` and call the argument `size = 4`. There are different shapes and they have numbers assigned to them, to call them we have to set size equal to the number of the shape. Check out [this](http://www.sthda.com/english/wiki/ggplot2-point-shapes) website for an overview over the different shapes.

-   We can also combine different colors with different shapes. We just leave the `color = name` argument in the `ggplot()` function.

-   In the `labs()` function we will set the argument to `color = ""` and `shape = ""`. So the legend shows the colored shape as the legend.

```{r colors and shapes}
ggplot(data8, aes(x = marketing_budget, 
                       y = sales, 
                       shape = name,
                       color = name)) +
  geom_point(size = 2.5) +
  scale_color_manual(values = c("#e71d36",
                                "#260701")) +
  scale_x_continuous(breaks = seq(0, 10000, 2000)) + 
  labs(
    x = "Marketing Budget", 
    y = "Sales per Unit", 
    title = "Chocolate Milk Sales and Marketing",
    shape = "",
    color = ""
  ) + 
  theme_classic() 
```

## Making Plots with `facet_wrap()` and `facet_grid()`

Sometimes we do not want to compare the elements in a plot (e.g. dots, lines), but the plot itself with other plots from the same dataset. This can be a powerful tool, in terms of telling a story with data. Further, we can gain several information by splitting the data into graphs and directly comparing them.

### The `facet_wrap()` function

That is rather abstract, let us stick with our chocolate company. We want to compare the effect of our marketing budget on sales for different quarters. We want to plot the same scatter plot as before, but this time for each quarter. We could of course split up the data set to each quarter and plot 4 plots. But that is not efficient. Let us copy the code from above for the basic plot, and just add the `facet_wrap()` function and inside this wave symbol `~` and add the variable we want separate for, in our case the quarters.

```{r basic facet_wrap}
#Basic facet_wrap() function
ggplot(data8, aes(x = marketing_budget, 
                       y = sales)) +
  geom_point() + 
  facet_wrap(~ quarters)
```

As you can see ggplot2 plots 4 graphs for each quarter. Instead of plotting 4 graphs and writing unnecessary long code, we can use the handy facet_warp() function. If we want to make the graph pretty, it is quite easy, since it is identical as if we want to make a single plot pretty. Thus, we can just copy the code from above and include it:

```{r facet_wrap colors}
ggplot(data8, aes(x = marketing_budget, 
                       y = sales)) +
  geom_point(color = "#99582a") +
  scale_x_continuous(breaks = seq(0, 10000, 2000)) + 
  labs(
    x = "Marketing Budget", 
    y = "Sales per Unit", 
    title = "Chocolate Milk Sales and Marketing"
  ) + 
  theme_classic() + 
  facet_wrap(~ quarters)
```

We can also add a facet_wrap() function for our plot with different shapes and colors for chocolate milk and dark chocolate:

```{r facet_wrap with shape and colors}
ggplot(data8, aes(x = marketing_budget, 
                       y = sales, 
                       shape = name,
                       color = name)) +
  geom_point(size = 2.5) +
  scale_color_manual(values = c("#e71d36",
                                "#260701")) +
  scale_x_continuous(breaks = seq(0, 10000, 2000)) + 
  labs(
    x = "Marketing Budget", 
    y = "Sales per Unit", 
    title = "Chocolate Milk Sales and Marketing",
    shape = "",
    color = ""
  ) + 
  theme_classic() +
  facet_wrap(~ quarters)
```

### The `facet_grid()` function

The facet grid function does the same as the facet_wrap() function, but it allows to add a second dimension. Image we want to know the development of the temperature for the first four months of the years 2018, 2019, 2020 of the cities London, Paris and Berlin. This time, we decide for a line chart to visualize the evolution of the temperatures. Manually we would have to make nine plots, For each city one plot for each year. Or we just use the `facet_grid()` function:

-   Since we have two dimensions, we have to define them. We define the row and then we define the column and separate them with this wave symbol `~` , thus `facet_wrap(row ~ column)`

-   We use the geom_line() function and make the plot pretty by giving meaningful label names, coloring each year with a unique color, giving a title, defining a theme and hiding the legend, since it would only show that the years have unique colors.

```{r data facet_grid}
#Set seed for reproducilty
set.seed(123)

# Define the cities, years, and months
cities <- c("London", "Paris", "Berlin")
years <- 2018:2020
months <- 1:4  # Only the first four months

# Create a data frame with all combinations of City, Year, and Month
data9 <- expand.grid(City = cities, Year = years, Month = months)

# Simulate temperature data with some variation depending on the city
data9$Temperature <- round(rnorm(nrow(data9), mean = 15, sd = 10), 1) + 
  with(data9, ifelse(City == "London", 0, ifelse(City == "Paris", 5, -5)))

# Check the first few rows of the dataset
head(data9)
```

```{r plot for temperatures}
# Convert Month to a factor for better axis labeling
data9$Month <- factor(data9$Month, levels = 1:4, labels = month.abb[1:4])

# Basic ggplot object
p <- ggplot(data9, aes(x = Month, y = Temperature, group = Year, color = factor(Year))) +
  geom_line() +
  labs(title = "Average Monthly Temperature (Jan-Apr, 2018-2020)",
       x = "Month",
       y = "Temperature (C)",
       color = "Year") +
  theme_bw() +
  theme(legend.position = "none") +
  facet_grid(Year ~ City)

#Printing it
p
```

## Outlook

That was a brief introduction to data visualization in R and the basic visualization used in Data Analysis. The start of most visualizations are those basic plots and as you saw it is the same workflow. First, you have to built the basic plot, second you have to add the layers you want. And ggplot2 seems to be complicated at first, but since data visualization is a crucial task in Data Science and Research you will have get very fluent, very fast.

I can only encourage you to go on and explore the world of data visualization in R with ggplot2. In this section, I want to give a glimpse of what is possible:

### Combining different types of Graphs

You can also combine different types of graphs. But be careful! Too much in one graph can be distracting. In the following, I will present a graph with two y-axis, one for a line chart with dots and one for a barplot. The x-axis presents the months of the year

```{r multiple graphs in one graph}
# Simulating example data
data10 <- data.frame(
  months = factor(1:12, levels = 1:12, labels = month.abb), 
  avg_temp = c(0.6, 1.8, 4.6, 6.1, 10.4, 19, 18.3, 
               17.9, 15.2, 9.6, 4.7, 2.6), 
  n_deaths = c(149, 155, 200, 218, 263, 282, 
               318, 301, 247, 250, 194, 205)
)

# Scaling factor to align avg_temp with n_deaths
scale_factor <- max(data10$n_deaths) / max(data10$avg_temp)

# Create the combined graph with dual y-axes
ggplot(data10, aes(x = months)) + 
  geom_bar(aes(y = n_deaths), stat = "identity", fill = "#FF8080", 
          alpha = 0.6) + 
  geom_line(aes(y = avg_temp * scale_factor, group = 1), 
            color = "#2c2c2c", linewidth = 1, linetype = "dashed") +
  scale_y_continuous(
    name = "Number of Traffic Deaths",
    sec.axis = sec_axis(~ . / scale_factor, name = "Average Temperature (Celsius)")
  ) + 
  labs(x = "", 
       title = "Number of Traffic Deaths and Average Temperature per Month") + 
  theme_bw() +
  theme(
    axis.title.y.left = element_text(color = "#FF8080"),
    axis.title.y.right = element_text(color = "#2c2c2c")
  )
```

### Distributions: Ridgeline Chart and Violin Chart

Two visualizations, which get more and more popular: The Ridgeline Chart and the Violin Chart.

The violin chart displays a density plot horizontally. Moreover, it displays mirrors the density plot and puts it toegether:

```{r violin chart}
# Setting seed for reproducibility
set.seed(123)  

# Simulate example sports data
sports_data <- data.frame(
  sport = factor(rep(c("Basketball", "Soccer", "Swimming", "Gymnastics", "Tennis"), each = 100)),
  height = c(
    rnorm(100, mean = 200, sd = 10),   # Basketball players are typically tall
    rnorm(100, mean = 175, sd = 7),    # Soccer players have average height
    rnorm(100, mean = 180, sd = 8),    # Swimmers
    rnorm(100, mean = 160, sd = 6),    # Gymnasts are typically shorter
    rnorm(100, mean = 170, sd = 9)     # Tennis players
  )
)

# Create the violin plot
ggplot(sports_data, aes(x = sport, y = height, fill = sport)) +
  geom_violin(trim = FALSE) +
  labs(
    title = "Distribution of Athletes' Heights by Sport",
    x = "Sport",
    y = "Height (cm)"
  ) +
  theme_bw() +
  theme(
    legend.position = "none",
    plot.title = element_text(hjust = 0.5, size = 16, face = "bold"),
    axis.title.x = element_text(size = 14),
    axis.title.y = element_text(size = 14)
  ) +
  scale_fill_brewer(palette = "RdBu")
```

The Ridgeline chart is a nice way to compare more than 2 distributions. The idea is to plot the scale on the x-axis. On the y-axis the groups you want to compare are plotted:

```{r ridgeline plot}
# Setting seed for reproducibility
set.seed(123)  

# Normal distribution
normal_data <- rnorm(1000, mean = 50, sd = 10)

# Left-skewed distribution (using exponential distribution)
left_skewed_data <- rexp(1000, rate = 0.1)

# Right-skewed distribution (using log-normal distribution)
right_skewed_data <- rlnorm(1000, meanlog = 3, sdlog = 0.5)

# Bimodal distribution (combining two normal distributions)
bimodal_data <- c(rnorm(500, mean = 35, sd = 5), rnorm(500, mean = 60, sd = 5))

# Combine the data into a data frame
example_data <- data.frame(
  value = c(normal_data, left_skewed_data, right_skewed_data, bimodal_data),
  distribution = factor(rep(c("Normal", "Left-Skewed", "Right-Skewed", "Bimodal"), each = 1000))
)

# Create the ridgeline chart
ggplot(example_data, aes(x = value, y = distribution, fill = distribution)) +
  geom_density_ridges() +
  scale_fill_brewer(palette = "Dark2") +
  labs(
    x = "Values", 
    y = "Distribution", 
    title = "A Ridgeline Chart"
  ) +
  theme_ridges() + 
  theme(legend.position = "none")
```

### Ranking: Lollipop Charts and Radar Charts

#### Lollipop Charts

Lollipop Charts are getting more and more popular, so I want to show them to you. The idea is quite simple, it is a Bar Chart, instead a bar it uses a line and a dot:

-   To implement it, we need to add a `geom_point()` layer in combination with a `geom_segment()` layer.
-   We define the axis within ggplot() layer.
-   Lastly, we have to define the aesthetics in the geom_segment() plot.

```{r Lollipop Chart}
ggplot(data4, aes(x=name, y=strength)) +
  geom_point() + 
  geom_segment(aes(x=name, xend=name, y=0, yend=strength))
```

Let us make it pretty. We can give the line different colors and adjust it with the same methods as the line chart. The same goes for the dots we can adjust them as much as we like:

```{r pretty lollipop chart}
ggplot(data4, aes(x=name, y=strength)) +
  geom_segment(aes(x=name, xend=name, y=0, yend=strength), 
               color = "grey") +
  geom_point(size = 4, color = "#74B72E") +
  labs(x = "Fictional Character", 
       y = "Strength", 
       title = "Strength of fictional Characters") +
  theme_light() +
    theme(
    panel.grid.major.x = element_blank(),
    panel.border = element_blank(),
    axis.ticks.x = element_blank()
  ) 
```

### Maps

R also offers a variety of possibilities to work with spatial data. Of course, visualization of maps is an integral part, when working with spatial data. With R you can plot all sorts of maps: Interactive maps with leaflet, shape files of countries and multiple layers with the sf package and standard visualization tools such as connection maps or Cartograms.

Here is an example of an interactive map filled with data. To keep the code as simple as possible I used the tmap package. It is a map of the world, which displays via its color, if a country is an high income, upper middle income, lower middle income or low income country:

```{r world map}
# Get country-level shapefiles
world <- ne_countries(scale = "medium", returnclass = "sf")
world <- world %>%
  filter(gdp_year == 2019) %>%
  mutate(`Income Group` = case_when(
    income_grp %in% c("1. High income: OECD",
                      "2. High income: nonOECD") ~ "1. High Income",
    income_grp == "3. Upper middle income" ~ "2. Upper Middle Income", 
    income_grp == "4. Lower middle income" ~ "3. Lower Middle Income", 
    income_grp == "5. Low income" ~ "4. Low Income")
)

# Plot using tmap
tmap_mode("view")
tm_shape(world) +
  tm_polygons("Income Group", 
              title = "Income Groups", 
              palette = "viridis", 
              style = "cat",
              id = "sovereignt")
```

## Outlook

This chapter was an introduction to one of the most fun part of R, making plots. I introduced you to the standard forms of visualization and gave you a little primer to further visualizations and what is possible in R. The package `ggplot2` is one of the most intuitive (although not for beginners) for data visualization.

-   There is only one book I have to recommend regarding data visualization and that is the ["R Gallery Book"](https://bookdown.org/content/b298e479-b1ab-49fa-b83d-a57c2b034d49/) by Kyle W. Brown. Also check out the [website](https://r-graph-gallery.com/) of this book, it is the standard website, where I search for code snippets for graphs, I can only recommend it.

## Exercise Section

In this exercise Section, we will work with the `iris` package. This is a classic built-in package in R, which contains data from the Ronald Fisher's 1936 Study "*The use of multiple measurements in taxonomic problems".* It contains three plant species and four measured features for each species. Let us get an overview of the package:

```{r inspect iris}
summary(iris)
```

### Exercise 1: Distributions

a\. Plot a Chart, which shows the distribution of `Sepal.Length` over the `setosa` Species. Choose the type of distribution chart for yourself. **HINT:** Prepare the data first and then plot it.

```{r ch4 exercise 1a, eval=FALSE}

```

b\. Now I want you to add the two other Species to the Plot. Make Sure, that every Species has a unique color.

```{r ch4 exercise 1b, eval=FALSE}

```

c\. Make a nice Plot! Give the Plot a meaningful title, meaningful labels for the x-axis and the y-axis and play around with the colors.

```{r ch4 exercise 1c, eval=FALSE}

```

d\. Interpret the Plot!

### Exercise 2: Rankings

a\. Calculate the average `Petal.Length` for every Species in a nice Barplot. HINT: You have to prepare the data again before you plot it

```{r ch4 exercise 2a, eval=FALSE}

```

b\. Add the Means of the `Petal.Width` variable to the plot, so you get a nice grouped Barplot.

```{r ch4 exercise 2b, eval=FALSE}

```

c\. Make a nice Plot! Give the Plot a meaningful title, meaningful labels for the x-axis and the y-axis and play around with the colors.

```{r ch4 exercise 2c, eval=FALSE}

```

d\. Interpret the Plot!

### Exercise 3: Correlation

a\. Make a scatter plot where you plot `sepal.length` on the x-axis and `sepal.width` on the y-axis. Make the plot for the species `virginica`

```{r ch4 exercise 3a, eval=FALSE}

```

b\. Now I want you to add the species `versicolor` to the plot. The dots of this species should have a different color AND a different form.

```{r ch4 exercise 3b, eval=FALSE}

```

c\. Make a nice plot! Add a theme, labels, and a nice title

```{r ch4 exercise 3c, eval=FALSE}

```

<!--chapter:end:04-data_visualisation.Rmd-->

# Exploratory Data Analysis (EDA)

In this chapter, we will start with data analysis. One crucial step when analyzing data is Exploratory Data Analysis. It describes the process of analyzing data sets to summarize their main characteristics. This step can help understanding the data, checking its quality, early detecting patterns and trends and gain first insights!

Since R is a software specifically designed for statistics, we have a lot of high value libraries to perform EDA. The dataset we will use for this part is called `palmerpenguins`. This is a dataset about penguin species and their attributes.

```{r loading packages and penguins data}
pacman::p_load("summarytools", "SmartEDA", "skimr", 
               "naniar", "gtsummary", "dlookr",
               "DataExplorer", "psych", "ggplot2",
               "palmerpenguins", "dplyr", "tidyr", "corrplot")

penguins <- na.omit(penguins)
penguins_raw <- penguins_raw
```

## Standard Descriptive Statistics

### Measures of Central Tendency

As the name suggests, measures of central tendency are helping us to understand the probability distribution of the data, its center and typical values. The three most common measures of central tendency are the arithmetic mean, the median and the mode.

-   **Mode:** The most frequent number

-   **Mean:** The sum of all values divided by the total number of values

-   **Median:** The middle number in an ordered dataset

#### Mode

The mode is probably the easiest measure out of all measures: It is defined as the most frequent number of all observations. We cannot directly calculate the mode, but there is a way to it. First we look at all unique values of or observations with the `unique()` function, then we count the occurrences of each unique value with `tabulate()`, and lastly we use the `which.max()` function to get the most frequent unique value:

```{r calculating the mode}
uniq_vals <- unique(penguins$bill_length_mm)  # Get unique values
freqs <- tabulate(match(penguins$bill_length_mm, uniq_vals))  # Count occurrences
uniq_vals[which.max(freqs)] # Getting the unique value with the most occurrences
```

#### Mean

Let us start by looking at the formula to calculate a mean:

$$
\bar{x} = \frac{\sum{x_i}}{n}
$$

whereas:

-   $\bar{x}$ is our mean

-   $\sum{x_i}$ is the sum of all our observations.

-   $n$ is the number of all our observations

We could do that by hand or we just use the built-in `mean()` function:

```{r calculating the mean}
mean(penguins$bill_length_mm)
```

#### Median

Image sorting all your data from the lowest to highest and then pointing at the value, which has exactly 50% of all values to its left and the other 50% to its right, this would be the median value. Well, at least you will point at a value if your distribution has an yeven number of observations. But you can also calculate the value for an uneven number of observations, let us have a look at both formulas:

-   $X_{(\frac{n+1}{2})}$ for an even number of *n*

-   $\frac{1}{2}X_{(\frac{n}{2})} + X_{(\frac{n}{2} + 1)}$ for an uneven number of *n*

Again we could calculate that by hand or we just use the `median()` function:

```{r calculating the median}
median(penguins$bill_length_mm)
```

### Measures of Dispersion

In statistics, measures of dispersion describe the extent to which a distribution of a variable is stretched or squeezed. In other words, they help to gauge the spread of our distributions.

#### Interquartile Range (IQR)

You remember the boxplot from the data visualization chapter? It is supposed to show the so-called interquartile range (IQR). It is defined as the difference between the 75th percentile (or third quartile) and the 25th percentile (or the first quartile). Basically the distribution is spread into four equally big areas, which are separated by three points, the first quartile denoted by $Q_1$ (also called the lower quartile), the second quartile is the median and denoted as $Q_2$, and the third quartile is measures of dispersiondenoted by $Q_3$ (also called the upper quartile), thus the formula is:

$$
IQR = Q_3 - Q_1
$$

In R, we can calculate the **Interquartile Range (IQR)** using the `IQR()` function. By hand, we would:

1.  **Sort the data** in ascending order.

2.  **Split the data into four equal parts** (quartiles).

3.  Identify **Q1 (first quartile, 25th percentile)** and **Q3 (third quartile, 75th percentile)**.

Note that in some cases the data cannot be divided into four even parts ddue to their size. In such cases, different statistical methods (e.g. Tukey's Hinges) approximate quartiles in such cases.

```{r calculating the iqr}
IQR(penguins$bill_length_mm)
```

#### Variance

In statistics, the variance is the expected value of the squared deviation from the mean of our random variable. The concept of the mean gets clear if we break down its formula:

$$
s = \frac{\sum(x_i - \bar{x})}{n-1}
$$

where

-   the index *i* runs over the observations (Respondents, Countries,...), *i* = 1,...,*n*

-   $x_i$ are our observations

-   $\bar{x}$ is the mean of our distribution

-   $n$ is the number of observations

Especially the nominator of the formula $\sum(x_i - \bar{x})$ is quite interesting because it uses an interesting technique:

Image our data is aligned on one dimension and somewhere in the middle there is our mean:

Now, image we would calculate the differences and sum them up without squaring.

You see that distance 1 would be -3 and distance 2 would be 4 an in sum that makes 0,8, but that is for sure not the distance between those two points. Here comes the squared part into action, every squared number is positive, this ensures that -3 becomes 9 and 4 becomes 16, thus the differences can be summed and result in 24.

In R, we can implement this by simply calling the var() function:

```{r calculating the variance}
var(penguins$bill_length_mm)
```

Well we get 29.9 and the problem with variance is, that the squaring technique I showed you earlier leads to a problem: We cannot really interpret the data because it loses its unit, for example if our data is in meter, the variance would be in square meters. But we can solve this problem with the next measure: standard deviation.

#### Standard Deviation

The standard deviation is the square root of the variance. It describes the amount of variation of the values of a variables about its mean. A low standard deviation indicate closeness of the values to the mean, and a high standard deviation indicates that the values are spread out over a wider range.

The standard deviation is the square root of the variance:

$$
s = \sqrt{s} = \sqrt\frac{\sum(x_i - \bar{x})}{n-1}
$$

Remember our example?:

We calculated the variance the distance of the squared sums to be 24 (9 + 16). However, what happens if we take the square root of the squared values before adding them? Well, we get 3 for distance 1 and 4 for distance 2. For distance 2 nothing changed, it came back to its original value 4, but distance 1 has become positive from -3 to 3. Now we can add it together and get the distance 7.

Let us apply it in R, with the function `sd()`:

```{r calculating the standard deviation}
sd(penguins$bill_length_mm)
```

The huge advantage of the standard deviation in comparison to the variance is that it can be interpreted in the original unit and is thus a more intuitive measure of dispersion to work with.

### Relationships between Variables

It is necessary to understand one key difference in EDA. There are values you simply look at and interpret such as the measures shown before. But on the other hand there are measures which look at the relationship between variables, thus analyzing how they interact which each other. In the following, we will look at two methods to do so.

#### Crosstables / Contingency Tables

```{r making crosstabs}
table(penguins$species, penguins$island)

summarytools::ctable(penguins$species, penguins$island)

gtsummary::tbl_cross(data = penguins, 
                     row = species, 
                     col = island)
```

#### Correlation

Correlation is an umbrella term for any statistical relationship, whether causal or not, between two random variables or bivariate data. The three main measures of correlations are named after their inventors: Pearsons Correlation (or simply Pearson's r), Spearman's Rank Correlation (or simply Spearman's rho) and Kendall's Tau.

Before going on let us make clear what a linear relationship means. It means that if an observation increases or decreases the corresponding variables changes in a proportional and predictable way.

##### Pearsons Correlation

Pearsons Correlation measures the linear relationship between two continous variables. To do so it assumes a normal distribution between two variables, a linear relationship and no major outliers.

Normally, you should test those before running a correlation, but we skip that part and look at the implementation in R:

```{r simple correlation}
cor(penguins$bill_length_mm, penguins$body_mass_g,
    method = "pearson")
```

The interpretation of pearson's r is quite straightforward: The result always ranges between +1 and -1, where +1 means a perfect linear relationship, 0 means no relationship at all, -1 means a perfect linear relationship.

In our case there is a strong positive, linear relationship of both variables with a r = 0.59.

##### Spearman's Rank Correlation

Spearman's Rank Correlation, or simply Spearman's Rho shows if ordinal or continuous variables have a Monotonic Relationship. This means that if one variables increases, the other always increases or decreases but not necessarily at a constant way, as it would be with a linear relationship.

It has different assumptions like outliers are allowed, the relationship is non-linear, the data contains rank and the variables are not normally distributed.

Let us have a look, how it is calculated in R:

```{r correlation spearman}
cor(penguins$bill_length_mm, penguins$body_mass_g,
    method = "spearman")
```

The interpretation is analog to Pearson's R, +1 means a perfect positive, monotonic relationship, 0 means no monotonic relationship and -1 means perfect negative, monotonic relationship.

A spearman's rho of 0.58 indicates a strong, positive, monotonic relationship between the two variables.

##### Kendalls Tau

Kendalls Tau measures the strength and direction of association between two ranked variables. We differentiate between two types of relationships: Concordant and Discordant relationships.

-   A concordant relationship means both data points move in the same direction

-   A discordant relationship if one data point increases, the other decreases (or vice versa).

Let us calculate it:

```{r correlation with kendall}
cor(penguins$bill_length_mm, penguins$body_mass_g,
    method = "kendall")
```

Kendalls Tau can be interpreted wiht +1 as a perfect rank agreement, 0 means no association at all, and -1 means a perfect rank reversal.

Our kendall's tau indicates a high rank agreement between both our variables.

#### Correlation Graphically

The part before was quite theoretical, but there are also nice approaches to look at correlations graphically. we start with a simple scatterplot:

```{r graphically correlation}
ggplot(penguins, aes(x = bill_length_mm, y = body_mass_g)) +
  geom_point(color = "#0077b6") +
  labs(x = "Length in mm",
       y = "Body Mass in g",
       title = "Relationship between Length (in mm) and Body Mass (in g)") + 
  theme_bw()
```

When looking at relationships between two variables, scatterplots are the standard visualization to do so. As we can see, there is a clear trend that the length in mm could be positively related to the body mass in g. In the end of the day, there are three types of linear relationships and the scatterplots always look accordingly:

```{r example correlations}
set.seed(123)

n <- 100

df_cor <- data.frame(
  x = rep(1:n, 3),
  relationship = rep(c("Positive", "Negative", "None"), 
                     each = n),
  y = c(
    (1:n) + rnorm(n, sd = 15),    # strong positive correlation
    (n:1) + rnorm(n, sd = 15),    # strong negative correlation
    rnorm(n, mean = 50, sd = 20) # no correlation
  )
)

# Reorder factor levels
df_cor$relationship <- factor(df_cor$relationship, 
                          levels = c("Positive", 
                                     "None", "Negative"),
                          labels = c("Positive", 
                                     "No Correlation", 
                                     "Negative"))

# Plot
ggplot(df_cor, aes(x = x, y = y)) +
  geom_point(color = "steelblue", size = 2) +
  facet_wrap(~relationship, nrow = 1) +
  labs(title = "Strong Positive, Negative, and No Correlation",
       x = "X", y = "Y") +
  theme_bw(base_size = 18)
```

With scatterplots in combination with facet_wrap() you can show several correlations graphically, but there is a way to calculate the correlation coefficient and to show it graphically with a **correlation plot**. A correlation plot combines the logic of contingency tables, heat maps and the correlation coefficients.

First a **correlation matrix** is created. A table that shows the **pairwise correlation coefficients** (typically Pearson) between several numerical variables. Each cell in the matrix represents the strength and direction of the linear relationship between two variables. A **correlation plot** is then a visual representation of this matrix, often using color gradients or circle sizes to show the strength and direction of correlations, making it easier to spot patterns.

Let us compute it in R: We will use the corrplot package in R (There are other ways to compute it, which I will show later).

-   First, we cut down our dataset to the variables you want to correlate with each other

-   Second, compute a correlation matrix with the cor() command

-   Third, call the corrplot variable, and take in the dataset, define the method (we will use the color to display the strength of the correlation), the type (`'full'` (default), `'upper'` or `'lower'`, display full matrix, lower triangular or upper triangular matrix).

```{r simple corr matrix}
# Step 1: Prepare numeric data
penguins_numeric <- penguins %>%
  select(bill_length_mm, bill_depth_mm, flipper_length_mm, body_mass_g) %>%
  drop_na()

# Step 2: Compute correlation matrix
corr_matrix <- cor(penguins_numeric)

# Step 3: Plot the correlation matrix
corrplot(corr_matrix, method = "color")
```

Now, we can see in an elegant way the correlation between the four selected variables, which are displayed through the color. Every cell displays the correlation coefficient of the variable on its respective column and on its respective row.

The corrplot() function allows for further aesthetics:

```{r pretty corrmatrix}
corrplot(corr_matrix, method = "color", type = "upper", 
         addCoef.col = "black", tl.col = "black", tl.srt = 45)
```

And finally, we can change the method to circular. This changes the plot insofar that it does not fill the cells with the color of the correlation coefficient, but makes a circle around the numbers and fills it then with the color. The size of the circles, thus the radius is determined but the strength of the correlation, meaning that the closer the coefficient to zero the smaller the circle:

```{r corrmatrix circular}
corrplot(corr_matrix, method = "circle", type = "upper", 
         addCoef.col = "black", tl.col = "black", tl.srt = 45)
```

If I use the circular method, I like to display it without the numbers, looks better in my opinion, and is more intuitive then filling the whole row:

```{r pretty corrmatrix circular}
corrplot(corr_matrix, method = "circle", type = "upper", 
         tl.col = "black", tl.srt = 45)
```

## Working with EDA packages

There are several other measures for EDA, and of course you do not have to calculate every single measure by hand, although you could. In the following, I will introduce you to the most popular packages for EDA. From my point of view, they are all basically the same, with some nuances and when talking to other R-users I noticed that everyone somehow established his or her own routine of EDA. Therefore I suggest that you get an overview of all those packages and find your own "EDA-Routine" so to speak.

### psych

The psych package is a crucial package for psychologists and I love to use its "describe()" function, which shows a bunch of descriptive statistics with one line of code:

```{r psych}
#Applying describe to the whole dataset
psych::describe(penguins)

#You can apply describe() also for single variables
#psych::describe(penguins$bill_length_mm)
```

The describe() function output is a summary table of the basic summary statistics as mean, standard deviation, median, and a lot more and it is a shortcut, so you do not have to calculate every measure on its own.

```{r psych corr.test}
corr.test(penguins_numeric)
```

In the next step, I want to show you the pairs.panel() function. It displays a really interesting visualization regarding correlations between the variables:

```{r psych pairs.panels}
pairs.panels(penguins_numeric)
```

So this is an interesting grid consisting of different types of data visualization, all related to correlations. Let us start with the easiest one, the diagonal.

-   The diagonal shows us a histogram of the distributions of our input variables, additionally it includes a line, so we can check if the distributions look right.
-   The upper right visualizations display the correlation coefficients of the respective two variables, like a correlation matrix.
-   The bottom left visualizations are scatterplots between the two variables, where a line is fitted between the data points.
-   In addition, the shape of the red line (a loess smoother) can reveal whether the relationship between the two variables is linear or more complex, such as curved or s-shaped.
-   The scatterplots also include correlation ellipses, which visually represent the strength and direction of the relationship: narrow, tilted ellipses indicate strong correlations, while rounder shapes indicate weaker or no correlations.

Together, this grid gives a comprehensive overview of both the distributions of individual variables and the pairwise relationships between them.

### skimr

The skimr package is a wonderful way to get an overview of our datasets structure and basic statistics, even with a visualization of the distribution of the variable. The main function is `skim()`:

```{r skimr}
skimr::skim(penguins)

#If you do not want to see the distribution
#skimr::skim_without_charts(penguins)
```

The advantage of the skimr package is that it directly calculates descriptive statistics and shows the missing values for every variable in your dataset. I often use this command to have a look at datasets I am not familiar with. I think it is more a function for exploring the dataset rather than EDA it can be a useful first step for conducting EDA.

### summarytools

We will dive into the summarytools package with the classic dfSummary command which summarizes the structure of our dataset:

```{r dfSummary}
dfSummary(penguins)
```

The huge advantage of this command is that if we wrap it around the dfSummary function and then it gives us a formatted in a nice table:

```{r dfSummary with view}
view(dfSummary(penguins))

# Or if you are a good R user, then you can use also a pipe
# dfSummary(penguins) %>%
# view()
```

The package also includes nice ways of showing frequency tables for single variables with more information than the standard table() command:

```{r summarytools freq}
freq(penguins$species)
```

We can also use the descr() function rom the package which shows us the most common descriptive statistics of our variables in our dataset:

```{r descr function summarytools}
descr(penguins)
```

Lastly, I want to show you how you can make cross tables with the summarytools package (you already saw it above):

```{r ctable summarytools}
ctable(penguins$species, penguins$island)
```

### naniar

Naniar is one of the most powerful packages for working with missing data. At first glance, dealing with missing values may seem straightforward  as covered in the "Data Manipulation" chapter, its common to simply remove rows with missing values using functions like `na.omit()` or `drop_na()`.

However, as you progress in data analysis, handling missing data becomes much more important and nuanced. Here's why:

-   **Dropping missing values can lead to a small sample size (`n`)**, reducing statistical power.

-   **If a large portion of data is missing**, removing it may introduce bias, especially if the missingness is not random.

In such situations, **advanced techniques like multiple imputation** become valuable. These methods estimate missing values using mathematical models that consider patterns in the data.

But there's a catch:

**These models have assumptions** (e.g., data are Missing at Random  MAR). Violating these assumptions can lead to misleading results. Thats why its crucial to **explore and understand the structure of missingness** before choosing a strategy.

Let us start with the basic miss_var_summary() function, it shows the number of missing values, and calculates the percentages of missing values:

```{r naniar miss_var_summary}
naniar::miss_var_summary(penguins_raw)
```

The function that made naniar famous is the gg_miss_upset() function, which shows us the structure of the missing values graphically:

```{r naniar gg_miss_upset}
naniar::gg_miss_upset(penguins_raw)
```

```{r naniar vis_miss}
naniar::vis_miss(penguins_raw)
```

### gtsummary

The gtsummary package is the package for data reporting, because it automatically creates data tables ready for publication with one line of code. Let us start with the tbl_summary() function:

```{r tbl_summary}
gtsummary::tbl_summary(penguins)
```

We get a nice table which splits up the categorical data in its categories and displays the absolute and relative frequencies (in the brackets) and for numeric variables we get the median value with first quartile and the third quartile.

We can also group by certain variables to get a more detailed overview:

```{r grouped tbl_summary}
penguins %>%
  tbl_summary(by = sex)
```

The gtsummary package gives you many options to customize your table, which can be regarding the content (mean instead of median, displaying p-value...) but you can also customize its appearance for example customizing the font.

It also includes a nice option to compute publish-ready cross tables:

```{r tbl_cross}
penguins %>%
  tbl_cross(
    row = species,
    col = island
  )
```

### dlookr

dlookr is a nice package with different functions that can support us with EDA. Let us start with the diagnose() function, which is a gelps us identify missing values and unique observations in the dataset:

```{r dlookr diagnose}
diagnose(penguins) %>%
  print()
```

We can also generate an output for summary statistics with an old friend, the describe() function, but this time from the dlookr package:

```{r dlookr describe}
dlookr::describe(penguins)
```

The dlookr package has one special feature: It can generate an EDA report with one line of code, I introduce you the eda_report() function:

`dlookr::eda_paged_report(penguins, output_format = "html")`

|  |  |  |
|------------------------|------------------------|------------------------|
| ![](images/Bildschirmfoto%20vom%202025-04-12%2019-48-38.png) | ![](images/Bildschirmfoto%20vom%202025-04-12%2019-49-55.png) | ![](images/Bildschirmfoto%20vom%202025-04-12%2019-50-21.png) |
| ![](images/Bildschirmfoto%20vom%202025-04-12%2019-52-17.png) | ![](images/Bildschirmfoto%20vom%202025-04-12%2019-53-30.png) | ![](images/Bildschirmfoto%20vom%202025-04-12%2019-54-07.png) |

### DataExplorer

DataExplorer is a powerful all-in-one EDA package, that helps us to explore our data with a few line of code. It also includes a function that generates an EDA report.

But let us start by getting basic information about our data with the introduce() function:

```{r introduce function dataexplorer}
introduce(penguins)
```

We can also plot missing values with DataExplorer by using the plot_missing() function:

```{r plot_missing}
plot_missing(penguins_raw)
```

Further, we can plot correlations with DataExplorer:

```{r plot_correlation}
plot_correlation(penguins_numeric)
```

And finally, we can use DataExplorer to generate an automated Data Report:

`create_report(penguins)`

|  |  |
|------------------------------------|------------------------------------|
| ![](images/clipboard-2450633220.png) | ![](images/clipboard-3067609921.png) |
| ![](images/DataExplorer_report_screenshot4.png) |  |

### smartEDA

The last package in this chapter is smartEDA. It is a powerful package designed to quickly create descriptive statistics and visualizations for numeric and categorical data.

The first function is ExpData(), it gives us the structure, missing values and variable types:

```{r ExpData}
ExpData(penguins, type = 1)
```

We can also let smartEDA calculate summary statistics such as mean, standard deviation, skewness, etc.

```{r ExpNumStat}
ExpNumStat(penguins)
```

Lastly, we can again generate an automatized EDA report with ExpReport():

`ExpReport(data = penguins, Target = "species", label = "Penguin Species", op_file="Samp1.html", Rc=3 )`

|  |  |
|------------------------------------|------------------------------------|
| ![](images/smartEDA_report_screenshot1.png) | ![](images/smartEDA_report_screenshot2.png) |
| ![](images/smartEDA_report_screenshot3.png) | ![](images/smartEDA_report_screenshot4.png) |

## Conclusion

And that is it - at least for now - the possibilities and functions of the packages presented could fill easily an own course and as I already said, at one point everyone gets its own EDA routine and has its own packages they want to work with. The important point is to always get an overview over your data and to always check for interesting patterns in your data before conducting substantial analysis.

## Exercise Section

### Exercise 1: Standard Descriptive Statistics

In this exercise we will work with the built-in `iris` package in R:

a\. Calculate the mode, mean and the median for the `iris$Sepal.Length` variable

b\. Calculate the interquartile range, variance and the standard deviation for iris\$Sepal.Length

c\. Calculate all five measures at once by using a function that does so (Choose by yourself, which one you want to use)

### Exercise 2: Contingency Tables and Correlations

a\. Make a Contingency Table for `esoph$agegp` and `esoph$alcgp`

b\. Cut down the iris dataset to Sepal.Length, Sepal.Width, Petal.Length and Petal.Width and save it in an object called iris_numeric.

c\. Make a correlation matrix with iris_numeric

d\. Make the correlation matrix prettyChapter 4: Exploratory Data Analysis

### Exercise 3: Working with packages

a\. Use a function to get an overview of the dataset mtcars

b\. Have a look at the structure of the missing values in mtcars

c\. Make an automatized EDA report for mtcars!

<!--chapter:end:05-exploratory-data-analysis.Rmd-->

# Data Analysis

In this chapter you I introduce you to the basic ideas of statistical analysis and hypothesis testing. For me, it is important that you get an intuition about what is going on rather than terrorizing you with complicated math. Although, I cannot and will not leave out central formulas, you will be fine with the maths. There extensively commented and in the end of the day not that hard to understand. You will notice that the programming in this chapter is quite easy compared to before. In the end, you only need formulas, who do everything you need to analyse data, that is also a strength of R. Please concentrate more on the concepts and get an idea what is going on. The goal of this chapter is that you get to know linear regression and how to check if the model fits and the results are robust and significant.

```{r loading packages}
pacman::p_load("tidyverse", "ggpubr","gapminder", 
               "sjPlot", "GGally", "car", "margins", "plotly")
```

## Linear Regression

### Terminology

The classical (bivariate) linear regression can be expressed in the following equation (systematic component):

$$
Y_i = \beta_0 + \beta_1X_i + e_i 
$$

where

-   the index *i* runs over the observations (Respondents, Countries,...), *i* = 1,...,*n*

-   $Y_i$ is the dependent variable, the variable we want to explain

-   $X_i$ is the independent variable or explanatory variable.

-   $\beta_0$ is the *intercept* of the regression line

-   $\beta_1$ is the *slope* of the regression line

-   $\epsilon_i$ is the *error term*, thus how our observed data differs from actual population data (e.g. Measurement Error).

### Estimating the Ordinary Least Squares Estimator

To get the idea of linear regression, let us look at an example. To do so, let us simulate some data and plot it.You now should have the data frame `df` in your environment. It contains a variable X, which is our independent variable. Y is also included, which is your dependent variable. You want to explain Y with your X. Let us plot the variables with a scatterplot:

```{r simulating data, echo=FALSE}

set.seed(123) # For reproducibility

n <- 30 

x <- runif(n) * 10 

categorical_variable <- factor(sample(c(0, 1), n, replace = TRUE))

y <- 0.8 + 1.6 * x + rnorm(n, 0, 3)

df <- data.frame(x,y, categorical_variable)

#Simulate further data

X_quadratic <- X <- runif(50, min = -5, max = 5)
u <- rnorm(50, sd = 1)  

#True relation
Y_quadratic <- X^2 + 2 * X + u

#Making a data frame out of it
df2 <- data.frame(X_quadratic, Y_quadratic)
```

```{r plotting x and y in df}
ggplot(df, aes(x, y)) + 
  geom_point() +
  theme_bw() + 
  scale_x_continuous(breaks = seq(0, 10, by = 1)) +
  scale_y_continuous(breaks = seq(0, 20, by = 2))
```

We can see that there has to be some relationship between both those variables: The higher x gets the higher y gets. Linear regression can help us investigate the relationship. We just have to take the formula and estimated $\beta_0$ and $\beta_1$ . To do so, we estimate the **ordinary least square (OLS) estimator**. To understand what the OLS estimator does, look at the scatter plot again: **The OLS estimator fits a line through all the dots, that minimizes the distance to the dots as much as possible**. Afterward, we only extract the intercept $\beta_0$ (that is the point, where the line crosses the y-axis), and $\beta_1$ (that is the slope of the line). However, since these are estimated values for our model we have to call them by convention $\hat{\beta_0}$ and $\hat{\beta_1}$ . We will denote them in the following as such.

#### Visualization

The visual estimation is good to get an intuition with the data, but we will see later, that it does not work with multiple independent variables. Further, it does not give much information, at least not as much as we would like to have.

```{r fitting an ols estimator line visually}
ggplot(df, aes(x, y)) + 
  geom_point() +
  theme_bw() + 
  scale_x_continuous(breaks = seq(0, 10, by = 1)) +
  scale_y_continuous(breaks = seq(0, 30, by = 5)) + 
  geom_smooth(method = "lm", se = FALSE) + 
  geom_segment(aes(x = x, y = y, xend = x, yend = predict(lm(y ~ x, data = df))), linewidth = 0.5) 
```

#### Calculation per Hand

We could also just calculate $\hat{\beta_0}$ and $\hat{\beta_1}$ by hand. The formula for both are as follows:

$$
\hat{\beta_1} = \frac{\sum_{i=1}^n(x_i - \bar{x})(y_i - \bar{y})}{\sum_{i=1}^n(x_i - \bar{x})^2}
$$

where $x_i$ is the answer of respondent *i* for our independent variable x, $\bar{x}$ is the average answer of the respondents. Those two values are subtracted, thus the deviation from the mean is calculated. The same goes with our dependent variable y, where $y_i$ is the answer of respondent *i*, and $\bar{y}$ is the average answer of all respondents. his is done for every respondent *i*, thus *n* - times. This is displayed with the sum symbol. We just calculated the so-called **covariance.**

The **covariance** is then divided by the squared deviation to the average answer of our independent variable x. This will get us the estimated coefficient for our model $\hat{\beta_1}$ . Sounds complicated but lets do it in R:

```{r calculating ols by hand}
#Let us first get the covariance
cov <- sum((df$x - mean(df$x)) * (df$y - mean(df$y)))

#Now we get the variance of x 
x_sq <- sum((df$x - mean(df$x))^2)

x_sq

# We just have to divide them 
slope <- cov/x_sq 

#printing it
print(slope)
```

To get the intercept $\hat{\beta_0}$ we have to take the result of the slope and implement it into this formula:

$$
\hat{\beta_0} = \bar{y} - \hat{\beta_1}*\bar{x}  
$$

We multiply $\hat{\beta_1}$ with the average of our independent variable X. The result is subtracted from the average of our dependent variable y.

```{r calculating intercept by hand}
#calculating the intercept
beta_0 <- mean(df$y) - (slope * mean(df$x))

#printing it
beta_0
```

Now we estimated our parameters and can display the for our model by simply putting it into the systematic component:

$$
Y_i = 1.68 + 1.54 * X_i
$$

#### Automated Calculation

This procedure by hand is way to time-wasting. R has a built in function to calculate the parameter for us called `lm()` :

```{r calculating linear regression automated}
#running a linear regression
model1 <- lm(y ~ x, 
             data = df) 

#Printing a summary of the model results
summary(model1)
```

When we check the results, we see that we did everything right and get the exact same values. The interpretation of the coefficient is with a one-unit increase in the independent variable X, the dependent increases on average about 0.68 units, holding all else constant. But you noticed that you get more information on the model, than just the coefficients. We will get to that later.

### Predictions with Linear Regression

We could calculated predictions based on our OLS calculations, reconsider the systematic component we calculated:

$$
\hat{y_i} = 1.68 + 1.54x_i 
$$

We just have to put in x-values and according to our model we would get a prediction of the respondents y-value $\hat{y_i}$. We can do that for all x-values in our dataset and get a column with all the predicted values, let us call it (`y_hat`) :

```{r calculating predictions}
#First, we calculate the predictions for y
df$y_hat <- 1.6821 + 1.5394*df$x 

#We could also do it automatically via the predict() function
df$auto_y_hat <- predict(model1)

#Checking it 
head(df)
```

## Hypothesis Testing in R

We are not only interested if our model fits or not. We want to investigate real world phenomena. Well, and if our model does not fit well, we have to make adjustments so it does. What comes next is that we want to know more about the world. Researcher do so by formulating **hypotheses**. These are nothing more than assumptions you make theoretically about the world. Let us say you think you assume that in our world education has an impact on income. This would be your **Alternative Hypothesis** $H_A$ . What you know want to do is to test it against the so-called **Null Hypothesis** $H_0$. That is nothing more than the opposite of our alternative hypothesis, thus that education has no impact on income. Let us formulate both to have an overview:

$H_0$ = Education does not impact Income.

$H_A$ = The more educated a person is, the higher the income.

The advantage of this approach is obvious, one of those will be true. Therefore statistical testing is needed. But before introducing it to you, we have to decide how we want to test our alternative hypothesis. More specifically, how do we want to measure our variables. For our example, we decide to conduct a survey and to get data by asking a random sample of 1000 people over 18 living in Germany (*N*=1000). We decide to measure our independent variable Education, by asking the respondents about the years they invested in their education. To get their income, you just ask them about it to fill it in.

Can you think about possible critics about our data collection strategy?

### Standard Error

#### Root Mean Square Error (RMSE)

Before moving on to Standard Errors, I will introduce another metric, the root mean square error. It is the average difference between the actual values $y_i$ and our predicted values $\hat{y_i}$. To calculate it, we square the residuals, to get only positive values. Then we take the mean, and lastly we take the square root.

```{r RSME}
#Getting the the sum of squared residuals (SSR)
SSR <- df$y_hat^2

#Calculating the mean of the squared residuals 
mean_SSR <- mean(SSR) 

#Calculatin the RSME 
rsme <- sqrt(mean_SSR)

#Printing it 
print(rsme)
```

The rule of thumb is that the lower the RSME, the better. It is a non-standardized goodness of fit measure. Its counterpart is the R-squared measure, which is a standardized measure. You can use both, and should use both to get a metric about how close the predicted values are distributed around the actual values.

#### Standard Error of the Estimate

The standard error of a coefficient estimate is the metric directly presented next to the coefficient in the regression output. It is the square root of the variance of the regression coefficient.

```{r se}
#calculating standard error by hand
se <- SSR/(nrow(df) - 2 * (sum(df$x - mean(x))))

#Printing it
print(se)
```

### T-Value or T-Statistic

The formula for calculating the t-value is simple:

$$
t_i = \frac{\beta_i}{SE(\beta_i)}
$$

where $\beta_i$ is the coefficient calculated by the linear regression, and $SE(\beta_i)$ is the standard error. Let us calculate it manually by hand for our example:

```{r t-value by hand}
#t value by hand
t_value_intercept <- -0.32773/0.30271 
t_value_x         <- 0.67949/0.04813 

#printing it
print(t_value_intercept) #-1.082653
print(t_value_x) #-14.11781
```

Well, that is the t-value on its own does not tell us about statistical significance. It is used to calculate the **p-value**, which tells us about the significance of the value. But before we calculate it, we have to understands some key concepts before.

-   **Degrees of Freedom** is the first concept. Let us say, we have a sample of me and my sister (*N* = 2). We collected the numbers of books each of us has, and calculated a mean of 30. I have 20 books. How many books does my sister have? Obviously, she has 40, if we have a mean of 30. Given my value, and the mean of the sample, the value of my sister had to be 20. That is what the degrees of freedom tells us. Given the mean of a sample, the values, which can freely be chosen. Or to put it more technically, the maximum number of logically independent values.

-   The rule is that **the larger the sample, the higher the degrees of freedom** and **the lower the sample, the lower the degrees of freedom.**

-   **T-distributions** are the distribution, we will use to calculate the p-value. I will talk about that in detail in a minute. But they are connected to the degrees of freedom, because degrees of freedom determine the tail behavior and shape of the curve until the point, where the t-distribution looks like a standard normal distribution. Let us visualize that:

    ```{r interactive t-statistics plot}
    # Generate data
    x <- seq(-5, 5, length.out = 100)

    # Calculate densities
    densities <- data.frame(
      x = rep(x, 4),
      density = c(dt(x, df = 1), dt(x, df = 2), dt(x, df = 10), dnorm(x, mean = 0, sd = 1)),
      distribution = rep(c("t(df=1)", "t(df=2)", "t(df=10)", "Normal"), each = 100)
    )

    densities$distribution <- factor(densities$distribution, 
                                     levels = c("Normal", 
                                                "t(df=10)", 
                                                "t(df=2)", 
                                                "t(df=1)"))

    # Plot
    plotly::ggplotly(ggplot(densities, aes(x = x, y = density, color = distribution)) +
      geom_line() +
      theme_minimal() +
      labs(x = "x", y = "Density", 
           title = "t-distributions with different degrees of freedom") + 
      scale_color_manual(values = c("black", "red", "green", "blue")) + 
      scale_x_continuous("X", seq(-5,5,1), limits = c(-5,5))) 
    ```

-   Alright, before we find out, how to determine if a value allows us to reject the null hypothesis, we have to determine a so-called **critical value**. This is no math or anything, it is a probability we decide about. More precisely, the probability that our t-value and thus our coefficient occured by chance alone. If we say the probability that it occured by chance alone is 10%, then this is our critical value. The rule of thumb is, that a probability lower than 5% that the coefficient occured by chance alone indicates statistical significance. Keep in mind, that the critical value can vary depending on the sample size, the degrees of freedom, the field you are working in etc.

```{r t-static identification visually}
#setting seed 
set.seed(42) 

# Generate data
x <- seq(-5, 5, length.out = 100)
t_density <- function(x) dt(x, df = 28)

# Calculate densities
t_value_data <- data.frame(
  x = rep(x, 1),
  density = dt(x, df = 28),
  distribution = rep("t(df=28)", 100)
)

# Plot
plotly:: ggplotly(ggplot(t_value_data, aes(x = x, y = density)) + 
  geom_line(lineend = "round") + 
  stat_function(fun = t_density, geom = "area", fill = "gray", 
                alpha = 0.75, xlim = c(-5, -1.701), n = 10000) +
  stat_function(fun = t_density, geom = "area", fill = "gray", 
                alpha = 0.75, xlim = c(5, 1.701), n = 10000) +
  geom_vline(xintercept = -1.701, linetype = "dashed", 
             colour = "red") +
  geom_vline(xintercept = 1.701, linetype = "dashed", 
             colour = "red") + 
  ggtitle("t-distribution with 28 df", subtitle = "The pink area marks the interval of significant values on a 95% level") +
  geom_segment(x = -1.082653, 
               xend = -1.082653, 
               yend = dt(-1.082653, df = 28), 
               y = -1, 
               color = "pink", 
               linetype = "dashed", 
               linewidth = 0.2) +
    annotate("point", x = -1.082653, y = dt(-1.082653, df = 28), 
             color = "pink") +
  scale_x_continuous("X", seq(-5,5,1), limits = c(-5,5)) +
  theme_classic() +
    theme(legend.position = "none")
)
```

-   As we can see, the blue line representing the value of the intercept is not in the area it would have to be, for us to reject the null hypothesis. However, the t-value of our coefficient from variable is with about 14 far away from the threshold, so we can reject the null hypothesis. This is how the t-value works. The problem is, that the threshold varies with the degrees of freedom, and you will not have the threshold value in your head for every degree of freedom. You could look it up every time or you use **p-values**, which directly tell you the probability of the coefficient occuring by chance alone.

### p-values

-   The p-value is a statistics that shows us the probability that a statistical measure (in our example 0) is greater/less than an observed value (in our example, the estimated coefficient). The null hypothesis would tell us that our independent variable X has no impact on Y. Statistically speaking, that would mean that our coefficient has a high probability to be zero, because zero means no effect, thus we cannot reject the null hypothesis. But if the probability that our estimated coefficient is 0 is low, we may reject the null hypothesis and would found an effect.

-   Before showing you two ways to find the p-value, we have to determine a **critical value**. This is no math or anything, it is a probability we decide about. The rule of thumb is that if the p-value is smaller than 5%, we can reject the null hypothesis. However, depending on various factors, it could also be different, that depends on your data, sample size, degrees of freedom etc.

-   For our example, we follow the rule of thumb and set the significance level to 0.05, thus if the coefficient has a smaller probability (p-value \< 0.05) than 5% to be zero we can reject the null hypothesis, otherwise we cannot reject it.

-   Since the math is complicated and thus not help to understand the intuition, I directly show you how to calculate the p-value by hand:

```{r p-value by hand }
#calculating the p values by hand
p_value_1 <- 2 * pt(-abs(t_value_intercept), 28)
p_value_2 <- 2 * pt(-abs(t_value_x), 28)

#printing it
print(p_value_1) 
print(p_value_2)
```

We get the same values as in our regression, and we decided before that our critical value (denoted as $\alpha$) should be less than 0.05 to reject the null hypothesis. Therefore we can reject the null hypothesis for the coefficient of our variable x. Since the coefficient is positive, the interpretation would be that x has on average a positive effect on y, since the probability that the value is different than 0 is lower than the critical value of 5%.

### Confidence Interval

The last way of testing hypothesis are confidence intervals. I recommend to use them, since they are intuitive and easier to interpret than p-values. To understand confidence intervals, we must remember the difference between a **population** and a **sample.** The population are all people we want to infer to, for example in an election, the population are all citizens eligible to vote in that election. Let us assume, all else equal, that the the vote share in the population for Party A is 24%. This is what we call the **true population parameter.** And our goal is to estimate this parameter with statistical methods, since it is more efficient. Think about Germany, we cannot ask 80 million people before the election to give us their thoughts, so what we do instead, is to draw a representative sample of less citizens from that sample to infer to the population. The problem is that even if the sample is representative, it could be that our sample today gives us a vote share of Party A of 23%, but tomorrow we would get 25% (This could have several reasons, can you think of some?).

In the following, let us assume we draw 1000 samples before the election, to get the vote share of Party A. We know that our true population parameter is 24%. But before we start, we have to set a rule again. This rule is basically the definition of confidence intervals:

-   In 95% of all samples, that could be drawn, the confidence intervals will cover the true population parameter.

So if we draw 1000 samples, in 950 the confidence intervals have to cover the true population parameter:

```{r simulating 100 CIs}
#Since this is a simulation we need to set a seed
set.seed(187)

#We will need to have vectors for the upper confidence interval and the lower one
lower_ci <- numeric(100)
upper_ci <- numeric(100)
estimates <- numeric(100)

#This loop represents 
for(i in 1:length(lower_ci)) {
  
  Y <- rnorm(100, mean = 24, sd = 2)
  estimates[i] <- Y[i]
  lower_ci[i] <- Y[i] - 1.96 * 24 / 10
  upper_ci[i] <- Y[i] + 1.96 * 24 / 10
  
}

#Let us bind both vectors together 
CIs <- data.frame(estimates, lower_ci, upper_ci)

#Print it 
head(CIs)
```

Now we have drawn our 1000 samples and computed our estimates as well as their corresponding intervals. Thus, 1000 samples about the vote share of Party A. Let us check, if the true population parameter is 95% of times within our computed confidence intervals:

```{r plotting simulated CIs}
#Getting the true mean 
true_mean <- 24

#First, we identify those who are not including 24 our true population parameter
CIs$missed <- ifelse(CIs$lower_ci > true_mean | CIs$upper_ci < true_mean, "Out", "In")

#Let us give every sample an identification number 
CIs$id <- 1:nrow(CIs)

#Plotting it 
ggplot(data = CIs) +
  geom_pointrange(
    aes(
      x = estimates, # point value
      xmin = lower_ci, # lower CI
      xmax = upper_ci, # upper CI
      y = id, # y axis - just observation number
      color = missed
    ) # color varies by missed variable
  ) +
  geom_vline(
    aes(xintercept = true_mean), # add vertical line at true_mean
  ) +
  scale_color_manual(values = c("azure4", "red")) + 
  theme_minimal() + 
  labs(
    title = "Confidence Interval for Mean",
    subtitle = "Population mean equals 24",
    x = "Estimates",
    y = "Sample",
    color = "Is true population parameter inside the CI?"
  ) +
  theme(legend.position = "top") + # switch the legend to the top
  scale_x_continuous(breaks = c(seq(15, 30, by = 1)))
```

Well, we can see that the majority of the computed intervals include the true population parameter. But there are three, which are not. That is within our definition.

What is now with confidence intervals for coefficients of linear regression? Well, it is the same story, we compute an estimate, in this case a coefficient. The true population parameter is unknown, but we know that the true population parameter is within 95% of the intervals.

Its calculation is fairly easy and you already saw it:

$$
CI_{lower} = \beta_i - 1.96 * SE(\beta_i) \\\
CI_{upper} = \beta_i + 1.96 * SE(\beta_i)
$$

```{r computing CIs by hand for model 1}
#Let us look at the confindence intervals of model 1 
confint(model1)

#Let us compute the confidence values by hand for the intercept and x 
ci_lower_int <- model1$coefficients[1] - 1.96 * summary(model1)$coef[, "Std. Error"][1]
ci_upper_int <- model1$coefficients[1] + 1.96 * summary(model1)$coef[, "Std. Error"][1]

#Print it 
print(ci_lower_int)
print(ci_upper_int)

#Estimate X 
ci_lower_est <- model1$coefficients[2] - 1.96 * summary(model1)$coef[, "Std. Error"][2]
ci_upper_est <- model1$coefficients[2] + 1.96 * summary(model1)$coef[, "Std. Error"][2]

#Print it
print(ci_lower_est)
print(ci_upper_est)
```

## Multivariate Regression

The world is a complex place and of course if we have a dependent variable Y, let us say for example income, we cannot explain it exclusively by the years of education or exclusively by the profession or any other single factor. Rather it is plausible that all these factors matter. The **Multivariate Linear Regression Model** allows us that we explain the variation in our dependent variable Y with multiple independent variables X. Mathematically, the systematic component changes like this:

$$ Y_i = \beta_0 + \beta_1X_{1i} + \beta_2X_{2i} + ... + \beta_kX_{ki} + \epsilon_i $$

where

-   the index ***i*** runs over the observations (Respondents, Countries,...), *i* = 1,...,*n*

-   $Y_i$ is the dependent variable, the variable we want to explain

-   $X_{1i}$ is the first independent variable or explanatory variable.

-   $X_{2i}$ is the second independent variable or explanatory variable.

-   $X_{ki}$ is the *k*-th independent variable or explanatory variable, where *k* is the number of all our independent variables in our model.

-   $\beta_0$ is the *intercept* of the regression line

-   $\beta_1$ is the *slope* of the regression line of the first explanatory variable.

-   $\beta_2$ is the *slope* of the regression line of the second explanatory variable.

-   $\beta_k$ is the slope of the regression line of the *k*-th explanatory variable, where *k* is the number of all our independent variables in our model.

-   $\epsilon_i$ is the *error term*, thus how our observed data differs from actual population data (e.g. Measurement Error).

In `R`, we can implement a multivariate model very easy with the already known `lm()` function. Let us run a multiple regression with our independent Variable X and our categorical Variable Z, the systematic component for this model looks like this:

$$ Y_i = \beta_0 + \beta_1X_i + \beta_2Z_i + \epsilon_i $$

Let us compute the slopes:

```{r computing model for multivariate analysis}
lm(y ~ x + categorical_variable, data = df) %>% 
  summary()
```

We can see, that this works fine, we only had to add the independent variable in the command such as in our systematic component. Regarding the interpretation it is analogous to the normal linear regression one. For a one-unit increase in our independent variable x, the dependent variable y increases 0.67 units on average, holding all else constant. For category 1 in comparison to category 0, the dependent variable y increases 0.52 units on average, holding all else equal. Now, we can put the calculated coefficients into our systematic component and make predictions:

$$ Y_i = -0.54 + 0.67X_i + 0.52Z_i + \epsilon_i  $$

If we want to visualize this, we would need a three-dimensional coordinate system. Why? Because every independent variable is one-dimension if you want. I said that *k* is the number of our independent variables, technically it is the number of dimensions our model has. I am not a fan of plotting graphs more than three dimensions and I do not recommend it to you either.

## Categorical Variables

As I mentioned in the first chapter, there are different types of variables, let us reconsider them:

|               |               |                                              |
|---------------|---------------|----------------------------------------------|
| **Numeric**   | Numbers       | `c(1, 2.4, 3.14, 4)`                         |
| **Character** | Text          | `c("1", "blue", "fun", "monster")`           |
| **Logical**   | True or false | `c(TRUE, FALSE, TRUE, FALSE)`                |
| **Factor**    | Category      | `c("Strongly disagree", "Agree", "Neutral")` |

Dependent variables must be numeric, when conducting linear regression. However, independent variables can take be scaled differently. Numeric variables are the easiest case, the interpretation is as mentioned in the previous examples. But categorical variables are differently to interpret. Let us inspect the categorical variable in our dataset, called `categorical_variable`:

```{r printing table of categorical variable}
table(df$categorical_variable) 
```

As we can see our data set now contains a categorical variable with two categories, named "A" and "B". Those categories could be anything: Female and Male, bought a product or did not buy a product, vaccine or placebo and so on. What happens, when we now run a model?

```{r model with categorical variable}
#running a model with a categorical variable
model2 <- lm(y ~ categorical_variable, 
             data = df) 

#Getting the summary
summary(model2) 
```

We see a coefficient of 0.64 that is fine. But as you see the category "category_A" is not display, why? Categorical variables are calculated based on so-called "reference categories". R determines the reference category based on alphabetical order. In this case the category "A" is the reference category. The reference category is named like this, since the computed coefficients refer to it. The reference category "A" takes on the value 0. The coefficient of the category B is "0.92". That means that category "B" if the a respondent is part of category "B" than the dependent variable Y increases on average 0.64 units in comparison to category "A", holding all else equal.

That sounds technocratic, let us get an intuition. We add a zero to our code and look at the output:

```{r running the model with + 0}
#running the model
model3 <- lm(y ~ categorical_variable + 0, 
             data = df) 
#getting a summary
summary(model3)
```

The coefficients changed, but not really. Let us subtract the coefficient from category A from category B:

```{r subtracting categories to get coefficient}
#results
result <- coefficients(model3)[2] - coefficients(model3)[1] #coefficents can be extracted this way

#printing it
result
```

We get the same coefficient as above and that is what R does automatically when computing coefficients of categorical variables. The reference category is scaled to 0 and the other categories are following.

### Interaction Effects

One technique, which is important and widely used in statistics are interaction effects. To keep things simple concentrate on our independent variable X and our categorical variable Z. As mentioned, Z shows us the coefficients for our categories "A" and "B". But what if we could further investigate the dynamics of the group? Interaction effects allow us to multiply our variable X by our categorical variable Z. Mathematically our systematic component looks like this:

$$
Y_i = \beta_0 + \beta_1X_i + \beta_2Z_i + \beta_3X_i*Z_i + e_i 
$$

where

-   the index *i* runs over the observations (Respondents, Countries,...), *i* = 1,...,*n*

-   $Y_i$ is the dependent variable, the variable we want to explain

-   $X_i$ is the independent variable or explanatory variable.

-   $Z_i$ is our categorical variable

-   $\beta_0$ is the *intercept* of the regression line

-   $\beta_1$ is the *slope* of the regression line

-   $\epsilon_i$ is the *error term*, thus how our observed data differs from actual population data (e.g. Measurement Error).

In the following, I will show you an example how to use interaction effects. Let's say that I conducted a survey among 1000 respondents and asked them about how many hours they spent on R online courses such as this one. Then I asked about their coding ability. Lastly, I asked if they learned with this course or with other courses (0 = other courses, 1 = this course). Now we want to find out if spending more hours on a course lead to a higher coding ability in R. Furthermore, we want to find out if this course in comparison to other courses lead to a higher ability. To do so, we interact hours spent on courses with the variable indicating if the respondent worked through other courses or this course.

-   Interactions effects can be implemented in R, by simply writing it explicitly into the function `lm()`, either with an asterisks `*` or a double point `:`, let us have a look it:

```{r model with interaction effect}
# Setting seed for reproducibility
set.seed(123)

# Generate hours spent on a course
hours_spent <- runif(100, min = 0, max = 10)

# Generate the course dummy (0 = other courses, 1 = this course)
this_course = sample(c(0, 1), 100, replace = TRUE)

# Generate y with interaction effect
coding_ability <- 2 + 0.5 * hours_spent + 0 * this_course + 
  1.5 * hours_spent * this_course + rnorm(100)

# Create a data frame
df_int <- data.frame(hours_spent, this_course, coding_ability)

# Fit the interaction model
model_interaction <- lm(coding_ability ~  hours_spent * this_course, data = df_int)

# Summarizing models
summary(model_interaction)
```

Interaction effects on their own are not intuitive. To get an intuition we have to graphically plot it. The `sjPlot` package offers the `plot_model()` command, which automatically plots so called predicted probabilities. It would be too much to go into detail about predicted probabilities. What is more important is, that we get a graph, which shows the effect of of the interaction:

-   We call `plot_model()` and include our model with the interaction effect `model_interaction`

-   Then we have to call the type and set it to `type="int"`, which explicitly plots interaction effects.

```{r plotting interaction effect, message=FALSE}
plot_model(model_interaction, type = "int") +
  scale_x_continuous(breaks = seq(0,10, 1)) + 
  labs(title = "Coding Ability after this Course in Comparison",
       x = "Hours spent", 
       y = "Coding Ability in R") +
  scale_color_manual(
    values = c("red", "blue"),
    labels = c("Other Courses", "This Course")
  ) +
  theme_sjplot() +
  theme(legend.position = "bottom", 
        legend.title = element_blank())
```

On the x-axis, we have our independent variable and on the y-axis we do have our dependent variable. So far, so normal. But we see two lines: One line for all observations in category "Other Courses" (red line) and one line for all observations with "This Course" (blue line). That is exactly what an interaction does. For every point in our independent variable a probability for y is computed in one category, and one line for every observation in the other category.

To interpret it we follow two steps: First, looking at the direction of the lines. We can see that X has a positive effect on Y. Both lines are increasing with higher X-values. Now, the interaction allows us to compare the effect from X on Y for all groups of the categorical variable. In our example that means, that observations in group 1 display a higher effect than observations in group 0. That could be for several reasons.

Let us fill this example with life: We are studying **the effect of study hours (x)** on **exam scores (y)** for two different groups of students: those **who attend a preparatory course (group 1) and those who do not (group 0).**

**Interpretation:**

1.  **Direction of the Lines:**
    -   Both lines (for group 0 and group 1) are increasing, indicating that more hours spent on courses lead to higher coding ability in R, on average.
2.  **Effect Comparison**:
    -   The slope for group 1 ( attended this course) is steeper compared to group 0 (attended to other courses).

    -   This means that for students who attended this course, each additional hour of study has a larger positive impact on their coding ability in R compared to those who did attend another course

By visualizing and analyzing the interaction effect, we can draw meaningful conclusions about how different factors (like attending a preparatory course) modify the relationship between study efforts and performance outcomes.

## Outlook

This chapter was an introduction to hypothesis testing and basic statistical applications. Further, you were introduced in the most popular and important model, linear regression. The chapter showed not only how linear regression in R is computed, but also how to check if effects are robust and how the models fits. Also extensions to linear regression, namely multivariate regression and categorical variable were introduced.

Linear regression is the basis of nearly everything. Advanced modelling of different classes of dependent variables to even machine learning techniques are based on linear regression. This was the reason, I did not show just linear regression, but also how vulnerable the model is. Poor modelling will always lead to poor results, so we have to aim to check how good our model fits our data and thus research interest.

Further Links:

-   A course I can recommend if you want to have a detailed deep dive into statistics is ["Introduction to Econometrics with R"](https://www.econometrics-with-r.org/index.html) by Christoph Hanck, Martin Arnold, Alexander Gerber, and Martin Schmelzer.

## Exercise Section

To understand linear regression, we do not have to load any complicated data. Let us assume, you are a market analyst and your customer is the production company of a series called "Breaking Thrones". The production company wants to know how the viewers of the series judge the final episode. You conduct a survey and ask people on how satisfied they were with the season final and some social demographics. Here is your codebook:

| Variable | Description |
|----|----|
| id | The id of the respondent |
| satisfaction | The answer to the question "How satisfied were you with the final episode of Breaking Throne?", where 0 is completely dissatisfied and 10 completely satisfied |
| age | The age of the respondent |
| female | The Gender of the respondent, where 0 = Male, 1 = Female |

Let us generate the data:

```{r}
#setting seed for reproduciability
set.seed(123)

#Generating the data
final_BT <- data.frame(
  id = c(1:10), 
  satisfaction = round(rnorm(10, mean = 6, sd = 2.5)), 
  age = round(rnorm(10, mean = 25, sd = 5)), 
  female = rbinom(10, 1, 0.5)
  )

print(final_BT)
```

### Exercise 1: Linear Regression with two variables

You want to know if age has an impact on the satisfaction with the last episode. You want to conduct a linear regression.

a\. Calculate $\beta_0$ and $\beta_1$ by hand

```{r ch6 exercise 1a, eval=FALSE}

```

b\. Calculate $\beta_0$ and $\beta_1$ automatically with R

```{r ch6 exercise 1b, eval=FALSE}

```

c\. Interpret all quantities of your result: Standard Error, t-statistic, p-value, confidence intervals and the $R^2$.

d\. Check for influential outliers

```{r ch6 exercise 1d, eval=FALSE}

```

### Exercise 2: Multivariate Regression

a\. Add the variable `female` to your regression

```{r ch6 exercise 2a, eval=FALSE}

```

b\. Interpret the Output. What has changed? What stays the same?

c\. Make an interaction effect between age and gender and interpret it!

```{r ch6 exercise 2c, eval=FALSE}

```

d\. Plot the interaction and make the plot nice

```{r ch6 exercise 2d, eval=FALSE}

```

<!--chapter:end:06-data-analysis.Rmd-->

# Loops and Functions

In this chapter I want to introduce to a way to work more efficient. R is a programming language for statistical analysis, but it also includes classical elements of programming. Two main operations are loops and functions. We can automate tasks and the earlier you learn about it the faster you can advance and understand the logic of R. The goal of this chapter is to make you familiar with functions and loops so you know them when you see them.

## Loops

For example, you can use a loop to iterate through a list of numbers and perform calculations on each number, or to go through the rows of a dataset and apply certain operations to each row. Loops provide a way to streamline your code and avoid writing repetitive instructions manually.

There are different type of Loops, but for this course we focus just on the `for` loops, since you will see them also in the QM Tutorial.

### `For` loops

Do you remember my grade example from the first chapter?

```{r ifelse example}

grade <- 4.0

if (grade == 1.0) {
  print("Amazing") 
} else if (grade > 1.0 & grade <= 2.0) {
  print("Good Job")
} else if (grade > 2.0 & grade <= 3.0) {
  print("OK")
} else if (grade > 3.0 & grade <= 4.0) {
  print("Life goes on") 
}

grade <- 3.3

if (grade == 1.0) {
  print("Amazing") 
} else if (grade > 1.0 & grade <= 2.0) {
  print("Good Job")
} else if (grade > 2.0 & grade <= 3.0) {
  print("OK")
} else if (grade > 3.0 & grade <= 4.0) {
  print("Life goes on") 
}

grade <- 4.0

if (grade == 1.0) {
  print("Amazing") 
} else if (grade > 1.0 & grade <= 2.0) {
  print("Good Job")
} else if (grade > 2.0 & grade <= 3.0) {
  print("OK")
} else if (grade > 3.0 & grade <= 4.0) {
  print("Life goes on") 
}

grade <- 2.3

if (grade == 1.0) {
  print("Amazing") 
} else if (grade > 1.0 & grade <= 2.0) {
  print("Good Job")
} else if (grade > 2.0 & grade <= 3.0) {
  print("OK")
} else if (grade > 3.0 & grade <= 4.0) {
  print("Life goes on") 
}

grade <- 1.7

if (grade == 1.0) {
  print("Amazing") 
} else if (grade > 1.0 & grade <= 2.0) {
  print("Good Job")
} else if (grade > 2.0 & grade <= 3.0) {
  print("OK")
} else if (grade > 3.0 & grade <= 4.0) {
  print("Life goes on") 
}

```

I could now write down all my grades and assign them as I did in the first chapter, but there is a way to automatize this process. For that I will use the `For` loop.

First, let us make a vector with grades:

```{r grades vector}
grades <- c(1.7, 3.3, 4.0, 2.3, 1.0)
```

Now, we can directly dive into the loop.

-   Write down `for` and in brackets you define the **loop iterator**, this is the **i** in the loop. Then you define in which object of interest you want to iterate. In our case, the operation should be iterated in the grades vector. I could also write down the number 5, but it is convention to define an object. Why?

-   After closing the brackets you open fancy brackets and write down your function, as you would normally, but this time you need to define how the iterator is used. Since I use the numbers in grades, my iterator needs to be put in brackets, after the name of the grades. Why?

-   And that's it basically

```{r basic loop}

for (i in 1:length(grades)) {
  if (grades[i] == 1.0) {
  print("Amazing") 
} else if (grades[i] > 1.0 & grades[i] <= 2.0) {
  print("Good Job")
} else if (grades[i] > 2.0 & grades[i] <= 3.0) {
  print("OK")
} else if (grades[i] > 3.0 & grades[i] <= 4.0) {
  print("Life goes on") 
}
  
}
```

Loops can look differently:

In this example I have a number vector and my let the console print a sentence, where I vary the number and therefore the sentence changes over every loop

```{r loop num}
# creating num vector
num <- c(1, 2, 3, 4, 5, 249)

# looping through vector
for (i in num) { 
  print(stringr::str_c("This is the ", i, "th Iteration")) 
}
```

### Nested Loops

Because you will eventually encounter them, I will show you shortly nested loops:

First, let us play a game of tic tac toe:

```{r ttt matrix}
#Defining a matrix 
ttt <- matrix(c("X", "O", "X",
                "O", "X", "O",
                "O", "X", "O"), nrow = 3, ncol = 3, byrow = TRUE)
```

We define a loop with an iterator **i** for the rows of the matrix, and we define another one for the columns with the iterator **j**.

Afterwords, we built up the body, in which aim to get information about the matrix and its content. The sentence shows, which rows and columns contain which values.

```{r nested loops}
for (i in 1:nrow(ttt)) {
  for (j in 1:ncol(ttt)) {
    print(paste("On row", i, "and column", 
                j, "the board contains", ttt[i,j]))
  }
}
```

## `apply()` Function Family

### `apply()`

`apply()` takes a data frame or matrix as an input and gives output in vector, list or array. Apply function in R is primarily used to avoid explicit uses of loop constructs. The idea is to apply a function repeatedly to a matrix or data frame: apply(X, MARGIN, FUNCTION)

```{r example matrix}
#Let us create a matrix with random numbers 
mat <- matrix(1:10, nrow = 5, ncol = 6)

#Checking it
head(mat)
```

Assume you now want to calculate the mean of every column:

```{r basic apply function}

apply(mat, 2, mean) #calculating mean 
apply(mat, 2, sum) #calculating sum
apply(mat, 2, sd) #calculating sd


#The corresponding Loop would look like this: 
for (i in 1:ncol(mat)) {
  mean_col <- mean(mat[, i])
  print(mean_col)
}
```

The `apply()` function is useful especially if you are working with dimensional bodies and want to calculate anything. However, they can not keep up with the flexibility of loops, you should be aware of that.

### Notes

Loops and the apply function are widely used in programming. However, this is no programming course, it is an introduction, so now you have an idea what is happening, if you are seeing those two things in the scripts. But if you are interested in this topic, please read into `while` loops and `repeat` loops. The `apply()` function is part of a family: `sapply()`, `lapply()`, `tapply()` are also in that family.

## Writing your own functions

We can again safe a lot of time and be more efficient by writing our own function.

-   First, you need to define a name for your function#
-   Afterwards, you write down the command with the `function()` command. In to your brackets you put your variables. Later your input follows those variables.
-   After the fancy brackets, you define your operation with your predefined variables.
-   Lastly, you want the function to return your quantity of interest and close the fancy brackets
-   Afterwards you have a function saved and can operate with it

```{r own function}

#My function is just a sum

add <- function(x, y) { 
  
  result <- x + y
  return(result)
}

add(2,7) #Now I can use my function

```

Let us calculate the area of a circle

```{r area of circle}

aoc <- function(radius) {
  pi <- 3.14159
  
  area <- pi * radius^2
  
  return(area)
}

aoc(5)
```

Let us combine what we have learned in this chapter with a meeting of animals. You do not have to understand the code, and I also do not want to explain it, but just to make clear what loops and functions are able to do. And all packages are written based on functions and loops, so it is useful to get an overview how it could look like.

In this loop, we let R print out what persons in a classroom are doing. Before that we identify where the individuals are sitting. And the seating order in a class, is nothing more that a matrix right? We have rows and columns. This loops identifies in which row (i) and in which column (j) an individual sits. Moreover, the values in the matrix are student ids and the loop knows the id of students and prints out where which student sits. So this is an unnecessary loop, but I think a good example. Imagine the class are 1000 of people, then this loop would be necessary. As I said you do not need to completely understand it. Its purpose is to illustrate what you can do in R.

```{r student sitting order}
# The function
print_classroom <- function(x) {
  for (i in 1:length(x)) {  # Outer loop iterates over rows
    for (j in 1:length(x[[i]])) {  # Inner loop iterates over columns
      student <- x[[i]][j]
      if (student == 1) {
        comment <- "Alice"
      } else if (student == 2) {
        comment <- "Bob"
      } else if (student == 3) {
        comment <- "Cathy"
      } else if (student == 4) {
        comment <- "David"
      } else if (student == 5) {
        comment <- "Eva"
      } else {
        comment <- paste("Unknown student", student, "is doing something interesting.")
      }
      cat("At row", i, "column", j, ":", comment, "\n")
    }
  }
}

# Example usage
seating_order <- list(
  c(1, 5, 2),
  c(4, 3, 7)
)

#Checking it
print_classroom(seating_order)
```

## Outlook

This course was a short introduction to automatize work and write efficient code: Loops and Functions. Since both concepts can get really complicated really fast, and in the beginning of your R-journey you will be less likely to use loops and functions. At some point however will you be confronted with loops and functions. And the earlier you see them, the better.

-   For further information on programming in R, I recommend ["Hands-On Programming with R"](https://rstudio-education.github.io/hopr/index.html) by Garret Grolemund. In this book, the authors shows projects where he uses functions and loops, so it is a nice illustration of the usage of loops and functions.

## Exercise Section

### Exercise 1: Writing a loop

Write a `for` loop that prints the square of each number from 1 to 10

```{r ch5 exercise 1}

#Assigning an object for a better workflow
number <- 10

#The Loop 




```

### Exercise 2: Writing a function

Write a function that takes the input x and squares it:

```{r ch5 exercise 2, eval=FALSE}

#Defining a function for squaring

sq <- function (x) {
  
  
  
}

#Defining a vector containing a vector from 1 to 10 
numbers <- c(1:10) 

#Applying the number 
sq(numbers)

```

### Exercise 3: The midnight Formula

This is the midnight formula separated in two equations:

$x_{1,2} = \frac{-b \pm \sqrt{b^2 - 4ac}}{2a}$

Make **one function** for the midnight formula, so the output are $x_1$ and $x_2$. Test it with a = 2, b = -6, c = -8

**Hint**: You need two split up the formula into two equations with two outputs.

```{r ch5 exercise 3, eval=FALSE}

mnf <- 


mnf(2, 6, 8)
```

<!--chapter:end:07-loops-and-functions.Rmd-->

# Further Explanations

In this Chapter we start into the main strength of R: Data Analysis. After cleaning our data we are ready to analyze it, but it is always recommend to get an overview of our data with exploratory data analysis. This yields the advantage that we can detect first trends and potential problems with our data. Furthermore, we will get a better understand of the Data Generating Process and how data is generated.

Let us tart with loading packages and you will see, that we are loading a lot of packages due to the fact, that R is a statistical analysis software and therefore it is only natural that Statisticians and Data Scientists, who use R, are also writing packages to (1) make their own life easier, but also (2) further developing R.

```{r loading packages eda}
pacman::p_load("dplyr", "tidyr", "ggpubr", "gapminder",
               "kableExtra", "car")
```

## Probability Theory

**Before diving into Exploratory Data Analysis and Linear Regression, it's worth taking a moment to look at Probability Theory.** This is crucial because probability forms the foundation of statistics: it allows us to make statements that include uncertainty. Instead of saying something definite like *"It will rain tomorrow,"* probability allows us to say, *"There is a 70% chance of rain tomorrow."* That means you might want to take an umbrella  but theres still a 30% chance you wont need it.

### Random Variable

We are interested in Data! And Data is nothing more than a bunch of **Random Variables.** Technically speaking, a Random Variable is a numerical outcome of a random process or experiment. Informally, let us say you want to collect Data about your family, just image all of them and now randomly pick some information about them, their height, their age, gender, glasses or not, cooking abilities, anything, and now you just need to find a system of assigning a number (e.g. height in cm, age in years,...) and et viol you collected a random variable. Note that a random variable is the mutually exclusive outcome of a random process.

Rolling a dice and getting the result of the dice could be a random variable, let us role the dice and say we got a 3:

```{r rolling one dice}
#rolling one dice
dice_role <- 3
print(dice_role)
```

We now collected our first random variable. On its own, a random variable is not really insightful, but what if we do not collect one random variable, but two? Or three? Or thousand?

#### Discrete and Continuous Variables

Before answering the question above, there is an important distinction to make between the types of random variables we can collect. There are **discrete** and **continuous variables**:

-   A **discrete variable** can take on a **finite or countably infinite** set of distinct values, usually integers. These values are **separated** and cannot take on values in between. For example: Number of children, coin flips, number of goals.

-   A **continuous variable** can take on **any value** within a given range, including **fractions and decimals**. Between any two values, there are **infinitely many** possible values. For example: Height, weight, temperature, time.

#### Population vs Sample Distribution

Another thing to clarify before going on with Probability Distributions, is the difference between Populations and Samples.

-   **Population**: The population is the entire group we are theoretically interested in studying, based on our research question. For example, if we want to know the **average height of German women**, then the **population** consists of **all German women**. The **population mean** is the true average height of all German women.

-   **Sample:** Asking every **German woman** (42.3 million, according to the Federal Office of Statistics, 2023) would indeed be impractical. Instead, we use **statistical sampling** to gather data from a smaller, more manageable subset of the population. **Probability theory** helps ensure that this sample is representative of the entire population.

    For the sample to be representative of the population, it needs to meet two key criteria:

    1.  **Sample Size:** The sample must be large enough to accurately reflect the population's characteristics.

    2.  **Randomness:** Every individual in the population must have an equal chance of being selected for the sample. This ensures that the sample is **random** and not biased in any way.

    When these conditions are met, we can confidently generalize the findings from the sample to the larger population. For example, we calculate the **sample mean** (the average of the sample). This is the true value for the sample itself, but we use it to make an inference about the **population mean**.

    However, the sample mean will not always be exactly equal to the population mean. This is because of **sampling variability**  there's always some random variation in the sample that can cause deviations from the population mean.

    Despite this, **if the sample is large enough** and **chosen randomly**, the sample mean will tend to be **close** to the population mean on average, and the difference between them decreases as the sample size increases.

The difference of population and sample is crucial and will be important in the later section of this chapter!

### Probability Distributions

#### Probability Mass Functions (PMF)

##### Uniform Distributions

We can let **R roll the dice for us** using the `sample()` function. This function takes in the range of possible outcomes, the number of observations to draw, and some additional options (which we wont worry about for now):

```{r sampling the dice}
dice_rolls <- data.frame(
  roll = sample(c(1:6), 1000, replace = TRUE)
)
```

This simulates rolling a fair six-sided die 1,000 times.

Let us plot it:

```{r simple dice rolls}
# Plot with ggplot
ggplot(dice_rolls, aes(x = factor(roll))) +
  geom_bar(fill = "#89CFF0", color = "gray") +
  labs(
    title = "Distribution of 1,000 Dice Rolls",
    x = "Dice Face",
    y = "Frequency"
  ) +
  theme_minimal()
```

What weve created here is called a **distribution of a discrete variable**. A **discrete variable** has a countable number of distinct outcomes. In this case, our die can only land on one of six values: 1 through 6  nothing in between.

This explicit type of distribution is called uniform distribution. A **uniform distribution** is one in which **all outcomes are equally likely**  in our case, each face of the die (1 through 6) has the same probability of occurring. Thus, the probability of one outcome is just 1/n, thus 1/6 for each outcome. Let us plot that as well:

```{r generating probabilities}
# Define the outcomes and their corresponding probabilities
uniform_dis <- data.frame(
  Outcome = factor(1:6),
  Probability = rep(1/6, 6)
)

# Plot it
ggplot(uniform_dis, aes(x = Outcome, y = Probability)) +
  geom_point() +
  labs(
    title = "Uniform Probability Distribution of a Fair Die",
    x = "Die Face",
    y = "Probability"
  ) +
  theme_minimal()
```

This a so-called Probability Mass Function (PMF). This is the term for probability distributions of discrete variables. It is useful since we can determine the probability of an event with it, for example the probability of getting 3:

$$
P(X = 3) = \frac{1}{6}
$$

What if we start to cumulate the probabilities, like in the following table:

| Outcome                    | 1   | 2   | 3   | 4   | 5   | 6   |
|----------------------------|-----|-----|-----|-----|-----|-----|
| **Probability**            | 1/6 | 1/6 | 1/6 | 1/6 | 1/6 | 1/6 |
| **Cumulative Probability** | 1/6 | 2/6 | 3/6 | 4/6 | 5/6 | 1   |

We can plot the cumulative Probability by calculating it with the `cumsum()` function:

```{r cumsum}
# Calculating the cumsum
uniform_dis$cumsum <- cumsum(uniform_dis$Probability)

# Plot it
ggplot(uniform_dis, aes(x = Outcome, y = cumsum)) +
  geom_point() +
  labs(
    title = "Cumulative Distribution of a Fair Die",
    x = "Die Face",
    y = "Probability"
  ) +
  ylim(0,1) +
  theme_minimal()
```

With this graph we can directly see the cumulative distribution for several outcomes, for example the cumulative probability for the outcome 3 is 0.5, thus 50%, so with a 50% probability I will get either 1, 2 or 3.

##### Bernoulli Distributions

In this part I want to introduce you a new type of distribution: The bernoulli distribution. It is super easy it takes on the value 1 with probability $p$ and the value 0 with the probability $q = 1 -p$. In more simple terms, it displays the outcomes for any experiment that has yes/no answers. The classical example is head and tails, you throw a coin and it either shows head or tails. Let us simulate data and plot it:

```{r bernoulli fair coin}
# Simulate 1000 tosses of a fair coin (1 = Head, 0 = Tail)
set.seed(101)  # for reproducibility
fair_coin <- data.frame(
  outcome = factor(rbinom(10, 1, 0.5), levels = c(0, 1), 
                   labels = c("Tail", "Head"))
)

# Plot the results
ggplot(fair_coin, aes(x = outcome)) +
  geom_bar(fill = "#89CFF0", width = 0.35) +
  labs(
    title = "Simulation of 10 Tosses of a Fair Coin",
    x = "Outcome",
    y = "Count"
  ) +
  theme_minimal()
```

As we probably expected, if we throw a fair coin ten times we will get approximately 5 Tails and 5 Heads. Furthermore, we plotted our first Bernoulli Distribution.

This is an example for a fair coin, what if our coin was "not fair" or "unfair", thus a "biased" coin, we could plot that as well:

```{r bernoulli biased coin}
# Simulate 1000 tosses of an unfair coin (1 = Head, 0 = Tail)
set.seed(102)  # for reproducibility
unfair_coin <- data.frame(
  outcome = factor(rbinom(10, 1, 0.28), levels = c(0, 1), 
                   labels = c("Tail", "Head"))
)

# Plot the results
ggplot(unfair_coin, aes(x = outcome)) +
  geom_bar(fill = "#89CFF0", width = 0.35) +
  labs(
    title = "Simulation of 1000 Tosses of an unfair Coin",
    x = "Outcome",
    y = "Count"
  ) +
  theme_minimal()
```

We can see that the bins are not equal the coin has a tendency for Tail. The Probability for getting Tail is higher than getting Head with this Unfair Coin.

How does the Probability Distribution of a Bernoulli Distribution looks like? Before answering this question, we have to calculate the probability of getting exact 7 Tails and 3 Heads as we got. I leave the maths out, thankfully R can do that with the `dbinom()` function, it takes in "x" which is the number of outcomes we got, thus our Tail = 7. Then we have to put in the size, which is the number of trials, thus 10 and lastly the probability, if it is a fair coin 0.5. I computed the biased coin with a probability of 0.28.

Let us calculate for the fair and the biased coin, the probability of our results:

```{r dbinom}
# fair coin
dbinom(
  x = 5,
  size = 10,
  prob = 0.5
)

# biased coin
dbinom(
  x = 3,
  size = 10,
  prob = 0.28
)
```

What you can see here is the probability of getting exact 5 heads for the fair coin and 3 heads for the unfair coin. So to get exact 5 heads with the fair coin has a probability of 24.6% and the probability of getting exact 3 heads has a probability of 26.4%.

##### Binomial Distribution

How does it come, that although we have compute a probability of 50% of a success in each trial, that we only get a probability of 5 heads with 24.6% probability and 26.4% of getting 3 heads with a biased coin?

We can solve this problem by plotting a so-called Binomial Distribution. It displays the number of **successes** in a fixed number of independent **yes/no, thus head and tails (Bernoulli)** trials.

It gets clear, when we plot all Probabilities for both coins for each possible outcome for head, thus 10, since we have 10 trials:

```{r binomial distributions}
# Create theoretical distributions
theoretical_probs <- data.frame(
  heads = rep(0:10, 2),
  coin_type = rep(c("unbiased", "biased"), each = 11),
  prob = c(dbinom(0:10, size = 10, prob = 0.5),
           dbinom(0:10, size = 10, prob = 0.28))
)

# Plot
ggplot(theoretical_probs, aes(x = heads, y = prob)) +
  geom_point(size = 1.5, color = "black") +
  facet_wrap(~ coin_type, 
             labeller = as_labeller(c(
               biased = "Biased Coin (p = 0.28)", 
               unbiased = "Unbiased Coin (p = 0.5)"))) +
  labs(title = "Theoretical Binomial Distribution of Number of Heads in 10 Flips",
       x = "Number of Heads",
       y = "Probability") +
  scale_x_continuous(breaks = 0:10) +
  theme_bw()
```

Now, the reason behind the probabilities for the outcome for heads becomes clear, the computed probability deviates from the outcome probability because in some cases the trials follow the computed probability but in some cases it deviates and head becomes 2 or 4 for the biased coin and 4 or 6 for the unbiased coin.

At this point, one thing becomes clear: the **computed probability** represents the *expected likelihood* of obtaining a specific number of heads **on average** over a large number of repeated experiments. Why "on average"? Because probability theory accounts for randomness  meaning that in any single simulation (say, flipping a coin 10 times), we might observe different outcomes: sometimes 5 heads, sometimes 2, or even 9.

For a fair coin, the theoretical expectation is 5 heads in 10 flips. However, this doesnt mean we will always get exactly 5. The actual results vary due to chance. In the case of a biased coin with a 0.28 probability of heads, we might expect around 2.8 heads on average  but again, we will only ever see whole numbers like 2 or 3 in practice.

In short, these deviations from the expected value arise from randomness. While the true probability (e.g., 0.5 or 0.28) describes the long-term pattern, it does not guarantee specific outcomes in short runs. Instead, it tells us what to expect **most frequently** over a large number of repetitions.

This concept will become important later again, but before that let us have a look at the cumulated distribution function (CDF) of the probability distribution function (PDF):

```{r cumulative probabilities}
# Fix cumulative sum: compute it within each coin_type group
theoretical_probs <- theoretical_probs %>%
  group_by(coin_type) %>%
  arrange(heads, .by_group = TRUE) %>%
  mutate(cumsum_prob = cumsum(prob)) %>%
  ungroup()

# Plot cumulative probabilities
ggplot(theoretical_probs, aes(x = heads, y = cumsum_prob)) +
  geom_point(size = 1.5, color = "black") +
  facet_wrap(~ coin_type,
             labeller = as_labeller(c(
               biased = "Biased Coin (p = 0.28)",
               unbiased = "Unbiased Coin (p = 0.5)")),
             nrow = 2) +
  labs(
    title = "Cumulative Distribution of Number of Heads in 10 Flips",
       x = "Number of Heads",
       y = "Cumulative Probability") +
  scale_x_continuous(breaks = 0:10) +
  theme_bw()
```

The CDF is quite interesting, we see that in round about 70% of all experiments of the biased coin will result in 3 or less heads and on the other side the unbiased coin will result in round about 80% in 6 or less heads.

#### Probability Density Functions (PDF)

##### **Normal Distributions**

Thus far we looked at the two typical distributions for **discrete variables**. In the following, we will look at distributions for **continuous variables**.

The probability distributions of the Bernoulli trials we saw in the part follow the form of the arguably most important distribution: The normal distribution. This distribution is also called a Gaussian distribution after its inventor Carl-Friedrich Gauss, or Bell Curve due to its shape. Let us have a look at it at the Probability Distribution Function (PDF):

```{r pdf, error=FALSE}
# simulating the data
set.seed(123)
snd <- data.frame(
  sample = rnorm(1000000, mean = 0, sd = 1)
  )

# Plot it
ggplot(snd, aes(x = sample)) +
  geom_density() +
  labs(title = "Normal Distribution",
       x = "Value of Random Variable",
       y = "Density") +
  scale_x_continuous(breaks = -5:5,
                     limits = c(-5,5)) +
  theme_minimal()
```

The normal distribution has a lot of properties:

-   The distribution is perfectly symmetrical around the mean

-   The mean equals the median equals the mode

-   The shape is bell-like, with most values clustered around the mean and tails tapering off in both directions.

-   About 68% of the data fall within 1 standard deviation of the mean, about 95% fall within 2 standard deviations and around 99.7% fall within 3 standard deviations.

-   The distribution is defined by its mean ($\mu$) and its standard deviation ($\sigma$).

-   The tails extend infinitely in both directions but never touch the x-axis (approach zero asymptotically).

-   The area under the density curve represents probability, and it sums up to exactly 1. That is the reason the reason why we talk about Probability Density Functions, because we can determine the probability for each data point by calculating the density of the curve.

-   It is unimodal, thus has only one peak.

-   As Standard Normal Distributions is a special case where the mean, $\mu$ = 0 and the standard deviation, $\sigma$ = 1

We already saw the Cumulative Density Function in the Bernoulli Trials, but let us have a look again:

```{r cdf}
# Plot empirical CDF
ggplot(snd, aes(x = sample)) +
  stat_ecdf(geom = "step", color = "black") +
  labs(title = "Cumulative Distribution Function (CDF)",
       x = "Value of Random Variable",
       y = "Cumulative Probability") +
  theme_minimal()
```

We can use the cumulative distribution function (CDF) to determine the probability that a random variable takes on a value **less than or equal to** a specific value. For example, with a standard normal distribution, the CDF tells us there's a 50% chance that the random variable will take on a value **less than or equal to 0**.

##### Central Limit Theorem (CTL)

In the examples of the Chapter \@ref(working-with-distributions) we will use normal distributions due to its significance for natural and social sciences, here is why:

-   When we collect random variables such as height, IQ, sizes of snowflakes, milk production of cows follows a normal distribution.

-   Due to its elegant mathematical properties like the symmetry around the mean, the 68-95-99.7 rule, and it is defined by only two parameters, the mean and the standard deviation.

-   Statistical Analysis assumes in many models a normal distributions, because its simplicity, for example the it is the building block for machine learning, finance, etc., and for statistical models such as regression analysis assumes also normality.

But besides these, let's say "convenient reasons", there is also one mathematical reason why it is so frequently used in statistics, **the central limit theorem**. I will not dive into its maths, but it is quite easy to understand:

The Central Limit Theorem states that, under certain conditions, the distribution of the **sample means** of a large number of independent and identically distributed random variables will approximate a normal distribution**regardless of the shape of the original population distribution**. In other words, as the sample size increases, the distribution of the **means of these samples** becomes increasingly normal.

Let us visualize that! Remember the sample of our dice? What if we draw many, many samples and collect their sample means, and then plot the sample means? We would expect a uniform distribution, right? Because every outcome has the same probability to happen, exactly 1/6?

Let us draw samples and look what happens. In the following I first define a function that rolls the dice 1000 times. In the next step, I use the function to draw a sample. I first take one sample, thus role the dice 1000 times and save the result, and plot it, I do the same for 2, 3, and 4 samples:

```{r central limit theorem}
set.seed(123)  # For reproducibility

# Function to simulate rolling a die n_rolls times, repeated n_sim times
simulate_dice_means <- function(n_rolls, n_sim = 1000) {
  replicate(n_sim, mean(sample(1:6, n_rolls, replace = TRUE)))
}

# Simulate sample means for different numbers of rolls
means_1 <- simulate_dice_means(1)
means_2 <- simulate_dice_means(2)
means_3 <- simulate_dice_means(3)
means_4 <- simulate_dice_means(4)

df <- bind_rows(
  data.frame(mean = means_1, rolls = "1000 Rolls"),
  data.frame(mean = means_2, rolls = "2000 Rolls"),
  data.frame(mean = means_3, rolls = "3000 Rolls"),
  data.frame(mean = means_4, rolls = "4000 Rolls")
)

# Plotting
ggplot(df, aes(x = mean)) +
  geom_density(fill = "skyblue", alpha = 0.6) +
  facet_wrap(~ rolls, scales = "free", ncol = 2) +
  labs(title = "Central Limit Theorem Demonstration with Dice Rolls",
       x = "Sample Mean",
       y = "Density") +
  scale_x_continuous(breaks = 1:6,
                     limits = c(1,6)) +
  theme_minimal()
```

Here we can clearly see how the distribution of the sample means converge to the form of a normal distribution. That leaves only one question, why does the mean of the sample means converge to 3.5?

##### **Law of Large Numbers**

The phenomenon that the sample converges to 3.5 is due to the Law of large numbers. It is a mathematical law that states that if the sample size increases, the sample mean gets closer population mean. The true population mean for our dice example is 3.5 (1+2+3+4+5+6/6 = 3.5), thus our expected value is our true population mean. According to the Law of large numbers, the more the dice is rolled the more the sample mean converges to 3.5 and that is what we see in the density plot in the bottom, right corner. When the dice is rolled 4000 times in total the curve gets closer to the sample mean.

#### Different Types of Distributions

Alright, that was a lot of theory, but it is crucial to understand, because probability distributions are the basic ground for the statistical models to analyse our data. And here is the deal, knowing the distributions of our data leads to configuration possibilities later in the models, so that our models can better fit our data! In the next chapter, I hope that will become clear!

To conclude this chapter, I will briefly introduce you in different types of distributions and their interpretations:

```{r, message=FALSE, warning=FALSE}
# Create a data frame with different distributions
x_vals <- seq(0, 10, by = 0.1)

dist_df <- data.frame(
  x = rep(x_vals, 4),  # only 4 now, Poisson removed from here
  distribution = rep(c("Exponential (=1)", 
                       "Gamma (shape=2, rate=1)", 
                       "Chi-Square (df=3)", 
                       "t-Distribution (df=10)"), each = length(x_vals)),
  y = c(
    dexp(x_vals, rate = 1),
    dgamma(x_vals, shape = 2, rate = 1),
    dchisq(x_vals, df = 3),
    dt(x_vals - 5, df = 10)  # shift t-distribution for better display
  )
)

# Add Poisson and F-distribution
poisson_vals <- data.frame(
  x = 0:10,
  y = dpois(0:10, lambda = 3),
  distribution = "Poisson (=3)"
)

f_vals <- data.frame(
  x = x_vals,
  y = df(x_vals, df1 = 5, df2 = 10),
  distribution = "F-Distribution (df1=5, df2=10)"
)

# Combine all distributions
plot_df <- bind_rows(dist_df, poisson_vals, f_vals)

# Plot using facet_wrap
ggplot(plot_df, aes(x = x, y = y)) +
  geom_line(data = filter(plot_df, !distribution %in% c("Poisson (=3)")),
            color = "steelblue", linewidth = 1) +
  geom_point(data = filter(plot_df, distribution == "Poisson (=3)"),
             color = "steelblue", size = 1) +
  facet_wrap(~ distribution, scales = "free", ncol = 3) +
  labs(title = "Overview of Common Statistical Distributions",
       x = "x", y = "Density / Probability") +
  theme_minimal(base_size = 14)
```

Let us go shortly through these distributions and their meanings:

-   Chi-Square: This distribution arises when squaring standard normal values. Its fundamental in hypothesis testing (e.g., the Chi-Square Test for independence) and constructing confidence intervals for population variances.

-   Exponential: It describes the time between two independent events in a Poisson process. Essential for survival analysis and modeling waiting times.

-   F-Distribution: It describes the ratio of two sample variances, each from normally distributed populations. Used in comparing model fits.

-   Gamma: A flexible distribution that models the waiting time until multiple independent events occur. It generalizes the exponential distribution and is widely used in Bayesian statistics, reliability engineering, and insurance modeling.

-   Poisson: Models the number of events occurring in a fixed time or space interval when events happen independently at a constant average ratelike the number of emails per hour. Commonly used for count data and in event-based modeling.

-   t-Distribution: Used when estimating the mean of a normally distributed population in small samples. It accounts for the added uncertainty from estimating the standard deviation. Essential in hypothesis testing and regression analysis.

### Working with Distributions

After the fairly theoretical part before, let us work with distributions hands-on in `R`: For this purpose let us simulate data. For the following part, I will simulate a normal distribution and introduce you to the first central function `rnorm()`. This functions takes in three arguments, the number of observations **n**, the **mean** and the standard deviation, **sd**. And again, the beauty of the normal distribution becomes clear, we do not need to tell R more than that, since normal distributions can be generated with only the mean and the standard deviation and on top we can define the number of observations.

We will simulate the normal distribution for IQ. Let the mean be 100 and the standard deviation 15. These are not randomly selected, most samples of the IQ result in this mean and sd:

```{r simulating sample}
#Setting seed for reproduciability
set.seed(123)

#Simulating sample
sample_iq <- data.frame(
  height = rnorm(10000, mean = 100, sd = 15))

#Plotting it
ggplot(sample_iq, aes(x=height)) +
  geom_density(linewidth = 1, color = "#E35335") +
  labs(
    x = "IQ", 
    y = "Frequency"
  ) +
  scale_x_continuous(breaks = seq(40, 160, 20),
                     limits = c(40, 160)) +
  theme_minimal()
```

#### The 68-95-99.7 Rule

Remember this Rule? Without any complex mathematics, we can determine, where the most values are located, just by by subtracting the standard deviation from the mean:

-   68% of all values falls within one standard deviation of the mean:

    $$
    \mu + \sigma = 100 + 15 = 115
    $$

    $$
    \mu - \sigma = 100 + 15 = 85
    $$

    We can interpret these values as follows, when your IQ falls between 115 and 115 you share the fate of 68% of our sample.

-   95% of all values falls within two standard deviations of the mean:

    $$
    \mu + 2\sigma = 100 + 2 \cdot 15 = 130
    $$

    $$
    \mu - 2\sigma = 100 - 2 \cdot 15 = 70
    $$

    If an IQ is not within 70 and 130 then it is either in the 2.5 % lowest or 2.5 highest values. An IQ smaller than 70 means being among the 2.5% lowest IQs and on the other side, if the IQ is above 130 than your IQ is higher than 97.5% !

-   99.7% of all values falls within three standard deviations of the mean:

    $$
    \mu + 3\sigma = 100 + 3 \cdot 15 = 145
    $$

    $$
    \mu + 3\sigma = 100 - 3 = 55
    $$

If the IQ value is outside of 145 or 55, the value is either at the top 0.15 or if it is below 55 it is at the bottom 0.15.

#### Determining the probability of single values?

The 68-95-99.7 rule is a helpful rule of thumb for understanding normal distributions. However, it's not very precise. If we want to determine the **exact probability** of a value in our distribution, we need to use the **Probability Density Function (PDF)** of the normal distribution.

Now, because a continuous distribution has **infinitely many possible values**, the probability of observing **any exact value** (like exactly 100) is actually **zero**. That's why we work with **probability densities**, not exact probabilities at a single point.

Instead of asking "What is the probability of exactly 100?", we ask "What is the **density** at 100?"  which reflects how likely it is to observe values **around** 100.

In R, we can use the `dnorm()` function to compute this **density**. It takes three arguments:

-   **x**: the value we're interested in

-   **mean**: the mean of the distribution

-   **sd**: the standard deviation

Let us calculate the probability of having an IQ of exactly 100, 87 and 140:

```{r dnorm}
dnorm(x = 100, mean = 100, sd = 15) #Probability of 100 IQ
dnorm(x = 87, mean = 100, sd = 15) #Probability of 65 IQ
dnorm(x = 140, mean = 100, sd = 15) #Probability of 135 IQ
```

So the probability of having an IQ of 100 is 2.6%, getting an IQ of 87 is 1.8% and lastly the probability of having an IQ of 140 is 0.07%.

#### Getting the probabilities until a certain value

The function `pnorm()` in R gives the **cumulative probability** up to a certain value under the normal distribution. In simpler terms, it tells you **how likely it is that a value drawn from a normal distribution is less than or equal to a given number**.

It takes the following arguments:

-   `q`: the value you are interested in (the quantile)

-   `mean`: the mean () of the distribution

-   `sd`: the standard deviation () of the distribution

```{r pnorm}
pnorm(q = 100, mean = 100, sd = 15) #Probability of 100 IQ
pnorm(q = 87, mean = 100, sd = 15) #Probability of 65 IQ
pnorm(q = 140, mean = 100, sd = 15) #Probability of 135 IQ
```

Now what do those values mean? The interpretation is quite easy, 0.5 means that an IQ of 100 is higher than 0.5, thus 50% of our distribution has lower values than 100. In simpler terms. If a person has an IQ of 100, the person has an IQ smarter than 50% of all other people. If a person has an IQ of 87, the person has an IQ higher than 19.3% of the sample/population. For an IQ of 140 we can say that the person has a higher IQ than 99.6% of all other persons in the sample. In other ways, with an IQ of 140, a person is part of the top 1%.

What if we want to know which value we need, to be in a certain percentile? Well, for this case R has the `qnorm()` function.

-   It takes in the argument **p**, which takes probability, we want the corresponding value and of course our two parameters mean and standard deviation

```{r qnorm}
qnorm(p = 0.5, mean = 100, sd = 15) #Probability of 100 IQ
qnorm(p = 0.1930623, mean = 100, sd = 15) #Probability of 65 IQ
qnorm(p = 0.9961696, mean = 100, sd = 15) #Probability of 135 IQ
```

We see that we get our estimated values before. This function is especially useful when setting thresholds (e.g., what IQ is needed to qualify as a "genius") or determining cutoffs in standardized testing.

#### Conclusion

This concludes our chapter on probability theory. We explored different types of variables, learned about **Probability Mass Functions (PMFs)** and **Probability Density Functions (PDFs)**, and finally saw how to work with probability distributions in R.

At first glance, this might seem like basic theory. However, a solid understanding of distributions is essential: it allows us to **detect and correct mistakes** when building statistical models, which often **rely on distributional assumptions**.

Moreover, understanding distributions shows us **why** statistical analysis works in the first placeeven in the presence of uncertainty. It allows us to assign **probabilities to predictions**, interpret results meaningfully, and quantify how likely certain outcomes are.

## Regression Diagnostics

### Model Fit: Bivariate Regression

Now the linear regression has a lot of assumptions, it is not like we can run the model every time how we want. Since it is a model, it makes assumptions and instead of just assuming them to be right, we can test them. To techniques to test them are called **Measures of Fit**. Because they test how much our data fits the data. Let us have a look at the assumptions and how we can test them:

#### Measures of Fit

##### Residuals

1.) Calculating Residuals:

$$
Residuals = y_i - \hat{y_i} = y_i - (\hat{\beta_0} - \hat{\beta_1}x_i) 
$$

where

-   $y_i$ = our actual observed values of our dependent variable (`df$y`)

-   $\hat{y_i}$ = are our predicted values based on our OLS estimator

Reconsider the graph at 2.2.1, the residuals are basically the red lines, thus the distance from the line to the point. We can calculate the residuals for our graph:

```{r sumulating data}
#Getting the data
set.seed(123) # For reproducibility

n <- 30 

x <- runif(n) * 10 

categorical_variable <- factor(sample(c(0, 1), n, replace = TRUE))

y <- 0.8 + 1.6 * x + rnorm(n, 0, 3)

df <- data.frame(x,y, categorical_variable)

# And or Model
#running a linear regression
model1 <- lm(y ~ x, 
             data = df) 
```

```{r calculating residuals}
#First, we calculate the predictions for y
df$y_hat <- 1.6821 + 1.5394*df$x 

#We get the Residuals by subtracting our actual y from y_hat
df$residuals <- df$y - df$y_hat 

#cheking it
head(df) 

#We could have done that automatically with R as well !  
df$residuals_auto <- residuals(model1)

#Checking it
head(df)
```

Let us have a look at a so-called **Residual Plot**: On the x-axis you plot the **fitted values**, thus our `y_hat`. On the y-axis you plot the residuals, thus $y-\hat{y}$. Then you plot a horizontal line at y = 0. All dots on those lines show us the values correctly predicted by our model.

```{r residual plot}
ggplot(df, aes(x, residuals_auto)) + 
  geom_point() + 
  geom_hline(yintercept = 0) +
  scale_y_continuous("Residuals", seq(-6, 6, 1), 
                     limits = c(-6, 6)) +
  scale_x_continuous("Fitted Values", seq(0, 10, 1), 
                     limits = c(0, 10)) +
  theme_bw()
```

##### Homoskedasticity and Heteroskedasticity

One assumption of linear regression is that the variance of the error term is not correlated with our independent variable. Well, that is quite technocratic and means basically, that the residuals are distributed equally over the independent variables. Let us plot it to get a visual intuition:

```{r homo and hetero plot, message=FALSE, warning=FALSE}
#setting seed for reproduciability
set.seed(123)

# Generate some data
x <- runif(150, 0.05, 1)
e <- rnorm(150, 0, 0.5)

#homoskedastic data 
y_homo <- 2 * x + e 
#heteroskedastic data 
y_hetero <- 2 * x + e*x^2 
#making a data frame with both data
df_homo_hetero <- data.frame(x, y_homo, y_hetero)

# Scatterplot with homoscedasticity
homoskedastic_plot <- ggplot(df_homo_hetero, aes(x = x, y = y_homo)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE) +# Add linear regression line
  scale_y_continuous("Y", seq(-0.5, 3.5, 0.5), limits = c(-0.5, 3.5)) +
  labs(title = "Homoskedastic Plot") +
  theme_minimal()

# Scatterplot with heteroscedasticity
heteroskedastic_plot <- ggplot(df_homo_hetero, 
                               aes(x = x, y = y_hetero)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE) +#Add linear regression line
  labs(title = "Heteroskedastic Plot") +
  scale_y_continuous("Y", seq(-0.5, 3.5, 0.5), limits = c(-0.5, 3.5)) +
  theme_minimal()

# Combine plots using facet_wrap
facet_plots <- ggarrange(homoskedastic_plot, heteroskedastic_plot, nrow = 1)

# Print the combined plots
print(facet_plots)
```

In the left plot, you see the homoskedastic data. The dots are equally and constantly distributed around the fitted line. However, the right plot shows that the more the independent variable **x** increases, the more the observations are increasing. The dots are not constantly distributed over the line. In the case, that the data is heteroskedastic, then this is a problem. You could try to transform the independent variable by taking the logarithm (We will look into that later). You could also use so-called heteroskedastic regression, but this an advanced model.

#### TSS, ESS and $R^2$

Now that we have the Residuals, we can calculate the Total Sum of Square (TSS), the Explained Sum of Squared ESS, and $R^2$:

-   TSS (Variation in the DV): $TSS =\sum(y_i - \bar{y})^2$ , we just subtract our actual values (`df$y`) from its mean and square it to avoid negative numbers. This gives us the total variation of our dependent variable.

-   ESS (Variation we explain in the DV): $ESS = \sum(\hat{y_i} - \bar{y})^2$ , now we use our predicted values (`df$y_hat`) instead of our actual values. That gives us the variation in the dependent variable, we can explain with our model.

-   $R^2$ (The Variation we can predict from our model): $R^2 = \frac{ESS}{TSS}$ , well to get the proportion we just divide the variation we can explain from our DV from the actual variation through the total variation in the DV. If these two values are the same, thus our model predicts all the variation in our dependent variable and this $R^2$ is 1. If our model could not explain anything the variation would be 0, since the values of both cannot be negative. Let us calculate them:

```{r calculating r2 by hand}
#total sum of squares
tss <- sum((df$y - mean(df$y))^2)
#explained sum of squares
ess <- sum((df$y_hat - mean(df$y))^2)
#caculating r squared
r_squared <- ess/tss

#Printing it
r_squared

#Summarizing it
summary(model1)$r.squared
```

#### Influential Outliers

Outliers are extremely deviating values, which can impact our analysis and bias it. Therefore, we have to check, if our data contains such values. But first let us see how they can impact our data:

```{r plotting bias through outliers}
#set seed 
set.seed(069)

#generate fake data with outlier 
x1 <- sort(runif(10, min = 30, max = 70))
y1 <- rnorm(10 , mean = 200, sd = 50)
y1[9] <- 2000
data_outlier <- data.frame(x1, y1)

#Model with Outlier 
model_outlier <- lm(y1 ~ x1) 

#Model without Outlier
model_without_outlier <- lm(y1[-9] ~ x1[-9]) 

#Plotting the Data 

# Scatter plot with points
ggplot(data_outlier, aes(x = x1, y = y1)) +
  geom_point(shape = 20, size = 3) +
  # Regression line for the model with outlier
  geom_abline(aes(slope = model_outlier$coefficients[2], intercept =
              model_outlier$coefficients[1], 
              color = "Model with Outlier"), linewidth = 0.75, show.legend = TRUE) +
  # Regression line for the model without outlier
  geom_abline(aes(slope = model_without_outlier$coefficients[2], 
              intercept = model_without_outlier$coefficients[1], 
              color = "Model without Outlier"), linewidth = 0.75, 
              show.legend = TRUE) +
  xlab("Independent Variable") +
  # Adding legend
  theme_classic() + 
  theme(legend.position = c(0.15,0.9), 
        legend.title = element_blank()) 

```

We see that this one observation completely biases our sample. But how do we find out, which observation is an influential outlier? There is a metric called **Cook's Distance**, we can use. Let us do it and plot it in R.

```{r plotting cooks distance}
#Cooks Distance can be calculated with a built-in function
data_outlier$cooks_distance <- cooks.distance(model_outlier) 

#Plotting it
ggplot(data_outlier, aes(x = x1, y = cooks_distance)) + 
  geom_point(colour = "darkgreen", size = 3, alpha = 0.5) + 
  labs(y = "Cook's Distance", x = "Independent Variables") + 
  geom_hline(yintercept = 1, linetype = "dashed") + 
  theme_bw()
```

We can clearly see that **Cook's Distance** detected the outlier. The rule is that values with a cooks distance bigger than 1 have to be eliminated. You can do that with the `filter()` function and run the model afterward again without the outliers.

#### Functional Form

First let us get our data:

```{r}
#Simulate further data
X_quadratic <- X <- runif(50, min = -5, max = 5)
u <- rnorm(50, sd = 1)  

#True relation
Y_quadratic <- X^2 + 2 * X + u

#Making a data frame out of it
df2 <- data.frame(X_quadratic, Y_quadratic)
```

Linear regression is a mathematical model. Therefore it is based on assumptions. But we should not just assume them, we should test them! One assumption is that linearity is assumed between X and Y. But that can be problematic consider following example:

```{r plotting linear fit through quadratic form}
# estimate a simple regression model 
model_simple <- lm(Y_quadratic ~ X_quadratic, data = df2)

# Summarize it
summary(model_simple)

#Plot it
ggplot(df2, aes(x = X_quadratic, y = Y_quadratic)) + 
  geom_point(shape = 20, size = 3) + 
  geom_smooth(method = "lm", se = FALSE) + 
  theme_bw()
```

As you can see something look wrong. There seems to be a correlation between the two variables, but it does not seem linear. In such a case it does make sense to square the independent variable and run the regression again:

```{r plotting quadratic fit with quadratic form}
# estimate a simple regression model 
model_quadratic <- lm(Y_quadratic ~ X_quadratic^2, data = df2)

#Summarize it 
summary(model_quadratic)

#Plot it
ggplot(df2, aes(x = X_quadratic, y = Y_quadratic)) + 
  geom_point(shape = 20, size = 3) + 
  geom_smooth(method = "lm", formula = y ~ poly(x, 2), 
              color = "red", 
              se = FALSE,) + 
  scale_x_continuous("X", breaks = seq(-5,5,1), limits = c(-5,5)) +
  ylab("Y") +
  theme_bw()
```

Well that looks better and is the proper way to deal with quadratic relationships in linear regression. Well, data can take on not only a quadratic form, it could also take on a form of a square-root function. I will show the most classical example of such a functional form. The `gapminder` data is loaded. It contains data about the average life expectancy (`lifeExp`)and the GDP per capita (`gdpPercap`) of countries in different years. Let us look if the GDP per Capita is correlated with Life Expectancy:

```{r unlogged fit, warning=FALSE}
#checking the data
head(gapminder)

#Plotting it
ggplot(gapminder, aes(gdpPercap, lifeExp)) + 
  geom_point() + 
  geom_smooth(method = "loess", se = FALSE) + 
  scale_y_continuous("Life Expectancy", seq(30, 80, 10), 
                     limits = c(30, 80)) + 
  theme_bw()
```

Well, that looks terrible. What can we do? I already mentioned that the fitted line looks like a square-root function ($y = \beta{\sqrt{x}}$ ). When you take the logarithm of square root, you neutralize the square root and only x remains $\log{(\sqrt{x})} = x$. When you do that the functional form changes to $y = \beta{x}$. Well, that is exactly the systematic component we are after:

```{r logged fit, warning=FALSE}
ggplot(gapminder, aes(log(gdpPercap), lifeExp)) + 
  geom_point() + 
  geom_smooth(method = "lm", se = FALSE) + 
  scale_y_continuous("Life Expectancy", seq(30, 80, 10), 
                     limits = c(30, 80)) + 
  xlab("GDP per Capita") +
  theme_bw()
```

This looks more like what we want to achieve. What can we see in that plot? When we run a model we want to take the logarithm of the independent variable, when we expect the following: If the most observations of a variable are low, but some observations are extremely high such functional forms can occur. In our example, most of the countries have a low GDP per Capita, but some countries such as the Western European countries or the USA have such a a high level of GDP per Capita, they change the functional form of the fitted line. These are influential outliers, but too much to delete them. It could bias the representativeness of the sample, therefore we can deal with them by taking the logarithm.

#### Independent Observation

Another assumption of the linear regression model is the independent, identically distributed (i.i.d) assumption. That sounds complicated but it really is not. Consider following plot:

```{r time trend}
# Getting the Data
# set seed
set.seed(123)

# generate a date vector
date <- seq(as.Date("1960/1/1"), as.Date("2020/1/1"), "years")

# initialize the employment vector
y_time <- c(5000, rep(NA, length(date)-1))

# generate time series observations with random influences
for (i in 2:length(date)) {
  
    y_time[i] <- -50 + 0.98 * y_time[i-1] + rnorm(n = 1, sd = 200)
}

# Plot it
df_time_series <- data.frame(y_time, date)
ggplot(df_time_series, aes(date, y_time)) + 
  geom_line() +
  ylab("Y") +
  xlab("Year") +
  theme_bw()  
```

If you look at the plot, can we assume that the observation Year = 2000 is independent from the Years before? No, the observation in the years are correlated to each other, thus the assumption is violated. This is basically the huge problem of working with longitudinal data (time-series cross-sectional or panel). If you face such problems there are plenty of other methods to use: Interrupted time series, Difference-in-Difference Designs, Panel-Matching, Fixed-Effects Models etc.

### Model Fit: Multivariate Regression

#### Model Fit: Adjusted R-squared

The last important aspect of Multivariate Regression is the Adjusted R-squared measure. Reconsider, the calculation of the classical R-squared:

-   TSS (Variation in the DV): $TSS =\sum(y_i - \bar{y})^2$ , we just subtract our actual values (`df$y`) from its mean and square it to avoid negative numbers. This gives us the total variation of our dependent variable.

-   ESS (Variation we explain in the DV): $ESS = \sum(\hat{y_i} - \bar{y})^2$ , now we use our predicted values (`df$y_hat`) instead of our actual values. That gives us the variation in the dependent variable, we can explain with our model.

-   $R^2$ (The Variation we can predict from our model): $R^2 = \frac{ESS}{TSS}$ , well to get the proportion we just divide the variation we can explain from our DV from the actual variation through the total variation in the DV. If these two values are the same, thus our model predicts all the variation in our dependent variable and this $R^2$ is 1.

The problem with the classical R-squared is, that if you would add useless independent variables to it, the classical R-squared would decrease, although your model did not increase in explanatory power. This is called **overfitting**. However, adjusted R-squared will account for that problem by introducing a "penalty" for every additional variable. Mathematically, it looks like this:

$$ Adj.R^2 = 1 - \frac{(1 - R^2)*(N - 1)}{N - k - 1} $$

where

-   $R^2$ is our classical R-squared calculated ($\frac{TSS}{ESS}$)

-   $N$ is the number of observations in our sample

-   $k$ is the number of independent variables

In `R`, we can extract the adjusted R-squared simply from our model in chunk, multivariate regression:

```{r calculating adj r2}
#running multivariate model
multivariate_model <- lm(y ~ x + categorical_variable, data = df)

#Getting summary
summary(multivariate_model)

#Extract Adjusted R-squared
summary(multivariate_model)$adj.r.squared

#Calculating by hand
adj_r_squared <- 1 - (((1-summary(multivariate_model)$r.squared) * (nrow(df) - 1))/(nrow(df) - 2 - 1))

#printing it
print(adj_r_squared)
```

#### Omitted Variable Bias

I already mentioned one (abstract) reason why we should include other variables in our model. But there is more to it: You could find effects between two variables X and Y, but it could be that in Reality there is not an association. For example, let us say you collect data about ice cream and shark attacks. Ice cream sales is your independent variable and you want to explain the number of shark attacks, here is your data:

```{r omitted variable bias}
# Set seed for reproducibility 
set.seed(0)  

# Number of data points 
n <- 100

# Simulate diet data (assuming a normal distribution) 
temperature <- rnorm(n, mean = 1500, sd = 200)  

# Simulate exercise data (assuming a normal distribution) 
ice_cream_sales <- rnorm(n, mean = 3, sd = 1)  

# Simulate weight loss data 
violence_crime_true <- 0.2 * temperature - 
  0.5 * ice_cream_sales + 
  rnorm(n, mean = 0, sd = 5) 

# Create a data frame 
data <- data.frame(temperature = temperature,
                   ice_cream_sales = ice_cream_sales,       
                   violence_crime_true = violence_crime_true)  

# Fit a model without including the diet variable 
model_without_temperature <- lm(violence_crime_true ~ ice_cream_sales, data = data)

#Fit a model with only the temperature variable

model_with_only_temperature <- lm(violence_crime_true ~temperature,                        data = data)

# Fit a model including both diet and exercise variables 
model_with_temperature <- lm(violence_crime_true ~ ice_cream_sales + temperature,                        data = data)  

# Output the summary of both models 
summary(model_without_temperature) 
summary(model_with_only_temperature)
summary(model_with_temperature)  

#Let us display both models next to each other
#EDIT: I created this function specifically, the code for the function is at the top.   
table_ovb(model_without_temperature, model_with_temperature)
```

We can see that the coefficient changes dramatically. What happened? Well, one important assumption of linear regression is that the error term captures all variance not explained by our model and is **not correlated** with the independent variable(s) nor the dependent variable. But if there is unexplained variation in our model that is correlated with our independent variable, then this assumption is violated. In our example, we can see that ice cream sales coefficient in the first model is biased, because ice cream sales and is correlated to temperature. The warmer it gets, the more ice cream is sold. But, the warmer it gets, the more violent people get, therefore we have an omitted variable and that is temperature. When we include temperature in the model, we see the problem of omitted variable bias: It biases our coefficients, by either overestimating (like in our example) or by underestimating it. What we should do in such a case, is to delete the omitted variable, which is the drastically changing variable (ice cream sales in our case). This is also the reason, why people talk about additional variables as control variables in a multiple linear model. This way you can control if an association between two variables is due to omitted variable bias or other variables, which can explain the variation better.

#### Multicollinearity

Another, and I promise, the last OLS assumption, which has to tested is that **there is no Multicollinearity**. The concept is simple: The independent variables should not be correlated. In our previous example, ice cream sales and temperature were correlated. This would have hurt these assumption. In strong cases, multicollinearity can bias our estimates, so that they gain statistical significance and lead us to wrong conclusions. Let us look at an obvious example. You want to find out how the grades of children is affected by different factors. You choose 2 factors: The time they spent on doing their homework (*learning time*) and the time they spent on playing video games (*gaming time*). The systematic component looks like this:

$$
Grades_i = \beta_0 + \beta_1*\text{learning time}_i + \beta_2 *\text{gaming time}_i + \epsilon_i
$$

Let us compute them:

```{r Multicollinearity}
# Set seed for reproducibility
set.seed(42)

# Number of samples
n <- 100

# True coefficients
beta_0 <- 80
beta_1 <- 1.5
beta_2 <- 1.5

# Generate independent variables
learning_time <- runif(n, 1, 10)
gaming_time <- 0.7 * learning_time + 
  sqrt(1 - 0.7^2) * rnorm(n, sd = 1) 

#generate error term
epsilon <- rnorm(n, 0, 3)

# Generate grades
grades <- beta_0 + beta_1 * learning_time + beta_2 * gaming_time + epsilon

# Create a data frame
df_grades <- data.frame(learning_time,
                   gaming_time,
                   grades)

# Display first few rows of the data frame
grades_model <- lm(grades ~ learning_time + gaming_time, 
                   data = df_grades)

#Getting the summary
summary(grades_model)
```

By looking at the model, we could conclude that the more a student learns, the better its grades on average, holding all else constant. So far, so clear, but the same goes for gaming time. Why is that the case? Because if we think that a student has per day 3 hours, which the student can assign to either learning or gaming, than both are correlated, because assigning 2 hours to learning means 1 hour for gaming, 0.5 hours for learning means 2.5 hours for gaming and so forth. This means both coefficients are explaining each other and bias each other. How to detect them?

##### Testing Correlations to each other

The first technique is to check the correlations of the variables to each other beforehand. You can do that two-ways: Just print out a correlation table:

```{r testing correlations}
#First store the variables you need in a seperate data frame 
cormatrix_data <- df_grades %>% 
  select(learning_time, gaming_time)


#Second, calculate the table, the 2 at the end are the dimensions
cormatrix <- cor(cormatrix_data) #Calculate the correlations
round(cormatrix, 2) #round it to the second digit and display it 

#We could have also done this code in one step 

#df_grades %>% 
#  select(learning_time, gaming_time) %>% 
#  cor() %>% 
#  round(2) 
```

We can see that the correlation between both variables is way to high. Now, with only two variables the table works fine, but what if we have, let us say, 20 variables? It could get messy, therefore you could also use the correlation matrix, which I introduced in the chapter before. In this case, it does not make sense, since it would just print out one block. But keep it nevertheless in mind for the future.

##### Variance of Inflation (VIF)

Another measure for multicolinearity and probably the most famous one, is the Variance-of-Inflation (VIF) factor. Intuitively spoken, this measure fits models with multiple variables, by calculating the variance of each variable. Then it fits a model with only one independent variable and calculates the variance of it. The result is a measure that displays high values if the variance of a variable increases, when other variables are added. That is exactly what this measure does.. The formula of it is really simple:

$$
VIF_i = \frac{1}{1 - R^2_i} 
$$

where, the Variance of inflation (VIF) of variable *i* is calculated by 1 divided by 1 - the R-squared ($R^2_i$) of the regression with only that variable.

In `R`, we can use the `VIF()` function from the `car` - package to do it.

```{r vif}
#We only have to use the function VIF() on our model 
vif(grades_model)
```

The rule is that if a value exceeds 10, it is considered critical. We should always test for multicolinearity, and if we detect it, run the regression separately without the correlated variables.

<!--chapter:end:08-Further-Explanations.Rmd-->

# Solutions Exercises

```{r loading packages for Exercises}
pacman::p_load("tidyverse", "gapminder", "babynames", "sjPlot", "ggridges")
```

## Chapter 1: Fundamentals

### Exercise 1: Making your first Vector

Create a vector called `my_vector` with the values 1,2,3 and check is class.

```{r chapter 1 exercise 1, eval=FALSE}
#create the vector  
my_vector <- c(1,2,3)

#check the class 
class(my_vector)
```

### Exercise 2: Making your first matrix

Create a Matrix called `student`. This should contain information about the `name`, `age` and `major`. Make three vectors with three entries and bind them together to a the matrix `student`. Print the matrix. Choose the three names, age, and major by yourself:

```{r chapter 1 exercise 2, eval = FALSE}
#Create the vectors   
name <- c("James", "Elif", "Jonas")
age <- c("19", "17", "24")
major <- c("Political Science", "Data Science", "Physics")
  
#Create the matrix 
student <- cbind(name, age, major)

#Or with the matrix command
student <- matrix(name, age, major)
  
#Print the matrix 
print(student)
```

### Exercise 3: `ifelse` function

Write an `ifelse` statement that checks if a given number is positive or negative. If the number is positive or 0, print "Number is positive", otherwise print "Number is negative". Feel free to decide if you want to use the `ifelse` function or the `ifelse` condition.

```{r chapter 1 exercise 3, eval = FALSE}
#Assigning the number to the object "number" 
number <- 0

#With ifelse() function
ifelse(number >= 0, "Number is positive", "Number is negative")

#With ifelse condition

if (number > 0) {
  print("Positive Number")
} else {
  print("Negative Number")
}
```

### Exercise 4: `ifelse` ladders

Write an if-else ladder that categorizes a student's grade based on their score. The grading criteria are as follows:

Score \>= 90: "A" Score \>= 80 and \< 90: "B" Score \>= 70 and \< 80: "C" Score \>= 60 and \< 70: "D" Score \< 60: "F".

```{r chapter 1 exercise 4, eval = FALSE}
#Defining the vector score 
Score <- 90

#Defing ifelse ladder with ifelse() function
ifelse(Score >= 90, "A", 
       ifelse(Score >= 80 & Score < 90, "B",
              ifelse(Score >= 70 & Score < 80, "C",
                     ifelse(Score >= 60 & Score < 70, "D", 
                            ifelse(Score < 60, "F")))))

#Defining it with a ifelse condition
if (score >= 90) {
  print("A") 
} else if (score >= 80 & score < 90) {
  print("B")
} else if (score >= 70 & score < 80) {
  print("C")
} else if (score >= 60 & score < 70) {
  print("D") 
} else if (score < 60) {
  print("F")
}
```

## Chapter 2: Data Manipulation

```{r loading packages for Data Manipualtion}
pacman::p_load("tidyverse")
```

### Exercise 1: Let's wrangle kid

You are interested in discrimination and the perception of the judicial. More specifically, you want to know if people, who fell discriminated evaluate courts differently. Below you see a table with all variables you want to include in your analysis:

+--------------+--------------------------------------------------------+-------------------------------------------------------------------------------------------------------+
| **Variable** | **Description**                                        | Scales                                                                                                |
+:============:+========================================================+=======================================================================================================+
| **idnt**     | Respondent's identification number                     | unique number from 1-9000                                                                             |
+--------------+--------------------------------------------------------+-------------------------------------------------------------------------------------------------------+
| **year**     | The year when the survey was conducted                 | only 2020                                                                                             |
+--------------+--------------------------------------------------------+-------------------------------------------------------------------------------------------------------+
| **cntry**    | Country                                                | BE, BG, CH, CZ, EE, FI, FR,GB, GR, HR, HU, IE, IS, IT, LT,NL, NO, PT, SI, SK                          |
+--------------+--------------------------------------------------------+-------------------------------------------------------------------------------------------------------+
| **agea**     | Age of the Respondent, calculated                      | Number of Age = 15-90                                                                                 |
|              |                                                        |                                                                                                       |
|              |                                                        | 999 = Not available                                                                                   |
+--------------+--------------------------------------------------------+-------------------------------------------------------------------------------------------------------+
| **gndr**     | Gender                                                 | 1 = Male;                                                                                             |
|              |                                                        |                                                                                                       |
|              |                                                        | 2 = Female;                                                                                           |
|              |                                                        |                                                                                                       |
|              |                                                        | 9 = No answer                                                                                         |
+--------------+--------------------------------------------------------+-------------------------------------------------------------------------------------------------------+
| **happy**    | How happy are you                                      | 0 (Extremly unhappy) - 10 (Extremly happy);                                                           |
|              |                                                        |                                                                                                       |
|              |                                                        | 77 = Refusal;                                                                                         |
|              |                                                        |                                                                                                       |
|              |                                                        | 88 = Don't Know;                                                                                      |
|              |                                                        |                                                                                                       |
|              |                                                        | 99 = No answer                                                                                        |
+--------------+--------------------------------------------------------+-------------------------------------------------------------------------------------------------------+
| **eisced**   | Highest level of education, ES - ISCED                 | 0 = Not possible to harmonise into ES-ISCED;                                                          |
|              |                                                        |                                                                                                       |
|              |                                                        | 1 (ES-ISCED I , less than lower secondary) - 7 (ES-ISCED V2, higher tertiary education, =\> MA level; |
|              |                                                        |                                                                                                       |
|              |                                                        | 55 = Other;                                                                                           |
|              |                                                        |                                                                                                       |
|              |                                                        | 77 = Refusal;                                                                                         |
|              |                                                        |                                                                                                       |
|              |                                                        | 88 = Don't know;                                                                                      |
|              |                                                        |                                                                                                       |
|              |                                                        | 99 = No answer                                                                                        |
+--------------+--------------------------------------------------------+-------------------------------------------------------------------------------------------------------+
| **netusoft** | Internet use, how often                                | 1 (Never) - 5 (Every day);                                                                            |
|              |                                                        |                                                                                                       |
|              |                                                        | 7 = Refusal;                                                                                          |
|              |                                                        |                                                                                                       |
|              |                                                        | 8 = Don't know;                                                                                       |
|              |                                                        |                                                                                                       |
|              |                                                        | 9 = No answer                                                                                         |
+--------------+--------------------------------------------------------+-------------------------------------------------------------------------------------------------------+
| **trstprl**  | Most people can be trusted or you can't be too careful | 0 (You can't be too careful) - 10 (Most people can be trusted);                                       |
|              |                                                        |                                                                                                       |
|              |                                                        | 77 = Refusal;                                                                                         |
|              |                                                        |                                                                                                       |
|              |                                                        | 88 = Don't Know;                                                                                      |
|              |                                                        |                                                                                                       |
|              |                                                        | 99 = No answer                                                                                        |
+--------------+--------------------------------------------------------+-------------------------------------------------------------------------------------------------------+
| **lrscale**  | Left-Right Placement                                   | 0 (Left) - 10 (Right);                                                                                |
|              |                                                        |                                                                                                       |
|              |                                                        | 77 = Refusal;                                                                                         |
|              |                                                        |                                                                                                       |
|              |                                                        | 88 = Don't know;                                                                                      |
|              |                                                        |                                                                                                       |
|              |                                                        | 99 = No answer                                                                                        |
+--------------+--------------------------------------------------------+-------------------------------------------------------------------------------------------------------+

a.  Wrangle the data, and assign it to an object called **ess**.
b.  Select the variables you need
c.  Filter for Austria, Belgium, Denmark, Georgia, Iceland and the Russian Federation
d.  Have a look at the codebook and code all irrelevant values as missing. If you have binary variables recode them from 1, 2 to 0 to 1
e.  You want to build an extremism variable: You do so by subtracting 5 from the from the variable and squaring it afterwards. Call it extremism
f.  Rename the variables to more intuitive names, don't forget to name binary varaibles after the category which is on 1
g.  drop all missing values
h.  Check out your new dataset

```{r chapter2 exercise 1a-h, eval=FALSE}
ess <- d1 %>% 
  select(cntry, dscrgrp, cttresa, agea, gndr, eisced, lrscale) %>% #a
  filter(cntry %in% c("AT", "BE", "DK", "GE", "IS","RU")) %>% #b
  mutate(dscrgrp = case_when( #c
    dscrgrp == 1 ~ 0, 
    dscrgrp == 2 ~ 1,
    dscrgrp %in% c(7, 8, 9) ~ NA_real_, 
    TRUE ~ dscrgrp),
    cttresa = case_when( 
    cttresa %in% c(77, 88, 99) ~ NA_real_, 
    TRUE ~ cttresa),
    agea = case_when(
    agea == 999 ~ NA_real_, 
    TRUE ~ agea),
    gndr = case_when(
      gndr == 1 ~ 0, 
      gndr == 2 ~ 1,
      gndr == 9 ~ NA_real_
    ),
    eisced = case_when( 
      eisced %in% c(55, 77, 88, 99) ~ NA_real_,
      TRUE ~ eisced),
    lrscale = case_when(
      lrscale %in% c(77, 88, 99) ~ NA_real_,
      TRUE ~ lrscale) #d, you could do this step also in a separate mutate function if you think that is more intuitive 
  ) %>% 
  mutate(extremism = (lrscale - 5)^2) %>% 
  rename(discriminated = dscrgrp,  
         court = cttresa,
         age = agea, 
         female = gndr, 
         education = eisced, 
         lrscale = lrscale, 
         extremism = extremism
         ) %>% 
  drop_na()

#Checking the dataset 
head(ess)
```

## Chapter 3: Data Visualisation

In this exercise Section, we will work with the `babynames` and the `iris` package. This is a classic built-in package in R, which contains data from the Ronald Fisher's 1936 Study "*The use of multiple measurements in taxonomic problems".* It contains three plant species and four measured features for each species. Let us get an overview of the package:

```{r inspect iris dataset}
summary(iris)
```

### Exercise 1: Distributions

a\. Plot a Chart, which shows the distribution of `Sepal.Length` over the `setosa` Species. Choose the type of distribution chart for yourself. **HINT:** Prepare the data first and then plot it.

```{r chapter 3 exercise 1a}

#Filtering for setosa
d1 <- iris %>%
  filter(Species == "setosa")

#Histogram
ggplot(iris, aes(x = Sepal.Length)) +
  geom_histogram()

#Density Plot
ggplot(iris, aes(x = Sepal.Length)) +
  geom_density()

#Boxplot
ggplot(iris, aes(x = Sepal.Length)) +
  geom_boxplot()

###EDIT: You could also do everythin in one step as shown below

#Histogram
#iris %>%
#  filter(Species == "setosa") %>%
#  ggplot(aes(x = Sepal.Length)) +
#  geom_histogram()

#Density Plot
#iris %>%
#  filter(Species == "setosa") %>%
#  ggplot(aes(x = Sepal.Length)) +
#  geom_density()

#Boxplot
#iris %>%
#  filter(Species == "setosa") %>%
#  ggplot(aes(x = Sepal.Length)) +
#  geom_boxplot()
```

b\. Now I want you to add the two other Species to the Plot. Make Sure, that every Species has a unique color.

```{r chapter 3 exercise 1b}
#Histogram
ggplot(iris, aes(x = Sepal.Length, fill = Species)) +
  geom_histogram()

#Density Plot
ggplot(iris, aes(x = Sepal.Length, fill = Species)) +
  geom_density()

#Boxplot
ggplot(iris, aes(x = Sepal.Length, fill = Species)) +
  geom_boxplot()

#Violin Plot
ggplot(iris, aes(x = Species, y = Sepal.Length, fill = Species)) +
  geom_violin()

#Ridgeline Plot
ggplot(iris, aes(x = Sepal.Length, y = Species, fill = Species)) +
  geom_density_ridges()
```

c\. Make a nice Plot! Give the Plot a meaningful title, meaningful labels for the x-axis and the y-axis and play around with the colors.

```{r chapter 3 exercise 1c}
#Histogram
ggplot(iris, aes(x = Sepal.Length, fill = Species)) +
  geom_histogram() +
  labs(
    x = "Species",
    y = "Sepal Length",
    title = "Distribution of Sepal Length across Species"
  ) +
  theme_bw() +
  theme(
    legend.position = "none"
  ) +
  scale_fill_brewer(palette = "Dark2")

#Density Plot
ggplot(iris, aes(x = Sepal.Length, fill = Species)) +
  geom_density() +
  labs(
    x = "Species",
    y = "Sepal Length",
    title = "Distribution of Sepal Length across Species"
  ) +
  theme_bw() +
  theme(
    legend.position = "none"
  ) +
  scale_fill_brewer(palette = "Dark2")

#Boxplot
ggplot(iris, aes(x = Sepal.Length, fill = Species)) +
  geom_boxplot() +
  labs(
    x = "Species",
    y = "Sepal Length",
    title = "Distribution of Sepal Length across Species"
  ) +
  theme_bw() +
  theme(
    legend.position = "none"
  ) +
  scale_fill_brewer(palette = "Dark2")

#Ridgeline Plot
ggplot(iris, aes(x = Species, y = Sepal.Length, fill = Species)) +
  geom_violin() +
  labs(
    x = "Species",
    y = "Sepal Length",
    title = "Distribution of Sepal Length across Species"
  ) +
  theme_bw() +
  theme(
    legend.position = "none"
  ) +
  scale_fill_brewer(palette = "Dark2")

#Ridgeline Plot
ggplot(iris, aes(x = Sepal.Length, y = Species, fill = Species)) +
  geom_density_ridges() +
  labs(
    x = "Species",
    y = "Sepal Length",
    title = "Distribution of Sepal Length across Species"
  ) +
  theme_bw() +
  theme(
    legend.position = "none"
  ) +
  scale_fill_brewer(palette = "Dark2")
```

d\. Interpret the Plot!

### Exercise 2: Rankings

a\. Calculate the average `Petal.Length` and plot it for every Species in a nice Barplot. **HINT:** You have to prepare the data again before you plot it. Decide for yourself if you want to do it vertically or horizontally.

```{r chapter 3 exercise 2a}
#Prepare the data 
d1 <- iris %>%
  group_by(Species) %>%
  dplyr::summarize(PL_average = mean(Petal.Length))

#Check it
head(d1)  
  
#Plotting a horizontal barplot
ggplot(d1, aes(x = Species, y = PL_average, fill  = Species)) +
  geom_bar(stat = "identity")

#Plotting a vertical barplot
ggplot(d1, aes(x = PL_average, y = Species, fill = Species)) +
  geom_bar(stat = "identity") 
```

b\. Make a nice Plot! Give the Plot a meaningful title, meaningful labels for the x-axis and the y-axis and play around with the colors.

```{r chapter 3 exercise 2c}
#Plotting a horizontal barplot
ggplot(d1, aes(x = Species, y = PL_average, fill  = Species)) +
  geom_bar(stat = "identity") +
  labs(
    x = "Species",
    y = "Average Sepal Length",
    title = "Average Sepal Length across Species"
  ) +
  theme_bw() +
  theme(
    legend.position = "none"
  ) +
  scale_fill_brewer(palette = "Set1")

#Plotting a vertical barplot
ggplot(d1, aes(x = PL_average, y = Species, fill = Species)) +
  geom_bar(stat = "identity") +
    labs(
    x = "Average Sepal Length",
    y = "Species",
    title = "Average Sepal Length across Species"
  ) +
  theme_bw() +
  theme(
    legend.position = "none"
  ) +
  scale_fill_brewer(palette = "Accent")
```

### Exercise 3: Correlation

a\. Make a scatter plot where you plot `Sepal.length` on the x-axis and `Sepal.width` on the y-axis. Make the plot for the species `virginica`

```{r chapter 3 exercise 3a}
#Preparing the data
d1 <- iris %>%
  filter(Species == "virginica")

#Making the plot
ggplot(d1, aes(x = Sepal.Length, y = Sepal.Width)) +
  geom_point()
```

b\. Now I want you to add the species `versicolor` to the plot. The dots of this species should have a different color AND a different form.

```{r chapter 3 exercise 3b}
#Preparing the Data 
d1 <- iris %>%
  filter(Species %in% c("virginica", "versicolor")) 

#Making the Plot
ggplot(d1, aes(x = Sepal.Length, y = Sepal.Width, 
               shape = Species, color = Species)) +
  geom_point() 
```

c\. Make a nice plot! Add a theme, labels, and a nice title, increase the size of the forms and make play around with the colors.

```{r chapter 3 exercise 3c}
#Making the Plot
ggplot(d1, aes(x = Sepal.Length, y = Sepal.Width, 
               shape = Species, color = Species)) +
  geom_point() +
  labs(
    title = "Relationship between Sepal Length and Sepal Width",
    x = "Sepal Length", 
    y = "Sepal Width"
  ) + 
  scale_color_brewer(palette = "Set1") +
  theme_classic()
  
```

## Chapter 4: Exploratory Data Analysis

### Exercise 1: Standard Descriptive Statistics

In this exercise we will work with the built-in `iris` package in R:

a\. Calculate the mode, mean and the median for the `iris$Sepal.Length` variable

```{r chapter 4 exercise 1a}
mean(iris$Sepal.Length) #Calculating the mean
median(iris$Sepal.Length) # Calculating the median

#Calculating the mode
uniq_vals <- unique(iris$Sepal.Length)
freqs <- tabulate(iris$Sepal.Length)
uniq_vals[which.max(freqs)]

# You could also define a function for the mode

#mode <- function (x) {
#  uniq_vals <- unique(x, na.rm = TRUE)
#  freqs <- tabulate(match(x, uniq_vals))
#  uniq_vals[which.max(freqs)]
#}
#mode(iris$Sepal.Length)
```

b\. Calculate the interquartile range, variance and the standard deviation for iris\$Sepal.Length

```{r chapter 4 exercise 1b}
IQR(iris$Sepal.Length)
var(iris$Sepal.Length) #Variance
sd(iris$Sepal.Length) #Standard deviatio
#Or we take just the squareroot of the variance to get the sd
#var(iris$Sepal.Length) %>%
#  sqrt()
```

c\. Calculate all five measures at once by using a function that does so (Choose by yourself, which one you want to use)

```{r chapter 4 exercise 1c}
psych::describe(iris$Sepal.Length) #psych package
skimr::skim(iris$Sepal.Length) #skim package
summarytools::descr(iris$Sepal.Length) #summarytools package
```

### Exercise 2: Contingency Tables and Correlations

a\. Make a Contingency Table for `esoph$agegp` and `esoph$alcgp`

```{r chapter 4 exercise 2a}
table(esoph$agegp, esoph$alcgp) #with base R
summarytools::ctable(esoph$agegp, esoph$alcgp) #with summarytools
gtsummary::tbl_cross(data = esoph,
                     row = agegp, 
                     col = alcgp) #with gtsummary
```

b\. Cut down the iris dataset to Sepal.Length, Sepal.Width, Petal.Length and Petal.Width and save it in an object called iris_numeric.

```{r chapter 4 exercise 2b}
iris_numeric <- iris %>% 
  select(Sepal.Length, Sepal.Width, Petal.Length, Petal.Width)
```

c\. Make a correlation matrix with iris_numeric

```{r chapter 4 exercise 2c}
# With corrplot package
cor_iris <- cor(iris_numeric)
corrplot::corrplot(cor_iris, method = "color")

# With DataExplorer
DataExplorer::plot_correlation(iris_numeric)
```

d\. Make the correlation matrix pretty

```{r chapter 4 exercise 2d}
# With Corrplot
corrplot::corrplot(cor_iris, method = "circle", type = "upper", 
         tl.col = "black", tl.srt = 45)

# With DataExplorer
DataExplorer::plot_correlation(
  iris_numeric,
  ggtheme = theme_minimal(base_size = 14),
  title = "Correlation Matrix of Iris Numeric Variables"
)
```

### Exercise 3: Working with packages

a\. Use a function to get an overview of the dataset mtcars

```{r chapter 4 exercise 3a}
skimr::skim(mtcars)
summarytools::dfSummary(mtcars)
gtsummary::tbl_summary(mtcars)
```

b\. Have a look at the structure of the missing values in mtcars

```{r chapter 4 exercise 3b}
# With a table
naniar::miss_var_summary(mtcars)
DataExplorer::introduce(mtcars)
SmartEDA::ExpData(mtcars, type = 1)

# Graphically
naniar::vis_miss(mtcars)
DataExplorer::plot_missing(mtcars)
```

c\. Make an automized EDA report for mtcars!

`dlookr::describe(mtcars)`

`DataExplorer::create_reports(mtcars)`

## Chapter 5: Data Analysis

To understand linear regression, we do not have to load any complicated data. Let us assume, you are a market analyst and your customer is the production company of a series called "Breaking Thrones". The production company wants to know how the viewers of the series judge the final episode. You conduct a survey and ask people on how satisfied they were with the season final and some social demographics. Here is your codebook:

| Variable | Description |
|----|----|
| id | The id of the respondent |
| satisfaction | The answer to the question "How satisfied were you with the final episode of Breaking Throne?", where 0 is completely dissatisfied and 10 completely satisfied |
| age | The age of the respondent |
| female | The Gender of the respondent, where 0 = Male, 1 = Female |

Let us generate the data:

```{r generating data for chapter 5 exercises}
#setting seed for reproduciability 
set.seed(123)  

#Generating the data 
final_BT <- data.frame(
  id = c(1:10),    
  satisfaction = round(rnorm(10, mean = 6, sd = 2.5)),    
  age = round(rnorm(10, mean = 25, sd = 5)),    
  female = rbinom(10, 1, 0.5)
  )  

#Print the Data Frame
print(final_BT)
```

### Exercise 1: Linear Regression with two variables

You want to know if age has an impact on the satisfaction with the last episode. You want to conduct a linear regression.

a\. Calculate $\beta_0$ and $\beta_1$ by hand

```{r chapter 5 exercise 1a}
#Calculating Covariance
cov <- sum((final_BT$age - mean(final_BT$age)) * (final_BT$satisfaction - mean(final_BT$satisfaction)))
  
#Calculating Variance
var <- (sum((final_BT$age - mean(final_BT$age))^2))

#Calculating beta_1
beta_1 <- cov/var

#printing beta_1
print(beta_1)

#calculating beta 1
beta_0 <- mean(final_BT$satisfaction) - (beta_1 * mean(final_BT$age))

#Print both
print(beta_0)
print(beta_1)
```

b\. Calculate $\beta_0$ and $\beta_1$ automatically with R

```{r chapter 5 exercise 1b}
#Calculating it automatically
summary(m1 <- lm(satisfaction ~ age, 
                 data = final_BT))
```

c\. Interpret all quantities of your result: Standard Error, t-statistic, p-value, confidence intervals and the $R^2$.

d\. Check for influential outliers

```{r chapter 5 exercise 1d}
#Cooks Distance can be calculated with a built-in function
final_BT$cooks_distance <- cooks.distance(m1) 

#Plotting it
ggplot(final_BT, aes(x = age, y = cooks_distance)) + 
  geom_point(colour = "darkgreen", size = 3, alpha = 0.5) + 
  labs(y = "Cook's Distance", x = "Independent Variable") + 
  geom_hline(yintercept = 1, linetype = "dashed") + 
  theme_bw()
```

### Exercise 2: Multivariate Regression

a\. Add the variable `female` to your regression

```{r chapter 5 exercise 2a}
summary(m1 <- lm(satisfaction ~ age + female, 
                 data = final_BT))
```

b\. Interpret the Output. What has changed? What stood the same?

c\. Make an interaction effect between age and female and interpret it!

```{r chapter 5 exercise 2c}
summary(m2 <- lm(satisfaction ~ age + female + age:female, 
                 data = final_BT))
```

d\. Plot the interaction and make the plot nice

```{r chapter 5 exercise 2d}
plot_model(m2, type = "int") +
  scale_x_continuous(breaks = seq(0,60, 1)) + 
  labs(title = "Relationship of Age and Gender on Satisfaction with BT.",
       x = "Age", 
       y = "Satisfaction") +
  scale_color_manual(
    values = c("red", "blue"),
    labels = c("Female", "Male")
  ) +
  theme_sjplot() +
  theme(legend.position = "bottom", 
        legend.title = element_blank())
```

## Chapter 6: Loops and Functions

### Exercise 1: Writing a loop

Write a `for` loop that prints the square of each number from 1 to 10

```{r chapter 6 exercise 1}
#Assigning an object for a better workflow 
number <- 10
```

### Exercise 2: Writing a function

Write a function that takes the input x and squares it:

```{r chapter 6 exercise 2, eval=FALSE}
#Defining a function for squaring  
sq <- function (x) {          
  x_squared <- x^2
  return(x_squared)
}  

#Defining a vector containing a vector from 1 to 10  
numbers <- c(1:10)   

#Applying the number  
sq(numbers)
```

### Exercise 3: The midnight Formula

This is the midnight formula separated in two equations:

$x_{1,2} = \frac{-b \pm \sqrt{b^2 - 4ac}}{2a}$

Make **one function** for the midnight formula, so the output are $x_1$ a d $x_2$. Test it with a = 2, b = -6, c = -8

**Hint**: You need two split up the formula into two equations with two outputs.

```{r chapter 6 exercise 3, eval=FALSE}
mnf <- function(a, b, c) {
  
  x_1 <- (-b + sqrt(b^2 - 4*a*c))/(2*a)
  
  print(x_1)
  
  x_2 <- (-b - sqrt(b^2 - 4*a*c))/(2*a)
  
  print(x_2)
  
}   
  
#Test it with these numbers (other numbers might throw an error if the number under the square root is negative)
mnf(2, 20, 8)
```

<!--chapter:end:09-solutions_exercises.Rmd-->


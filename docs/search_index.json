[["index.html", "An intuitive Introduction to R Welcome ! About this Course About Me", " An intuitive Introduction to R Okan Sarioglu 2025-09-20 Welcome ! Welcome to this R course! In this course I will teach you the workflow from raw data to your very first analyses in R. When I was a beginner myself, I tried a lot of courses like this, but there were two points, which I think slowed down my learning process, I do want to make different with this course: The courses taught way too much unnecessary stuff for the beginning. It does not matter to know several variations of a command for example, the intuition is way more important. This course was originally a tutorial for my juniors at my university and I asked myself the question: What do they need to know, to conduct data analyses themselves with R? Well, they have to get an intuition of the workflow. So I decided to design a course, that instead of showing unnecessary variations of functions aim to show how to analyse a question of interest. The courses were not reproducible! That is not the directly the fault of the courses. There are tools to make R scripts reproducible, however they are not beginner friendly, so the authors of those courses are facing a trade-off: Teaching unnecessary complicated stuff at to beginners, or to leave it out and make the course not reproducible. This course is different! In this course, you can download R-scripts and load them directly on your own device and follow the course by executing the codes for yourself to have the original experience of working with R. About this Course Prerequisities R and RStudio. You should download the most current version of R and RStudio. How to do that easily is described here. What is R and RStudio ? R is a programming language specifically developed for statistical analysis. RStudio is the standard graphical interface to work with R. Why R ? Free of cost and open-source Functionalities for all steps of research process from Data Collection to Data Analysis Programming language specifically developed for statistical analysis Very active Community: e.g. R community on StackOverflow e.g. #rstats on twitter What expects you and what not In this course you will learn: To get familiar with R and its basic language Core commands from the tidyverse package Data Wrangling, Data Bridging, Data Munging, Data Manipulation An efficient Workflow A brief introduction into basic Data Analysis and Exploratory Data Analysis. You will not learn: Advanced R usage (Webscraping, Quantitative Text Analysis etc…) Overview of the course structure: 1. The R environment Basic Functionality (Calculations, Vectors, Matrices, Lists) Object classes Accessing, Subsetting and Naming Objects 2. Data Manipulation Pipelines or Piping The tidyverse - Dplyr Loading and Storing Data Ordering your Data: Renaming, Re-Ordering, Subsetting and Selecting Transforming Variables Merging Data Missing Values 3. Exploratory Data Analysis/Descriptives Standard Descriptive Statistics (Mean, Median, SD,….) Contingency Tables Correlations Working with EDA packages 4. Data Visualization The Tidyverse - ggplot 2 Constructing Plots Plotting anything 5. Data Analysis Linear Regression Model Fit Hypothesis Testing with R Multivariate Regression Categorical Variables 6. R Programming For loops Apply function Functions About Me My name is Okan and I am a Data Scientist. To be more precise, my profession is Data Science, but originally I graduated in Political Science. I know, what kind of transition is that? From Politics to Data? Well, my university was specialized in empirical research and Political Science was no exception. To explain political phenomena, I analysed large data sets and it was the most fun part of my studies. So I decided to become a Data Scientist and I could not be happier about that decision! Be part of this Course! Please report errors, bugs and problems with the code. If you have any ideas how to improve this course please contact me on GitHub or via E-Mail. In general, let us stay in touch, follow me on GitHub and LinkedIn and if you like this course share your experience and recommend it to others! "],["fundamentals.html", "Chapter 1 Fundamentals 1.1 Getting familiar with RStudio and establishing a Workflow 1.2 Lets get started: R as a fancy calculator 1.3 ifelse statements and the ifelse() function 1.4 Outlook 1.5 Exercise Section", " Chapter 1 Fundamentals In this Chapter you will learn the user interface and basic functions in R. The user interface, as well as the basic functions for mathematical calculations. Further, you will be introduced to one of the most important logical operations: the if else function. The goal of this chapter is to give you an overview of RStudio and to give you a feeling and intuition for R by introducing you to mathematical concepts you already know. Do not forget, nobody is perfect, everyone has problems in the beginning and they can be frustrating, but that is most normal thing, when learning a new programming language. Have fun! 1.1 Getting familiar with RStudio and establishing a Workflow 1.1.1 The user interface RStudio has four main components and each of them has a purpose: Figure 1: Standard RStudio Interface On the top left you can see the Source window. In this window you write and run your code. The console opens after you choose, which type of Script you want to use: The standard R Script (Ctrl + Shift + N): In this file you can just write your code and run it. You can also make commenting lines with putting a # in front of it. The R Markdown File: In contrast to the standard R Script not everything written in a R Markdown File is automatically considered as Code (if not written after an #). A R Markdown File has options to design an HTML Output and a PDF Output. This can increase your efficiency in terms of working with partners. Further, you can write your code in chunks and have plenty options to work with those chunks. In QM and AQM you will exclusively use R Markdown and over time you will see the advantages of R Markdown. On the top right you can see the Environment. Here you have an overview over all the objects currently loaded in your environment. You will learn more about objects later in the course. On the bottom left you have the Console: This is where the results appear once you execute your R-code. You can also directly type R-code into the console and execute it. However, we cannot save this code which is why we usually work in the Editor. On the bottom right you can see the Output: Plots (if you use the standard R Script) and other things will appear here, don’t worry too much about it for the moment. 1.1.2 How to design it according to your preferences (optional) You can change the appearance of RStudio: Tools &gt; Global Options &gt; Appearance. Here you can change the zoom of the Console, the font size of the your letters and the style of your code. Further, you can change RStudio to a dark theme. Play around with it and find out how it is the most comfortable for you and of course you can change it over time. You can change the Pane Layout meaning where the four components of RStudio should be: RStudio: Tools &gt; Global Options &gt; Pane Layout. You should use key shortcuts. There are pre-installed short-cuts of RStudio, which are really helpful. You should get familiar with them. Tools &gt; Key Shortcut Helps or directly with Ctrl + Shift + K. You can add your own Key Shortcuts and we will do that in this course. 1.2 Lets get started: R as a fancy calculator 1.2.1 Mathmetical Operations in R You can use R for basic calculations: #You can use hashtags to comment 1 + 1 #Addition ## [1] 2 1 - 1 #Substraction ## [1] 0 1 * 1 #Multiplication ## [1] 1 1 / 1 #Division ## [1] 1 2^(1 / 2) #Mixed Terms ## [1] 1.414214 You can use R also for TRUE/FALSE statements for logical statements via the comparison operators: &gt; greater than &lt; smaller than == equal != not equal &gt;= greater than or equal to &lt;= less than or equal to 1 &lt; 3 #TRUE ## [1] TRUE 5 &gt;= 8 #FALSE ## [1] FALSE 11 != 10 #TRUE ## [1] TRUE 22 == 22 #TRUE ## [1] TRUE 7 &lt; 3 #FALSE ## [1] FALSE 5 &lt;= 2+3 #TRUE ## [1] TRUE You can also use logical operators: - &amp; element-wise AND operator. It returns TRUE if both elements are true - | element-wise OR operator. It returns TRUE if one of the statements is TRUE - ! Logical NOT operator. It returns FALSE if statement is TRUE 5 &amp; 4 &lt; 8 #TRUE ## [1] TRUE 5 | 4 &lt; 8 #TRUE ## [1] TRUE !5 &gt; 2 #FALSE ## [1] FALSE 1.2.2 Using Commands For more advanced operations you can and should use functions. Functios are mostly a word with brackets. Within a function, there are so called arguments. These arguments specify the function and give it the information it needs + optional information. e.g. sqrt(x) taking the square root, x is any number. exp(x) the constant e, x is any number. mean(x) for the mean, x is any number. median(x) for the median, x is any number. sqrt(x = 36) #square root exp(x = 0) # exponential of 1 print(&quot;U can stay under my umbrella&quot;) #with this command you can print what you want I explicitly choose easy examples, but sometimes commands can be complicated, because they demand special inputs. To get help, our first step should be to ask R itself: You can put a question mark in front of a function and execute it. In your output under the tab “Help” an explanation with examples will pop up and explain the function. You can also call the help() function and put the function you want to learn about without brackets in the help() function. ?exp() #questionmark help(exp) #help command 1.2.3 Assigning objects and printing them Most of the time you will store your results in objects. You can do so by using the &lt;- operator. Afterwards you can work with the objects. Let us assign numbers to out objects. The objects will be saved and you can see them in your environment. Pizza &lt;- 7.50 #pizza object Cola &lt;- 3.50 #cola object Pizza + Cola #addition of objects ## [1] 11 We can then go on work with the content of the objects. For beginners, let us add the object Pizza and the object Cola together. We can also save the result of those object in a new object and work with this object and this can go on forever technically. Offer &lt;- Pizza + Cola #assigning addition Offer #printing the object ## [1] 11 Offer^2 #square the term with ^2 ## [1] 121 1.2.4 Vectors In R a vector contains more than one information. You use the c() command, and divide the information with a ,. Let us compare food prices: food &lt;- c(&quot;Pizza&quot;, &quot;Kebab&quot;, &quot;Curry&quot;, &quot;Fish&quot;, &quot;Burrito&quot;) #food vector print(food) #printing it ## [1] &quot;Pizza&quot; &quot;Kebab&quot; &quot;Curry&quot; &quot;Fish&quot; &quot;Burrito&quot; prices &lt;- c(7.50, 6.00, 8.50, 3.00, 11.00) #price vector print(prices) #printing it ## [1] 7.5 6.0 8.5 3.0 11.0 cola_prices &lt;- c(3.50, 3, 4, 2.50, 3) #cola prices vector print(cola_prices) #printing it ## [1] 3.5 3.0 4.0 2.5 3.0 Now we can calculate the prices for a decent meal in one step by adding the two vectors together. The vector prices_combined will give us the prices for a meal plus a cola: prices_combined &lt;- prices + cola_prices #prices combined print(prices_combined) #printing it ## [1] 11.0 9.0 12.5 5.5 14.0 1.2.5 Object Classes Objects can contain information of different data types: Numeric Numbers c(1, 2.4, 3.14, 4) Character Text c(\"1\", \"blue\", \"fun\", \"monster\") Logical True or false c(TRUE, FALSE, TRUE, FALSE) Factor Category c(\"Strongly disagree\", \"Agree\", \"Neutral\") For data analysis commands sometimes require special object classes. With the class() command we can find out the class. And with as.numeric for example we can change classes by assigning it to itself, by it is common to assign it to a new object: #Let us find out the classes class(prices) #numeric ## [1] &quot;numeric&quot; class(food) #character ## [1] &quot;character&quot; class(cola_prices) #numeric ## [1] &quot;numeric&quot; We can also change the classes of variables. To do so, we can use as.factor(), as.numeric(), as.character() and so forth. You can do that for every class. Let us change the cola_prices to a vector. To do so, we change call as.character() and put the object in it. Then we assign it to another object called cola_prices_character. This object will have the class \"character\". #We want the cola_prices vector to be a character cola_prices_character &lt;- as.character(cola_prices) #Checking it class(cola_prices_character) ## [1] &quot;character&quot; print(cola_prices_character) ## [1] &quot;3.5&quot; &quot;3&quot; &quot;4&quot; &quot;2.5&quot; &quot;3&quot; 1.2.6 Matrices 1.2.6.1 Making Matrices There are different ways of building a matrix. Let us start by just binding the vectors as columns together. You can do that cbind() if you want to bind columns together. rbind() is therefore the command to bind rows together. You have to call cbind() and include the vectors you want to bind together The same with rbind() price_index &lt;- cbind(food, prices, cola_prices) #We bind it together print(price_index) #We print it ## food prices cola_prices ## [1,] &quot;Pizza&quot; &quot;7.5&quot; &quot;3.5&quot; ## [2,] &quot;Kebab&quot; &quot;6&quot; &quot;3&quot; ## [3,] &quot;Curry&quot; &quot;8.5&quot; &quot;4&quot; ## [4,] &quot;Fish&quot; &quot;3&quot; &quot;2.5&quot; ## [5,] &quot;Burrito&quot; &quot;11&quot; &quot;3&quot; #Let&#39;s do the same by binding the rows together price_index2 &lt;- rbind(food, prices, cola_prices) #We bind it together print(price_index2) #We print it ## [,1] [,2] [,3] [,4] [,5] ## food &quot;Pizza&quot; &quot;Kebab&quot; &quot;Curry&quot; &quot;Fish&quot; &quot;Burrito&quot; ## prices &quot;7.5&quot; &quot;6&quot; &quot;8.5&quot; &quot;3&quot; &quot;11&quot; ## cola_prices &quot;3.5&quot; &quot;3&quot; &quot;4&quot; &quot;2.5&quot; &quot;3&quot; We can also generate a matrix by simulating it. There are a lot of things to care about, simply because a lot is possible: You first call the matrix() command. The first argument is an interval of numbers. These are our total observations if you want. The second and third argument are our number of rows nrow() and our number of columns ncol(). If you multiply them, they have to result in the number of observations you defined before. We have 20 numbers and 4 multiplied by 5 is 20, thus this is fine. Lastly you have to define if the numbers should be included from left to right, thus by row or if they should be ordered from top to bottom. We go through both examples and then it should become clear. The dim() command is helpful, because it shows us to inspect the dimensions. # Create a matrix matrix_example &lt;- matrix(1:20, nrow = 4, ncol = 5, byrow = T) # # Checking it print(matrix_example) ## [,1] [,2] [,3] [,4] [,5] ## [1,] 1 2 3 4 5 ## [2,] 6 7 8 9 10 ## [3,] 11 12 13 14 15 ## [4,] 16 17 18 19 20 # Checking the dimensions dim(matrix_example) ## [1] 4 5 # What happens if byrow is set to FALSE? matrix_example2 &lt;- matrix(1:20, nrow = 4, ncol = 5, byrow = F) # Checking it print(matrix_example2) ## [,1] [,2] [,3] [,4] [,5] ## [1,] 1 5 9 13 17 ## [2,] 2 6 10 14 18 ## [3,] 3 7 11 15 19 ## [4,] 4 8 12 16 20 dim(matrix_example2) ## [1] 4 5 1.2.6.2 Working with Matrices We want to work with matrices. The first tool to learn is how to inspect the matrices: First, you call the matrix. In our example, matrix_example with square brackets. In these brackets you can call single rows by entering the number of the row you want to inspect, and then you put a comma behind it to signal R that you want to have the row. If you put a number behind the comma, you tell R to give you the column with the number. If you want a single number then you have to define a row and a column. #Let us get used to work with objects row &lt;- 1 column &lt;- 1 #Printing it print(object1 &lt;- matrix_example[row, ]) #printing the first row ## [1] 1 2 3 4 5 print(object2 &lt;- matrix_example[, column]) #printing the first column ## [1] 1 6 11 16 print(object3 &lt;- matrix_example[row, column]) #printing first row and column ## [1] 1 print(matrix_example) #printing the matrix ## [,1] [,2] [,3] [,4] [,5] ## [1,] 1 2 3 4 5 ## [2,] 6 7 8 9 10 ## [3,] 11 12 13 14 15 ## [4,] 16 17 18 19 20 #More Information nrow(matrix_example) #How many rows ## [1] 4 ncol(matrix_example) #How many columns ## [1] 5 dim(matrix_example) #Overall dimensions ## [1] 4 5 1.2.7 Data Frames The next type of data storage are data frames. These are the standard storage objects for data in R. The reason is simple, matrices are only able to contain one type of variables (numeric, character, etc…). In this example, we have a dataset with the variable country (character), the capital (character), the population in mio (numeric), and a if the country is in europe (logical) # making an example df df_example &lt;- data.frame( country = c(&quot;Austria&quot;, &quot;England&quot;, &quot;Brazil&quot;, &quot;Germany&quot;), capital = c(&quot;Vienna&quot;, &quot;London&quot;, &quot;Brasilia&quot;, &quot;Berlin&quot;), pop = c(9.04, 55.98, 215.3, 83.8), europe = c(TRUE, FALSE, TRUE, TRUE) ) # Checking it print(df_example) ## country capital pop europe ## 1 Austria Vienna 9.04 TRUE ## 2 England London 55.98 FALSE ## 3 Brazil Brasilia 215.30 TRUE ## 4 Germany Berlin 83.80 TRUE If you want to work with data frames you can call columns by calling the name of the data frame putting a dollar sign behind it and then calling the name of the column, you want to inspect: df_example$country ## [1] &quot;Austria&quot; &quot;England&quot; &quot;Brazil&quot; &quot;Germany&quot; You see that you get the vector of the column. We can go further and work more with data frames Let us call a single observation in the data frame: You call the column you want to inspect the same way as before, this time you also put square brackets behind and call the number of the observation in the column. We want to have “Brazil”. “Brazil” is the third element of the df_example$country, thus we include 3 in the square brackets: df_example$country[3] ## [1] &quot;Brazil&quot; The last thing to do with data frames is to get columns based on conditions. In the next part of this chapter we will get a method to do so, but we can do so as well with the data frame. Imagine you want to have a vector of df$country, but only with countries that have a df$pop bigger than 60. Meaning a population bigger than 60. To do so we call the df$country column and put square brackets behind it Further we need to call in the square brackets the condition. Thus, the variable df_example$pop and set it to bigger than 60. Et voila, we get the columns of df_example$country bigger than 60. df_example$country[df_example$pop &gt; 60] ## [1] &quot;Brazil&quot; &quot;Germany&quot; 1.3 ifelse statements and the ifelse() function One of the most frequently used and therefore most important logical operators in programming in general are ifelse commands. You probably know them from Excel, but every programming language includes them, because of their usefulness. A quick reminder of their logic. 1.3.1 If else statement with only one condition First, you define an if statement. The if statement is a logical statement for example bigger, smaller than X. The logical statement is your test expression. With this you tell the program to test the condition for an object, in excel a cell or whatever object in your programming language can be tested. The program then checks if the condition is TRUE or FALSE. Until this point every if else is the same and now we will look at some variations: Figure 2: Logic of if-else statements Figure 2 shows the logic of an if else statement with one condition. We say that if the test expression is true something should happen. If not nothing happens, because nothing was defined. We want R to judge our grades in school. For this reason we define an object called grade and assign 1.7 to it. In the next step we call if and open a bracket. We include the test expression in the bracket, which shall be if grade, our grade is smaller than 2 Then we open curly brackets to define what should happen if the test expression is TRUE. Meaning what should happen if the grade is better than 2. We define print(“Good Job”). Here is the general logic of a if else statement: if (test expression) { Body of if } grade &lt;- 1.7 if(grade &lt; 2) { print(&quot;Good Job&quot;) } # You write down if(test expression), and then the {body expression}, thus the body expression in fancy brackets. ## [1] &quot;Good Job&quot; grade &lt;- 2.5 if(grade &lt; 2) { print(&quot;Good Job&quot;) } #Since the condition is not met, nothing happens As you can see if the grade is bigger than 2 nothing happens. Otherwise “Good Job” is printed as intended. 1.3.2 if statements with else condition An if command on its own is quite useless. Things get interesting, when we also have a body for the else command. What happens now, is that instead of nothing being printed when the test expression is FALSE. Then the body of else is printed. Figure 3: Standard RStudio Interface Now we just add a body of else into the equation, in R that means we need to define it: We take the if else statement of the chunk before and now we add an else and open curly brackets, where we define the body of else. grade &lt;- 3.3 #assigning a grade if (grade &lt;= 2) { print(&quot;Good Job&quot;) } else { print(&quot;Life goes on&quot;) } ## [1] &quot;Life goes on&quot; grade &lt;- 1.3 #assigning a grade if (grade &lt;= 2) { print(&quot;Good Job&quot;) } else { print(&quot;Life goes on&quot;) } ## [1] &quot;Good Job&quot; We see that in any case a result is printed. 1.3.3 The ifelse() command What you see above is the manual way of coding an if else condition. But there is also an ifelse() function in R, where you do not have different colors and fancy brackets everywhere. you call ifelse() The first argument is your test expression The second the body of if The third the body of else ifelse(test expression, body of if, body of else) ifelse(grade &lt;=2, &quot;Good Job&quot;, &quot;Life goes on&quot;) #ifelse command ## [1] &quot;Good Job&quot; 1.3.4 if else ladders/ if else with multiple conditions The world is a complex place. Sometimes things are not black and white. Thus we have to be prepared for different scenarios. To make it less melodramatic, if else statements are also possible with several bodies of else. Let us take the example that we want to have a statement of R regarding our grade for different number intervals. What R does is to check the first condition, afterward he checks the second test expression and so forth until he finds a hit and prints it. If no expression is found, you have to define a last else expression. You add else if with curly brackets where the test expression is included. Lastly, you define an else with curly brackets for the case there is not test expression grade &lt;- 3.3 #Assigning a grade if (grade == 1.0) { print(&quot;Amazing&quot;) } else if (grade &gt; 1.0 &amp; grade &lt;= 2.0) { print(&quot;Good Job&quot;) } else if (grade &gt; 2.0 &amp; grade &lt;= 3.0) { print(&quot;OK&quot;) } else if (grade &gt; 3.0 &amp; grade &lt;= 4.0) { print(&quot;Life goes on&quot;) } else { print(&quot;No expression found&quot;) } ## [1] &quot;Life goes on&quot; grade &lt;- 1.7 #Assigning a grade if (grade == 1.0) { print(&quot;Amazing&quot;) } else if (grade &gt; 1.0 &amp; grade &lt;= 2.0) { print(&quot;Good Job&quot;) } else if (grade &gt; 2.0 &amp; grade &lt;= 3.0) { print(&quot;OK&quot;) } else if (grade &gt; 3.0 &amp; grade &lt;= 4.0) { print(&quot;Life goes on&quot;) } else { print(&quot;No expression found&quot;) } ## [1] &quot;Good Job&quot; grade &lt;- 5.0 #Assigning a grade if (grade == 1.0) { print(&quot;Amazing&quot;) } else if (grade &gt; 1.0 &amp; grade &lt;= 2.0) { print(&quot;Good Job&quot;) } else if (grade &gt; 2.0 &amp; grade &lt;= 3.0) { print(&quot;OK&quot;) } else if (grade &gt; 3.0 &amp; grade &lt;= 4.0) { print(&quot;Life goes on&quot;) } else { print(&quot;No expression found&quot;) } ## [1] &quot;No expression found&quot; Again we can do so-called nested ifelse function, using the ifelse() command: Instead of defining an else condition, we define another ifelse() command and do as long as we need to. The last ifelse() is then the only one with a clear else condition. grade &lt;- 1.7 #Assigning a grade ifelse(grade == 1.0, &quot;Amazing&quot;, ifelse(grade &gt; 1 &amp; grade &lt;= 2, &quot;Good Job&quot;, ifelse(grade &gt; 2 &amp; grade &lt;= 3, &quot;OK&quot;, ifelse(grade &gt; 3 &amp; grade &lt;=4, &quot;Life goes on&quot;, &quot;No expression found&quot; ) ) ) ) ## [1] &quot;Good Job&quot; grade &lt;- 3.3 #Assigning a grade ifelse(grade == 1.0, &quot;Amazing&quot;, ifelse(grade &gt; 1 &amp; grade &lt;= 2, &quot;Good Job&quot;, ifelse(grade &gt; 2 &amp; grade &lt;= 3, &quot;OK&quot;, ifelse(grade &gt; 3 &amp; grade &lt;=4, &quot;Life goes on&quot;, &quot;No expression found&quot;) ) ) ) ## [1] &quot;Life goes on&quot; # The same logic: ifelse(test expression, body expression if, ifelse(test expression 2, body expression if 2)) etc.. 1.4 Outlook This section was a brief introduction to the fundamentals of R. It was kept simple to give you a feeling for a R. These are the absolute basics, which are needed to understand everything, which will be built based on it. 1.5 Exercise Section 1.5.1 Exercise 1: Making your first Vector Create a vector called my_vector with the values 1,2,3 and check is class. #create the vector my_vector &lt;- #check the class 1.5.2 Exercise 2: Making your first matrix Create a Matrix called student. This should contain information about the name, age and major. Make three vectors wiht three entries and bind them together to a the matrix student. Print the matrix. #Create the vectors name &lt;- age &lt;- major &lt;- #Create the matrix student &lt;- #Print the matrix 1.5.3 Exercise 3: ifelse function Write an ifelse statement that checks if a given number is positive or negative. If the number is positive, print “Number is positive”, otherwise print “Number is negative”. Feel free to decide if you want to use the ifelse function or the ifelse condition. #Assigning the number to the object &quot;number&quot; number &lt;- -4 1.5.4 Exercise 4: ifelse ladders Write an if-else ladder that categorizes a student’s grade based on their score. The grading criteria are as follows: Score &gt;= 90: “A” Score &gt;= 80 and &lt; 90: “B” Score &gt;= 70 and &lt; 80: “C” Score &gt;= 60 and &lt; 70: “D” Score &lt; 60: “F” "],["data-manipulation.html", "Chapter 2 Data Manipulation 2.1 Packages 2.2 Working with packages 2.3 The Data we will work with: The European Social Survey (ESS) 2.4 Let’s wrangle the data: The dplyr package 2.5 Merging Datasets 2.6 Outlook 2.7 Exercise Section", " Chapter 2 Data Manipulation In this chapter we load and transform data. That is a standard step and probably the step taking the most time, when analyzing data. Preparing data is necessary, since data analysis requires data in a certain format. Which format depends on the model. I will introduce you to dplyr, the standard package for manipulating data. At first, it seems a bit complicated, but I guarantee you, if you do data analyses in R, you will have to work with it a lot and you will become quite fluent in it. The goal of this chapter is to teach you how to manipulate data so that we can bring it into the format we need it to analyse it properly. Have fun! 2.1 Packages This far, we’ve only covered so-called “base R functions” or “built-in functions”, but R has an active community and sometimes further operations are needed, so we use packages. These are including further functions, which we will use heavily in the following section. 2.1.1 The tidyverse package One of the most influential and widely used package in R is the tidyverse package. This package includes several other packages, which are key for data manipulation e.g. dplyr, ggplot2, stringr, readr, tidyr. 2.2 Working with packages 2.2.1 Installing packages To install packages you use the very creative install.packages() command in R. Note that it is necessary to directly install a package in R. This step is only required once: Call the install.packages() command. You put the name of the package in quotation marks into the function. install.packages(\"pacman\") 2.2.2 Loading your packages While you only need to install a package once, you need to load it every time in your script, when you open it. You can do that with the library() function in R: Call the library() function. Put the name of library(pacman) It is always important to have an efficient workflow in R. Traditional R users, load all packages they need at the beginning of their page. Logically, so they just need to go back to the top of the script and need to load it every time they open the script. But there are way more elegant and pragmatic ways to do that. One way is the pacman package: First, if you use the name of package and put a “::” behind it you tell R to go into the package and to specifically get one command of the package, in our case the p_load command. Second, the p_load command, loads the packages in the brackets and checks if they are installed, if not, it automatically installs and loads them. pacman::p_load(&quot;tidyverse&quot;, &quot;psych&quot;, &quot;gapminder&quot;) #loading packages 2.3 The Data we will work with: The European Social Survey (ESS) For the following Data Manipulation Part, we will use the European Social Survey Round 10 with the topic “Democracy, Digital social contacts”. It is a high-quality survey conducted in 31 European countries. Round 10 was conducted in 2020 and is the most recent ESS. We will use it, since survey data is quite popular among students and further at some point everyone needs to work with it. But do not worry if you do not like survey data, we will also cover other prominent Datasets. You can freely download it via the website of the ESS. From there you need to go to the Data Portal and than you can download the Round you want, in the format you want. As already mentioned, use the .dta or .csv format. Variable Description Scales idnt Respondent’s identification number unique number from 1-9000 year The year when the survey was conducted only 2020 cntry Country BE, BG, CH, CZ, EE, FI, FR,GB, GR, HR, HU, IE, IS, IT, LT,NL, NO, PT, SI, SK agea Age of the Respondent, calculated Number of Age = 15-90 999 = Not available gndr Gender 1 = Male; 2 = Female; 9 = No answer happy How happy are you 0 (Extremly unhappy) - 10 (Extremly happy); 77 = Refusal; 88 = Don’t Know; 99 = No answer eisced Highest level of education, ES - ISCED 0 = Not possible to harmonise into ES-ISCED; 1 (ES-ISCED I , less than lower secondary) - 7 (ES-ISCED V2, higher tertiary education, =&gt; MA level; 55 = Other; 77 = Refusal; 88 = Don’t know; 99 = No answer netusoft Internet use, how often 1 (Never) - 5 (Every day); 7 = Refusal; 8 = Don’t know; 9 = No answer trstprl Most people can be trusted or you can’t be too careful 0 (You can’t be too careful) - 10 (Most people can be trusted); 77 = Refusal; 88 = Don’t Know; 99 = No answer lrscale Left-Right Placement 0 (Left) - 10 (Right); 77 = Refusal; 88 = Don’t know; 99 = No answer Note: In the following, I will simulate the ESS with the same names and same range values. Which means if you read in the actual ESS, you should be able to run the code as well without problems! I do so for reproducibility reasons. When downloading the script, users should be able to run the whole script with one click. This is because users, who are new to R might have problems with setting working directories, since working directories are prone to errors, especially at the beginning. 2.3.1 Load the data 2.3.1.1 How to load data The first thing to do, when cleaning data is to load the data into R. The first thing to ensure is to know, where R can take the data from and load it into its own environment. You can call data from your local devise or remote through e.g. an API: Remote: This means the dataset is laying around on some external Cloud or Server and you have the possibility to load it into R, without downloading it on your local devise. Depending on the dataset there are different ways, I will show you later how to get data from the World Bank. Local: To get data from your devise, you have to tell R, where to find it, meaning you tell him the path. Go to your file and click on it. The path will be shown in the address bar, just copy it and paste it in R with the responding command. The command is defined by the type of the file as you can see in the table below. Here is an example how it could look wit a .csv file data &lt;- read.csv(\"C:/Users/YourUsername/Documents/data.csv\") This table presents some of the most frequent data types, which you can download and load into R. There are of course more data types, and to find out if you can read them into R, you can just google the type and look for the command. Mostly you will find a package with a command. And mostly, those commands start with “read”. Examples of Different Data Types and how to load them into R File File Extension Package Command Stata .dta haven read_dta() CSV-Files .csv readr (is included in the tidyverse package) read_csv() Excel-Files .xlsx; .xls readxl read.rds() RData (also RDS Data) .RData;.rds base R functions, no package required load() , read.rds() 2.3.1.2 Working Directories and R-Projects If you have more data and files you want to load into R, it is not recommended to copy always the path of every file. Instead it is common to work with working directories or R-Projects. There is always a working directory you can find out, in which working directory you are currently at by using the command getwd(), just run it in your console and R will give you the path it currently is working at. Let us assume you saved all your data in a folder called “Intro_to_R_course”, then you can set the working directory separately with this command (do not forget to change the names in the paths): setwd(\"C:/Users/YourUsername/Intro_to_R_course\") After you set the working directory you can run again getwd() and the path now has to be changed to the content of the setwd(). The advantage is now that if the working directory is set, you can load in the data without specifying the path. If your data is in the path of the working directory you can load like that: data &lt;- read.csv(\"data.csv\") You can also create an R-Project. If you click on file &gt; New Project &gt; New Directory &gt; New Project you can determine the working directory and create a folder in it. In this folder, there will be an R-Project file, if you open this file, a blank R environment will appear. Now, you can create files in this folder. And every time you enter R through the R-Project file, you do not have to set any working directories. Because if the R-Projects opens R, it automatically sets the working directory to where you created the folder. I would not recommend to work with R-Projects in the beginning, but when you work with collaborators and want to make your code reproducible for others, then you will have to work with R-Projects one day. Again, this also the reason, I just simulate all the data, so you can work with the code without worrying about any working directories. 2.3.2 One last thing: Pipelines Sometimes codes have several dimensions, which could make it quite complicated leave_house(get_dressed(get_out_of_the_bed(wake_up(me)))) Well, as you can see there are too many dimensions and with tidyverse you can basically split it up into so-called Pipelines, for them you use a Pipe %&gt;% : me %&gt;% wake_up() %&gt;% get_out_of_the_bed() %&gt;% get_dressed() %&gt;% leave_house It is the same code, in R this code would do the same. But the advantage is that it is way more intuitive and makes the code clear. Here an example: First, I create a vector with three random numbers named q. Then I take the mean, then the exponential and lastly the square root. I could just wrap the codes around each other like this: sqrt(exp(mean(q))). But I could also use pipes, where I clearly see that first the mean, then the exponential and then the square root is taken: q %&gt;% mean() %&gt;% exp() %&gt;% sqrt(). Run both codes, they are producing the same result. #Making a vector with three random numbers q &lt;- c(6,3,8) #Taking first the mean, second the exponential and lastly the square root sqrt(exp(mean(q))) ## [1] 17.00204 ###With a Pipe q %&gt;% mean() %&gt;% exp() %&gt;% sqrt() ## [1] 17.00204 This was an easy and short example, but as you will see at the end of this chapter, the code can get really fast really messy and to have a clean code, we will use pipes. Furthermore, tidyverse users use pipes all the times, so even if you do not use them, you have to understand them. 2.4 Let’s wrangle the data: The dplyr package The dplyr package is THE standard package, when it comes to data manipulation (next to Base R of course). It has essential functions, and helpful further functions. If you are able to understand the flexibility of these functions you can easily handle every data set. 2.4.1 The filter() command The first function I introduce you is the filter() function. Within the filter() function we can define certain conditions to cut our Data Set to. We need the filter() function and then a variable we want to filter based on. In my example, I only want to keep all observations, which have “HU” in their cntry - variable. Substantially this means, I cut down to all observations from Hungary. I use the == operator since I want to have all observations where the condition is true. Here is a quick reminder of logical operators in R: Logical Operator Meaning == equals &lt; smaller than &gt; greater than &lt;= less than or equal to &gt;= greater than or equal to ! (e.g. !=; &gt;!; &lt;!…) not equal, not greater than, not smaller than &amp; element-wise AND operator. It returns TRUE if both elements are true | element-wise OR operator. It returns TRUE if one of the statements is TRUE 2.4.1.1 Filtering for only one condition We start by defining a new object, let us call it d1. Then we take the data we want to filter, in our case ess. We define a pipe, write down filter and define a condition, in our case that only cases where cntry is equal to the iso2c code of Hungary, \"HU\". In the next code, we do the same and filter for cases that are equal or smaller than 40. Thus we get a dataset with observations who are 40 or younger. Note: Since the cntry variable is a character variable, we have to put the condition in square brackets. The variable agea is a numeric variable, therefore we only need the number. #filtering for cases only in Hungary d1 &lt;- ess %&gt;% filter(cntry == &quot;HU&quot;) #checking it head(d1) ## idnt year cntry agea gndr happy eisced netusoft trstprl ## 1 4501 2020 HU 21 2 3 6 4 3 ## 2 4502 2020 HU 48 1 5 5 3 5 ## 3 4503 2020 HU 80 1 6 3 4 4 ## 4 4504 2020 HU 38 1 6 3 2 1 ## 5 4505 2020 HU 40 2 10 3 3 9 ## 6 4506 2020 HU 40 1 1 7 2 2 ## lrscale ## 1 2 ## 2 3 ## 3 1 ## 4 3 ## 5 7 ## 6 3 #We only want participants younger than 40 d2 &lt;- ess %&gt;% filter(agea &lt;= 40) #checking it head(d2) ## idnt year cntry agea gndr happy eisced netusoft trstprl ## 1 3 2020 BE 28 2 3 4 5 4 ## 2 8 2020 BE 28 2 5 1 2 1 ## 3 9 2020 BE 39 2 8 3 1 4 ## 4 12 2020 BE 23 1 77 1 4 4 ## 5 14 2020 BE 40 2 3 5 2 3 ## 6 15 2020 BE 21 1 1 4 1 8 ## lrscale ## 1 7 ## 2 1 ## 3 6 ## 4 7 ## 5 2 ## 6 5 2.4.1.2 Filtering for multiple condition #filtering for cases in Hungary and France d1 &lt;- ess %&gt;% filter(cntry %in% c(&quot;HU&quot;, &quot;FR&quot;)) #Checking it head(d1) ## idnt year cntry agea gndr happy eisced netusoft trstprl ## 1 2701 2020 FR 41 9 7 7 8 8 ## 2 2702 2020 FR 64 1 2 5 4 5 ## 3 2703 2020 FR 90 1 5 6 5 1 ## 4 2704 2020 FR 23 1 9 3 3 9 ## 5 2705 2020 FR 30 2 8 7 2 7 ## 6 2706 2020 FR 42 2 10 88 9 9 ## lrscale ## 1 6 ## 2 4 ## 3 8 ## 4 3 ## 5 9 ## 6 5 #filtering for cases under 40 in Hungary and France d2 &lt;- ess %&gt;% filter(cntry %in% c(&quot;HU&quot;, &quot;FR&quot;) &amp; agea &lt;= 40) #Checking it head(d2) ## idnt year cntry agea gndr happy eisced netusoft trstprl ## 1 2704 2020 FR 23 1 9 3 3 9 ## 2 2705 2020 FR 30 2 8 7 2 7 ## 3 2708 2020 FR 17 1 1 2 2 8 ## 4 2709 2020 FR 26 2 10 1 4 9 ## 5 2710 2020 FR 34 1 8 3 1 3 ## 6 2711 2020 FR 30 1 5 3 5 9 ## lrscale ## 1 3 ## 2 9 ## 3 3 ## 4 6 ## 5 8 ## 6 10 #d2 &lt;- ess %&gt;% #Our dataset # filter(cntry %in% c(&quot;HU&quot;, &quot;FR&quot;), # agea &lt;= 40) #filtering for cases under 40 in Hungary and France with a comma #head(d2) 2.4.2 The select() function: We obviously do not care about all variables a dataset can offer (mostly). To select the variables we need, we can use the select() function. This of course depends on our research question. Let us say we want to select the year, the country, the happy variable, age, gender, income and education. 2.4.2.1 Selecting and deleting single Rows We only have to pass their column names to select(). That’s it. #Selecting relevant variables d1 &lt;- ess %&gt;% select(year, cntry, happy, agea, gndr, eisced) #Checking it head(d1) ## year cntry happy agea gndr eisced ## 1 2020 BE 1 45 1 6 ## 2 2020 BE 3 65 1 5 ## 3 2020 BE 3 28 2 4 ## 4 2020 BE 1 81 1 7 ## 5 2020 BE 4 56 2 7 ## 6 2020 BE 3 64 1 5 2.4.2.2 Deleting Rows To delete row, you just put a minus in front of the column, you want to delete. That’s it, the rest stays the same. #We delete columns by simply putting a comma before it d2 &lt;- d1 %&gt;% select(-agea) #Checking it head(d2) ## year cntry happy gndr eisced ## 1 2020 BE 1 1 6 ## 2 2020 BE 3 1 5 ## 3 2020 BE 3 2 4 ## 4 2020 BE 1 1 7 ## 5 2020 BE 4 2 7 ## 6 2020 BE 3 1 5 2.4.2.3 Combining select() with the filter() function One huge advantage of piping is, that we can clear our data in one step or at least big steps. The only thing you have to be aware of is what you put there. Remember the last command, is always the first to be executed. In this example, we select the 7 variables and then filter it for all observations under 40. The two commands are separated by a pipe. #Combining codes d1 &lt;- ess %&gt;% filter(agea &lt; 40) %&gt;% select(year, cntry, happy, agea, gndr, eisced) #Checking it head(d1) ## year cntry happy agea gndr eisced ## 1 2020 BE 3 28 2 4 ## 2 2020 BE 5 28 2 1 ## 3 2020 BE 8 39 2 3 ## 4 2020 BE 77 23 1 1 ## 5 2020 BE 1 21 1 4 ## 6 2020 BE 5 23 2 6 2.4.3 The arrange() function If we want our data to be in a certain order, we arrange it with this function. 2.4.3.1 Arranging in ascending order The arrange() function is called, and afterwards we call the variable we want to arrange based on. The default function of arrange, orders the data always in ascending order. #Adding arrange() d1 &lt;- ess %&gt;% filter(agea &lt; 40) %&gt;% select(year, cntry, happy, agea, gndr, eisced) %&gt;% arrange(agea) #Checking it head(d1) ## year cntry happy agea gndr eisced ## 1 2020 BE 6 15 2 4 ## 2 2020 BE 2 15 1 1 ## 3 2020 BG 3 15 9 99 ## 4 2020 BG 4 15 1 6 ## 5 2020 BG 2 15 2 2 ## 6 2020 BG 2 15 1 3 2.4.3.2 Arranging in descending order If we want to order them in ascending order, we have to call desc() inside the arrange() and put the name of the variable inside desc(). #arranging in descending order d1 &lt;- ess %&gt;% filter(agea &lt; 40) %&gt;% select(year, cntry, happy, agea, gndr, eisced) %&gt;% arrange(desc(agea)) #Checking it head(d1) ## year cntry happy agea gndr eisced ## 1 2020 BE 8 39 2 3 ## 2 2020 BE 1 39 1 2 ## 3 2020 BE 10 39 2 5 ## 4 2020 BE 2 39 1 3 ## 5 2020 BE 6 39 2 7 ## 6 2020 BE 3 39 1 1 2.4.4 The rename() and relocate() function Two functions to make our dataset structured more useful are rename() and relocate(), well they do what they basically named after: 2.4.4.1 Renaming Variables: rename() Rename() follows a simple logic, you call the function and write down the new name, thus the name you want to assign, then you put an equal sign, and put in the old name, thus the current name of the column. Note: If you have a variable, which is binary, thus has two discrete categories, you name it after the category which corresponds to the higher value. If male is 0 and female is 1, you name it after the higher category 1, therefore the variable is named female. #Renaming variables d1 &lt;- ess %&gt;% filter(agea &lt; 40) %&gt;% select(year, cntry, happy, agea, gndr, eisced) %&gt;% arrange(desc(agea)) %&gt;% rename(country = cntry, age = agea, education = eisced, female = gndr) #Checking it head(d1) ## year country happy age female education ## 1 2020 BE 8 39 2 3 ## 2 2020 BE 1 39 1 2 ## 3 2020 BE 10 39 2 5 ## 4 2020 BE 2 39 1 3 ## 5 2020 BE 6 39 2 7 ## 6 2020 BE 3 39 1 1 2.4.4.2 Relocating Variables: relocate() You call relocate and determine the order of the columns. You can also rearrange single columns, you call the name of the column, which you want to rarrange. and then you either call .before and a column or .after and a column. And the column you want to rearrange is placed before or after the column you want to place. You separate both arguments with a comma. #relocating variables d1 &lt;- ess %&gt;% filter(agea &lt; 40) %&gt;% select(year, cntry, happy, agea, gndr, eisced) %&gt;% arrange(desc(agea)) %&gt;% rename(country = cntry, age = agea, education = eisced, female = gndr) %&gt;% relocate(education, age, female, country, happy, year) #determine the order #Checking it head(d1) ## education age female country happy year ## 1 3 39 2 BE 8 2020 ## 2 2 39 1 BE 1 2020 ## 3 5 39 2 BE 10 2020 ## 4 3 39 1 BE 2 2020 ## 5 7 39 2 BE 6 2020 ## 6 1 39 1 BE 3 2020 #relocate after d2 &lt;- ess %&gt;% filter(agea &lt; 40) %&gt;% select(year, cntry, happy, agea, gndr, eisced) %&gt;% arrange(desc(agea)) %&gt;% rename(country = cntry, age = agea, education = eisced, female = gndr) %&gt;% relocate(country, .after = age) #Checking it head(d2) ## year happy age country female education ## 1 2020 8 39 BE 2 3 ## 2 2020 1 39 BE 1 2 ## 3 2020 10 39 BE 2 5 ## 4 2020 2 39 BE 1 3 ## 5 2020 6 39 BE 2 7 ## 6 2020 3 39 BE 1 1 #relocating before d3 &lt;- ess %&gt;% filter(agea &lt; 40) %&gt;% select(year, cntry, happy, agea, gndr, eisced) %&gt;% arrange(desc(agea)) %&gt;% rename(country = cntry, age = agea, education = eisced, female = gndr) %&gt;% relocate(country, .before = age) #Checking it head(d3) ## year happy country age female education ## 1 2020 8 BE 39 2 3 ## 2 2020 1 BE 39 1 2 ## 3 2020 10 BE 39 2 5 ## 4 2020 2 BE 39 1 3 ## 5 2020 6 BE 39 2 7 ## 6 2020 3 BE 39 1 1 2.4.5 The mutate() function The next function is the powerful mutate() command. For the start, just think about mutate as a Variable with which you can transform or mutate variables to other variables as you please. You call mutate() and first you define the name of a new column with a name that is not existent in your dataset. You could also use the name of an existing column, but be careful! Then the new values are overwriting the old ones and we do not want that necessarily, therefore I recommend to always define new columns. After defining the new name you put an equal sign after it and define the calculation. In our case we just multiply happy, so all values in the happy column by 10 #mutating variables d1 &lt;- ess %&gt;% mutate(happy_10 = happy*10) #checking it head(d1) ## idnt year cntry agea gndr happy eisced netusoft trstprl ## 1 1 2020 BE 45 1 1 6 2 2 ## 2 2 2020 BE 65 1 3 5 4 3 ## 3 3 2020 BE 28 2 3 4 5 4 ## 4 4 2020 BE 81 1 1 7 3 2 ## 5 5 2020 BE 56 2 4 7 5 6 ## 6 6 2020 BE 64 1 3 5 4 6 ## lrscale happy_10 ## 1 3 10 ## 2 6 30 ## 3 7 30 ## 4 6 10 ## 5 4 40 ## 6 10 30 We can also use mutate for more than calculations. We can also define new columns by mutating existing columns into different classes: The variable new_variable is just a random mathematical operation including two variables. The second call in mutate() changes the class of the gndr variable to a character and saves it as such in the dataset. #more mutating d2 &lt;- ess %&gt;% mutate(new_variable = happy*10/eisced+67, female_char = as.character(gndr)) %&gt;% select(female_char, new_variable) #Checking it head(d2) ## female_char new_variable ## 1 1 68.66667 ## 2 1 73.00000 ## 3 2 74.50000 ## 4 1 68.42857 ## 5 2 72.71429 ## 6 1 73.00000 What makes mutate() so powerful is that you can use other functions in it to define your variables as you want. Image you have the happy variable. Quick reminder, this variable contains the answers to the question “How happy are you?” in the questionnaire of the European Social Survey. It is scaled on a 0 (not happy at all) to 10(very happy). Let us say we want to change that. We want to make a variable with only three categories (unhappy, neutral, happy). We decide that all values from 0-4 should be classifies as unhappy, 5 should be classified as neutral and everything above 5 as happy. How can we do that in R? There are different ways and I will show you some of them: 2.4.5.1 Recoding with mutate() using recode(). You can use the recode() function: First, we define a new column name, in our example happy_cat. Then we call recode() and inside of it we call the variable we want to transform, in our case happy. We put call the category we want to transform in these brackets ``. We put an equal sign after it and define the new value we want to assign to the category. Note: We can assign different values, do not worry about the “NA_real_” value I will come back to that later #Get an overview of the variable table(ess$happy) ## ## 1 2 3 4 5 6 7 8 9 10 77 88 99 ## 849 918 888 897 888 852 883 877 908 895 49 46 50 #recoding variables d1 &lt;- ess %&gt;% mutate( gndr_fac = as.factor(gndr), #always check the class happy_cat = dplyr::recode(happy, `1` = 0, `2` = 0, `3` = 0, `4` = 0, `5` = 1, `6` = 2, `7` = 2, `8` = 2, `9` = 2, `10` = 2, `77` = NA_real_, `88` = NA_real_, `99` = NA_real_), female = dplyr::recode(gndr_fac, `1` = &quot;Male&quot;, `2` = &quot;Female&quot;, `9` = NA_character_)) #Let us check how it worked out table(d1$happy_cat) ## ## 0 1 2 ## 3552 888 4415 table(d1$female) ## ## Male Female ## 4381 4404 2.4.5.2 Recoding with mutate() using case_when() As you see, the recode() command is quite extensive. The tidyverse offers a way more intuitive command, the case_when() function. The function case_when is a generalized ifelse function. Which means we can use logical operators. The recode() function is way too extensive, it gives you full control over the data, but we do not that much control: We again define happy_cat. We call case_when() and inside we call our variable we want to transform. We define a logical statements, in our case that happy smaller than 5, meaning that we tell R that all values under 5 should be transformed. We call the wave ~ and tell R what value should substitute all values which are TRUE for the logical statement. Note: What is not explicitly stated in the case_when() function will be coded as NA. #recoding with case_when d1 &lt;- ess %&gt;% mutate(gndr_fac = as.factor(gndr), happy_cat = case_when( happy &lt; 5 ~ 0, happy == 5 ~ 1, happy &gt; 5 ~ 2), female = case_when( gndr == 1 ~ &quot;Male&quot;, gndr == 2 ~ &quot;Female&quot; )) #Checking it table(d1$female) ## ## Female Male ## 4404 4381 table(d1$happy_cat) ## ## 0 1 2 ## 3552 888 4560 2.4.5.3 Recoding with mutate() using ifelse() Do you remember the ifelse() function? As already mentioned, the case_when() command is a generalized ifelse() function. If you want to keep it old school, we can also recode with the ifelse() function: #recoding with ifelse function d1 &lt;- ess %&gt;% mutate(gndr_fac = as.factor(gndr), happy_cat = ifelse(happy &lt; 5, 0, ifelse(happy == 5, 1, ifelse(happy &gt; 5, 2, NA ))), female = ifelse(gndr_fac == 1, &quot;Male&quot;, ifelse(gndr_fac == 2, &quot;Female&quot;, NA)) ) #Check it table(d1$happy_cat) ## ## 0 1 2 ## 3552 888 4560 table(d1$female) ## ## Female Male ## 4404 4381 2.4.6 Handling Missing Values/Incomplete Data As you saw right now, not all data in a dataset is complete. Of course not, there are several sources, which can lead to incomplete/missing data. Can you think of reasons why? In Data Sciences we need to deal with missing values directly. If we look into the codebook, the ESS declares different types of missing values with high numbers: 7(7) means “Refusal”, so the respondent refused to answer, 8(8) means “dont know”, and 9(9) “No answer”. Note that the ESS does so, since some researchers are interested in missing values, and why they happen, so they can investigate it. For us, this is a problem, because we cannot run an analysis with missing values. There are two options: Using statistics to artificially fill them out, this called multiple imputation techniques, but this requires advanced data science knowledge so I do not recommend that for beginners. Just delete incomplete observations to have a dataset without missing values The ESS assigns values to missing values. First, we have to tell R that we do want those values to be missing values otherwise it biases our analyses. In the following, we have to recode the variables and tell R, that the missing values are declared as such: I already showed you how to do so. In the following I will use case_when() We will do the second one, and I already showed how to recode useless values to NAs so this should be clear by now. Remember, the ifelse() and recode() explicitly need input to turn values into NAs. #Creating missing values and showing a mutating workflow d1 &lt;- ess %&gt;% filter(agea &gt;=40) %&gt;% select(year, cntry, netusoft, agea, eisced, gndr, happy) %&gt;% arrange(desc(agea)) %&gt;% rename( internet_use = netusoft, age = agea, education = eisced, female = gndr) %&gt;% mutate( internet_use = case_when( internet_use &lt; 5 ~ NA_real_, TRUE ~ internet_use), age = case_when( age == 999 ~ NA_real_, TRUE ~ age), education = case_when( education %in% c(55, 77, 88, 99) ~ NA_real_, TRUE ~ education), female = case_when( female == 1 ~ 0, female == 2 ~ 1, female == 9 ~ NA_real_, TRUE ~ female), happy = case_when( happy %in% c(77, 88, 99) ~ NA_real_, TRUE ~ happy) ) %&gt;% arrange(age) #Checking it head(d1) ## year cntry internet_use age education female happy ## 1 2020 BE NA 40 5 1 3 ## 2 2020 BE NA 40 6 0 10 ## 3 2020 BE NA 40 1 0 9 ## 4 2020 BE NA 40 5 0 6 ## 5 2020 BE NA 40 6 1 9 ## 6 2020 BE NA 40 6 0 4 When you print the dataset, you see that now there are some values named “NA”. That stands for not available. If we want to delete NAs, we can use the tidyverse way by simply piping to drop_na() The base R way would be to put the dataset name na.omit(). We assign both to a new data frame. We will see in the result that all rows are deleted which include NAs. This is one of the reasons it is important to cut down to variables we only need, so that only incomplete observations of our variables we need are deleted. So always remember, first transforming, than dropping. #Checking it head(d1) ## year cntry internet_use age education female happy ## 1 2020 BE NA 40 5 1 3 ## 2 2020 BE NA 40 6 0 10 ## 3 2020 BE NA 40 1 0 9 ## 4 2020 BE NA 40 5 0 6 ## 5 2020 BE NA 40 6 1 9 ## 6 2020 BE NA 40 6 0 4 #dropping NAs d2 &lt;- d1 %&gt;% drop_na() #Checking if there are NAs colSums(is.na(d2)) ## year cntry internet_use age ## 0 0 0 0 ## education female happy ## 0 0 0 #dropping NAs d3 &lt;- na.omit(d1) #Checking if there are NAs colSums(is.na(d2)) ## year cntry internet_use age ## 0 0 0 0 ## education female happy ## 0 0 0 We see that there are no missing values left in our dataset, and our number of observations are reduced. Now we have all ingredients to make our dataset. Do you remember our research question? We want to find out if people, who tend to not trust science are less willing to get vaccinated. We want to do that for all people over 40. 2.4.7 The group_by() and summarize() functions Two of the most useful commands in R for summary statistics are group_by() and summarize(). group_by() helps us to, when we have categorical variables with several observations and we want to calculate a metric e.g. for this group. The dataset we have loaded, the ESS for example asked 9000 respondents about their level of happiness. Image you are interested in the average level of happiness of men and women. Here the group_by() functions defines the groups we want to aggregate e.g. gender. 2.4.7.1 With one grouping variable and one metric summarize() defines the metric we want to search. For example we want to calculate the mean of the level of happiness of men and women. We also could just calculate the median for example. Let us have a look: First, we call the group_by() function and define the group we want to aggregate, thus the group we are interested in. In our example, we want to aggregate based on the sexes, therefore we need the to define that. Before that we have to delete NAs or transform the variable to the categories we are interested in, therefore we first call mutate() and transform the variable. Second, we call summarize() and define the name of the new column, let us call it average_happiness() and then we call the metric of the variable we are interested in. In our example, we were interested in the average happiness, so we have to call mean() and the happy variable: #group_by and summarize d1 &lt;- ess %&gt;% mutate( gndr_fac = as.factor(gndr), female = case_when( gndr_fac == 1 ~ &quot;Male&quot;, gndr_fac == 2 ~ &quot;Female&quot;, gndr_fac == 9 ~ NA_character_ )) %&gt;% drop_na() %&gt;% group_by(female) %&gt;% dplyr::summarize(average_happiness = mean(happy)) #Checking it head(d1) ## # A tibble: 2 × 2 ## female average_happiness ## &lt;chr&gt; &lt;dbl&gt; ## 1 Female 6.54 ## 2 Male 7.17 Et voilà, we get a dataset with two observations, because we have only two groups. The second row is the average_happiness row we defined in the summarize() function. 2.4.7.2 With more grouping variables and metrics The group_by() and summarize() functions are of course way more flexible, for example, we can define more groups. What about that, you are interested in the level of happiness of females, and males in the countries conducted by the ESS. You just put the countries and then the gender variable in the group_by(). Further we might be interested in more metrics, no problem, let us just define more columns with summarize(). Again, you have to first clean the data by transforming the class and deleting missing values. #grouping and summarize d1 &lt;- ess %&gt;% mutate( country = cntry, gndr_fac = as.factor(gndr), female = case_when( gndr_fac == 1 ~ &quot;Male&quot;, gndr_fac == 2 ~ &quot;Female&quot;, gndr_fac %in% c(77, 88, 99) ~ NA_character_), age = case_when( agea == 999 ~ NA_real_, TRUE ~ agea) ) %&gt;% drop_na() %&gt;% group_by(country, female) %&gt;% dplyr::summarize(average_happiness = mean(happy), median_happiness = median(happy), average_age = mean(age), meadian_age = median(age) ) ## `summarise()` has grouped output by &#39;country&#39;. You can ## override using the `.groups` argument. #Check it out head(d1) ## # A tibble: 6 × 6 ## # Groups: country [3] ## country female average_happiness median_happiness ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 BE Female 6.14 6 ## 2 BE Male 7.79 6 ## 3 BG Female 6.26 6 ## 4 BG Male 8.69 6 ## 5 CH Female 7.17 5 ## 6 CH Male 5.72 5.5 ## # ℹ 2 more variables: average_age &lt;dbl&gt;, meadian_age &lt;dbl&gt; 2.5 Merging Datasets 2.5.1 Introduction to merging with dplyr and preparing data Sometimes it could be the case that you need variables, which are not in one dataset a priori available, but in another dataset. For this case you load both datasets and merge them together. This only works if there is a similiar data structure, so know your data ! As an example, I will show how to do that with World Bank Data. From this data we can gather nearly all important economic indicators for countries since the 1970s. But mostly we need to merge them to datasets we are interested in. We will merge the World Bank Data with the ESS data. So we can analyze variables, which were not collected in the same dataset. There are several ways of getting World Bank Data, but I will show you the most efficient. There is the package WDI with which you can get data through an API (Application Programming Interface). Long story short, we do not need to download anything and get the data directly with code: First we define, which countries should be included: countries &lt;- c(&quot;BE&quot;, &quot;BG&quot;, &quot;CH&quot;, &quot;EE&quot;, &quot;FR&quot;,&quot;GB&quot;) Afterwards we define, which variables we want. You do that by using the official indicator, thus the variable you want. You can find the indicators on the website of the world bank data. Click on the variables you want, then click on the details, and there you find the indicator. I will use GDP per capita, Fuel exports, CO2 emissions (kt). indicators = c(&quot;NY.GDP.PCAP.CD&quot;, &quot;TX.VAL.FUEL.ZS.UN&quot;, &quot;EN.ATM.CO2E.KT&quot;) Now we are ready to use the API. To do so we call the WDI function. We define the argument country to only get countries we are interested in. We also define the indicators. Lastly, with the “start =” argument we define the starting year, so data which goes back to that date is loaded and with “end =” is analagos to define where the time should stop. Thus, both arguments define the time span we want to inspect #wb &lt;- WDI( # country = countries, #We include our countries # indicator = indicators, #We include our variables # start = 2020, #start date # end = 2020) #end date #This takes some time, especially if you have more countries, more indicators and a longer time span. #Checking it #head(wb) #Simulating the data wb &lt;- data.frame( iso2c = c(&quot;BE&quot;, &quot;BG&quot;, &quot;CH&quot;, &quot;EE&quot;, &quot;FR&quot;, &quot;GB&quot;), NY.GDP.PCAP.CD = c(45587.97, 10148.34, 85897.78, 23565.18, 39179.74, 40217.01), TX.VAL.FUEL.ZS.UN = c(5.021, 4.644, 0.6111, 4.863, 1.886, 7.062), EN.ATM.CO2E.KT = c(85364.10, 34138.10, 34916.10, 7097.52, 267154.70, 308650.30) ) #Checking it head(wb) ## iso2c NY.GDP.PCAP.CD TX.VAL.FUEL.ZS.UN EN.ATM.CO2E.KT ## 1 BE 45587.97 5.0210 85364.10 ## 2 BG 10148.34 4.6440 34138.10 ## 3 CH 85897.78 0.6111 34916.10 ## 4 EE 23565.18 4.8630 7097.52 ## 5 FR 39179.74 1.8860 267154.70 ## 6 GB 40217.01 7.0620 308650.30 Let us transform the dataset (Only variables we need, arranging it alphabetically, renaming it and rounding one variable to make the numbers more intuitive): #Cleaning the wb data wb &lt;- wb %&gt;% select(iso2c, NY.GDP.PCAP.CD, TX.VAL.FUEL.ZS.UN, EN.ATM.CO2E.KT) %&gt;% arrange(iso2c) %&gt;% rename(gdp_per_cap = NY.GDP.PCAP.CD, fuel_exp = TX.VAL.FUEL.ZS.UN, co2 = EN.ATM.CO2E.KT ) %&gt;% mutate(fuel_exp = round(fuel_exp, 2)) #Checking it head(wb) ## iso2c gdp_per_cap fuel_exp co2 ## 1 BE 45587.97 5.02 85364.10 ## 2 BG 10148.34 4.64 34138.10 ## 3 CH 85897.78 0.61 34916.10 ## 4 EE 23565.18 4.86 7097.52 ## 5 FR 39179.74 1.89 267154.70 ## 6 GB 40217.01 7.06 308650.30 Now, we cut down and prepare our ESS data by selecting the countries we are interested in, renaming the country variable (I’ll explain later why), we group by the country (iso2c) and the year (year) to get the average happiness by country. Lastly, we round the value to get only two decimals. #preparing ess d1 &lt;- ess %&gt;% filter(cntry == c(&quot;BE&quot;, &quot;BG&quot;, &quot;CZ&quot;, &quot;EE&quot;, &quot;FI&quot;)) %&gt;% rename(iso2c = cntry) %&gt;% group_by(iso2c, year) %&gt;% summarise(happy_agg = round(mean(happy), 2)) #Checking it head(d1) ## # A tibble: 5 × 3 ## # Groups: iso2c [5] ## iso2c year happy_agg ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 BE 2020 7.53 ## 2 BG 2020 6.41 ## 3 CZ 2020 7.08 ## 4 EE 2020 8.52 ## 5 FI 2020 7.51 2.5.2 left_join() and right_join() with one identifier To merge data there are important functions from the dplyr package: The left_join() and the right_join() function. Since they are a bit complicated to understand, we will go through them and in the end, you can choose the one you prefer. left_join(): You want to keep all observations in the first table, including matching observations in the second table. You merging the data from the right table to left table and get a datset with the same number of rows as the left table: right_join(): You want to keep all observations in the second table, including matching observations in the first table. You join from the left table to the right table, this time the table takes on the number of observations of the table which is right joined. Well, in the end of the day it is a matter of programming socialisation and taste, which one do you prefer. I will show you both. To merge two datasets, you need at least one common variable. One variable will be your unique identifier. Since every country is unique in the dataset that is our unique identifier i.e. the variable we give R to tell him how to merge the datasets: First, we define a new object, where we will save the dataset called merged_data. Second, we call left_join() Then we set our left table and our right table. We define the by = argument and put the unique identifier in quotation marks, meaning the variable name #left_join merged_data &lt;- left_join(d1, wb, by = &quot;iso2c&quot;) #Checking it head(merged_data) For right_join() you do the exact same, but remember you get a different result. #right_join merged_data2 &lt;- right_join(d1, wb, by = &quot;iso2c&quot;) #Checking it head(merged_data2) ## # A tibble: 6 × 6 ## # Groups: iso2c [6] ## iso2c year happy_agg gdp_per_cap fuel_exp co2 ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 BE 2020 7.53 45588. 5.02 85364. ## 2 BG 2020 6.41 10148. 4.64 34138. ## 3 EE 2020 8.52 23565. 4.86 7098. ## 4 CH NA NA 85898. 0.61 34916. ## 5 FR NA NA 39180. 1.89 267155. ## 6 GB NA NA 40217. 7.06 308650. 2.5.3 left_join() and right_join() with two identifiers Sometimes, you have multiple dimensions. For example, what if we also include the year? Then every country-year observation is our unique identifier. Why? Because one observation was collected in country X to time point Y. That is why you should always know your data and your research goal, because accordingly you have to write your code. Let us get again World Bank Data and clean it: #Getting the Data #wb &lt;- WDI( # country = c(&quot;BE&quot;, &quot;BG&quot;), #We include our countries # indicator = indicators, #We include our variables # start = 2019, #start date # end = 2020) #end date #Simulating the data wb &lt;- data.frame( iso3c = c(&quot;BEL&quot;, &quot;BEL&quot;, &quot;BGR&quot;, &quot;BGR&quot;), iso2c = c(&quot;BE&quot;, &quot;BE&quot;,&quot;BG&quot;, &quot;BG&quot;), year = c(2019, 2020, 2019, 2020), NY.GDP.PCAP.CD = c(46641.72, 45587.97, 9874.336, 10148.34), TX.VAL.FUEL.ZS.UN = c(7.38, 5.02, 9.53, 4.64), EN.ATM.CO2E.KT = c(92989.4, 85364.10, 39159.9, 34138.10) ) #Cleaning the Data wb &lt;- wb %&gt;% select(-iso3c) %&gt;% arrange(iso2c) %&gt;% rename(gdp_per_cap = NY.GDP.PCAP.CD, fuel_exp = TX.VAL.FUEL.ZS.UN, co2 = EN.ATM.CO2E.KT ) %&gt;% mutate(fuel_exp = round(fuel_exp, 2)) #Checking the Data head(wb) ## iso2c year gdp_per_cap fuel_exp co2 ## 1 BE 2019 46641.720 7.38 92989.4 ## 2 BE 2020 45587.970 5.02 85364.1 ## 3 BG 2019 9874.336 9.53 39159.9 ## 4 BG 2020 10148.340 4.64 34138.1 Now we just simulate some data we want to merge: #Getting the Data d1 &lt;- data.frame( iso2c = c(&quot;BE&quot;, &quot;BE&quot;, &quot;BG&quot;, &quot;BG&quot;, &quot;CZ&quot;, &quot;CZ&quot;), year = c(2019, 2020, 2019, 2020, 2019, 2020), happy_agg = c(5.95, 6.76, 6.56, 7.54, 6.27, 6.88) ) #Checking the Data head(d1) ## iso2c year happy_agg ## 1 BE 2019 5.95 ## 2 BE 2020 6.76 ## 3 BG 2019 6.56 ## 4 BG 2020 7.54 ## 5 CZ 2019 6.27 ## 6 CZ 2020 6.88 How does it look like when we have two variables and the combination out of those is our unique identifier? left_join() with two identifiers: As you can see the dataset again takes on the number of observations of our left table To implement it, we have to change by argument. We define a vector, where we put our two identifiers in quotation marks: #Merging the Data with left_join() merged_data3 &lt;- left_join(d1, wb, by = c(&quot;iso2c&quot;, &quot;year&quot;)) #Checking it head(merged_data3) ## iso2c year happy_agg gdp_per_cap fuel_exp co2 ## 1 BE 2019 5.95 46641.720 7.38 92989.4 ## 2 BE 2020 6.76 45587.970 5.02 85364.1 ## 3 BG 2019 6.56 9874.336 9.53 39159.9 ## 4 BG 2020 7.54 10148.340 4.64 34138.1 ## 5 CZ 2019 6.27 NA NA NA ## 6 CZ 2020 6.88 NA NA NA right_join() with two identifiers: Again we do the same with right_join(): merged_data4 &lt;- right_join(d1, wb, by = c(&quot;iso2c&quot;, &quot;year&quot;)) head(merged_data4) ## iso2c year happy_agg gdp_per_cap fuel_exp co2 ## 1 BE 2019 5.95 46641.720 7.38 92989.4 ## 2 BE 2020 6.76 45587.970 5.02 85364.1 ## 3 BG 2019 6.56 9874.336 9.53 39159.9 ## 4 BG 2020 7.54 10148.340 4.64 34138.1 Note: However, this chapter only touched the basics and for merging alone there are several further commands, like inner_join(), anti_join(), semi_join()…etc. But when you encounter problems with the two functions I showed you will run into them eventually. 2.6 Outlook This Chapter introduced you to the basic functions of the dplyr package. You are now able to transform variables according to your needs. Further, you learned how to use pipes to work efficient code. Loading data, transforming data, and preparing datasets for the analysis. There are a lot of techniques, and in the end of the day data wrangling is the most extensive part, because every analysis is individual and requires an individual preparation. The more individual the analysis, the more individual the preparation. I recommend the standard book for data science in R in this chapter, since it has a strong emphasis on data manipulation: “R for Data Science” by Hadley Wickham &amp; Garrett Grolemund. 2.7 Exercise Section 2.7.1 Exercise 1: Let’s wrangle kid You are interested in discrimination and the perception of the judicial. More specifically, you want to know if people, who fell discriminated evaluate courts differently. Below you see a table with all variables you want to include in your analysis: Variable Description Scales idnt Respondent’s identification number unique number from 1-9000 year The year when the survey was conducted only 2020 cntry Country BE, BG, CH, CZ, EE, FI, FR,GB, GR, HR, HU, IE, IS, IT, LT,NL, NO, PT, SI, SK agea Age of the Respondent, calculated Number of Age = 15-90 999 = Not available gndr Gender 1 = Male; 2 = Female; 9 = No answer happy How happy are you 0 (Extremly unhappy) - 10 (Extremly happy); 77 = Refusal; 88 = Don’t Know; 99 = No answer eisced Highest level of education, ES - ISCED 0 = Not possible to harmonise into ES-ISCED; 1 (ES-ISCED I , less than lower secondary) - 7 (ES-ISCED V2, higher tertiary education, =&gt; MA level; 55 = Other; 77 = Refusal; 88 = Don’t know; 99 = No answer netusoft Internet use, how often 1 (Never) - 5 (Every day); 7 = Refusal; 8 = Don’t know; 9 = No answer trstprl Most people can be trusted or you can’t be too careful 0 (You can’t be too careful) - 10 (Most people can be trusted); 77 = Refusal; 88 = Don’t Know; 99 = No answer lrscale Left-Right Placement 0 (Left) - 10 (Right); 77 = Refusal; 88 = Don’t know; 99 = No answer Wrangle the data, and assign it to an object called ess. Select the variables you need Filter for Austria, Belgium, Denmark, Georgia, Iceland and the Russian Federation Have a look at the codebook and code all irrelevant values as missing. If you have binary variables recode them from 1, 2 to 0 to 1 You want to build an extremism variable: You do so by subtracting 5 from the from the variable and squaring it afterwards. Call it extremism Rename the variables to more intuitive names, don’t forget to name binary varaibles after the category which is on 1 drop all missing values Check out your new dataset 2.7.2 Exercise 2: Merging Datasets The gapminder package in R loads automatically the gapminder dataset. The gapminder project is an independent educational non-profit fighting global misconceptions, check out their website: https://www.gapminder.org/ The gapminder dataset is already loaded. Get an overview of the gapminder dataset. There are different ways to do so, you can choose by yourself Load World Bank Data from 1972 to 2007 and load the variable “Exports and Goods (% of GDP)”. Merge the World Bank data to the gapminder data, so a dataset evolves with the number of observations of the gapminder data. d. Clean the data by dropping all missing values "],["data-visualisation.html", "Chapter 3 Data Visualisation 3.1 Introduction to ggplot2 3.2 Distributions: Histogram, Density Plots, and Boxplots 3.3 Ranking: Barplot 3.4 Evolution: Line Chart 3.5 Correlation: Scatterplots 3.6 Making Plots with facet_wrap() and facet_grid() 3.7 Outlook 3.8 Outlook 3.9 Exercise Section", " Chapter 3 Data Visualisation In this chapter, I will introduce you to the most fun part in R, data visualisation (at least in my opinion). I will introduce you to the basic graphs and how to plot them in R. To do so, I introduce you the framework of ggplot2. This is a quite famous and powerful package and it became the standard package of data visualization in R. The goal of this chapter is to show you the basic plots everyone should know about, how you can program nice plots to include them in your papers and reports, and how to work with the ggplot2 package. pacman::p_load(&quot;tidyverse&quot;, &quot;babynames&quot;, &quot;sf&quot;, &quot;ggridges&quot;, &quot;rnaturalearth&quot;, &quot;rnaturalearthdata&quot; ,&quot;forcats&quot; ,&quot;tmap&quot;) 3.1 Introduction to ggplot2 The tidyverse includes the most popular package for data visualization in R, ggplot2. With its relative straight forward code and its huge flexibility, and I mean HUGE FLEXIBILTY, it became the standard form of Data Visualization. It is aims to simplify data visualization by utilizing the “Grammar of Graphics” defined by Leland Wilkinson. While it may appear complicated at first, it just creates a frame and adds elements to it. Let us start by looking at the code structure and creating the frame. The central code here is ggplot(): ggplot() As we can see, we get an empty frame and in the following we will go through the standard forms of data visualizations by simply adding elements to this empty frame. But this is only the Peak of what is possible with Data Visualization in R. I strongly recommend to further work on this topic for two reasons, especially R is the perfect language to dive deeply in this topic. R is known for beautiful data visualizations and it is a reason for its popularity. 3.2 Distributions: Histogram, Density Plots, and Boxplots The first type of visualizations are displaying distributions. We should always get an overview of how our variables are distributed, because the distributions gives us valuable information about the data structure. For example a lot of statistical models assume certain distributions and to identify if we can test data with those models, we have to make sure that it does not violate the distribution assumption. Further, distributions make it easy to detect outliers or biases, since they are easy to spot with such visualizations. 3.2.1 Histograms 3.2.1.1 Basic Histogram Let us start with a normal Histogram. A histogram is an accurate graphical representation of the distribution of a numeric variable. It takes as input numeric variables only. The variable is cut into several bins, and the number of observation per bin is represented by the height of the bar. Before making our first plot, let us simulate some data: #Setting Seed for reproducibility set.seed(123) #Simulating data data1 &lt;- data.frame( type = c(rep(&quot;Variable 1&quot;, 1000)), value = c(rnorm(1000)) ) #Looking at the data glimpse(data1) ## Rows: 1,000 ## Columns: 2 ## $ type &lt;chr&gt; &quot;Variable 1&quot;, &quot;Variable 1&quot;, &quot;Variable 1&quot;, &quot;Var… ## $ value &lt;dbl&gt; -0.56047565, -0.23017749, 1.55870831, 0.070508… We now have a dataset for a random variable called “Variable 1” and this variable has 500 values assigned to it. We now want to know the distribution of these values and decide to plot a histogram. Now we have data and we can go straight to business. For a histogram in ggplot, we need the ggplot() command. Afterward, we include our dataset, in our case data. We use a comma in the ggplot() command after the data and add a new command, called aes(). In this command we need to define the x-axis and the y-axis. Here we just need the x-axis, since a histogram logically plots the “count” thus how often one value appears in the dataset, ggplot does that automatically. Last thing remaining is to close the bracket of aes() and of the ggplot() command and to tell ggplot, what kind of visualization we want. Our answer comes with a “+” after the closed command and we add the command geom_histogram(). ggplot(data1, aes(x = value)) + geom_histogram() ## `stat_bin()` using `bins = 30`. Pick better value with ## `binwidth`. And you just made your first histogram. But as you can see, it does not look nice. The reason is that we have to tell ggplot2 what we specifically want to change. And we can do so by defining the inside of the geom_histogram() function. I guess the first step is to make the bins visible and to change the color from gray to something nicer. We can do so by the defining the color for the borders of the bins, and the fill command to change the color of the bins in the geom_historgram() function. Let us set it to white to make it visible. Note: I could have defined any color, the only condition is to put it in quotation marks. Some colors such as white can be just written down, but you can always use any hexcode inside the quotation marks and it will work fine. ggplot(data1, aes(x = value)) + geom_histogram(color = &quot;white&quot;, fill = &quot;#69b3a2&quot;) ## `stat_bin()` using `bins = 30`. Pick better value with ## `binwidth`. Looks better! But still, we have to think about that we want to publish this in an article or report. And for this purpose it is not sufficient. Next we should change the names of the labs, we can do so by adding a plus + again after the geom_histogram() command and using the labs() function. In this function we define the name of our x-axis and the y-axis. While we are at it, we can define the title in this function as well. What I like to do next is to scale the x-axis and to have ggplot display the values of each of the horizontal grid lines. Here an important mechanic is needed. The code scale_x_continous() helps us to rescale the x-axis. In general, the family of scale_* functions are powerful, because re-scaling the axis can (must not necessarily) change the visualization, thus these are powerful tools we should be aware of: ggplot(data1, aes(x = value)) + geom_histogram(color = &quot;white&quot;, fill = &quot;#69b3a2&quot;) + labs( x = &quot;Value&quot;, y = &quot;Count&quot;, title = &quot;A Histogram&quot;) + scale_x_continuous(breaks = seq(-4, 4, 1), limits = c(-4, 4)) I do not know about you, but I have a huge problem with the gray grid as a background. This is the default grid by ggplot2 and we can change that. Again, we need a “+”, and then we can just add the function without any things in it. I decided for the theme_bw() function, which is my favorite theme, but I found a website, where you can have a look at the different themes, look here. ggplot(data1, aes(x = value)) + geom_histogram(color = &quot;white&quot;, fill = &quot;#69b3a2&quot;) + labs( x = &quot;Value&quot;, y = &quot;Count&quot;, title = &quot;A Histogram&quot;) + scale_x_continuous(breaks = seq(-4, 4, 1), limits = c(-4, 4)) + theme_minimal() ## `stat_bin()` using `bins = 30`. Pick better value with ## `binwidth`. Well, we did it. I think that this plot can be displayed in an article or report. Good job! One elemental thing I want to talk about is the width of the size. Currently, the binwidth is at 0.3. We can adjust that by including binwidth in the geom_histogram() command: #histogram bindwidth = 0.1 ggplot(data1, aes(x = value)) + geom_histogram(color = &quot;white&quot;, fill = &quot;#69b3a2&quot;, binwidth = 0.1) + labs( x = &quot;Value&quot;, y = &quot;Count&quot;, title = &quot;A Histogram with binwidth = 0.1&quot;) + scale_x_continuous(breaks = seq(-4, 4, 1), limits = c(-4, 4)) + theme_minimal() #histogram with bindwidth = 0.6 ggplot(data1, aes(x = value)) + geom_histogram(color = &quot;white&quot;, fill = &quot;#69b3a2&quot;, binwidth = 0.6) + labs( x = &quot;Value&quot;, y = &quot;Count&quot;, title = &quot;A Histogram with binwidth = 0.6&quot;) + scale_x_continuous(breaks = seq(-4, 4, 1), limits = c(-4, 4)) + theme_minimal() 3.2.1.2 Multiple Histograms In this part, I want to show you variations of the Histogram visualization plot. We will start with multiple distributions we probably want to display. To do so, we need a new variable we will call “Variable 2”, with its own observations and add it to our dataset: #Creating data data2 &lt;- data.frame( type = c(rep(&quot;Variable 2&quot;, 1000)), value = c(rnorm(1000, mean = 4)) ) #rowbinding it with data1 data2 &lt;- rbind(data1, data2) We have two variables, each with their own distribution. We have to tell ggplot2 to distinguish the numbers by the different variables. We do so by modifying the inside of the aes() function. Our x-axis stays the same, right? We still want the values to be on the x-axis, so that parts stays the same. We define the fill within the aes() command to tell ggplot to fill the values of the two variables. Additionally, I will specify position = “identity” in the plot, this specification helps to adjust the position, when two histograms are overlapping, which will be the case. Note: I leave out the `fill` specification for the reason that the colors are defined by default for both graphs (but we can change that, I will show that later). ggplot(data2, aes(x=value, fill=type)) + geom_histogram(color=&quot;#e9ecef&quot;, position = &quot;identity&quot;) + theme_bw() ## `stat_bin()` using `bins = 30`. Pick better value with ## `binwidth`. As you can see, we get two plots colored by the type there are assigned to. We can now play around a bit. I want to introduce you the alpha specification. This makes colors more transparent. Again this a command should be used if objects are overlapping to have a clearer picture of the overlap. Additionally, I will scale new colors, here the scale_* function family comes again into play. We will use the scale_fill_manual command, since we want to change the color of the fill specification in the aes() command: ggplot(data2, aes(x=value, fill=type)) + geom_histogram(color=&quot;#e9ecef&quot;, alpha = 0.6, position = &quot;identity&quot;) + scale_fill_manual(values = c(&quot;#8AA4D6&quot;, &quot;#E89149&quot;)) + theme_bw() ## `stat_bin()` using `bins = 30`. Pick better value with ## `binwidth`. 3.2.2 Density Plots A density plot is a representation of the distribution of a numeric variable. It uses a kernel density estimate to show the probability density function of the variable. It is basically a smoothed version of a histogram. Since the logic is the same, except that the geom_histogram() is changed with geom_density(). 3.2.2.1 Basic Density Plot Let us start with a basic density plot: ggplot(data1, aes(x = value)) + geom_density() Well, we now can do the exact same things as we did above: Fill the density plot with a color with fill(), make the fill color more transparent with alpha() and change the color of the line with color() in the geom_density() function. We can rescale the x-axis with scale_x_continous, and we can change the labels of the axis with labs(), and change the theme to theme_minimal(). ggplot(data1, aes(x = value, fill =)) + geom_density(color = &quot;white&quot;, fill = &quot;orange&quot;, alpha = 0.6) + labs( x = &quot;Value&quot;, y = &quot;Count&quot;, title = &quot;A Density Plot&quot;) + scale_x_continuous(breaks = seq(-4, 4, 1), limits = c(-4, 4)) + theme_minimal() 3.2.2.2 Multiple Density Plots We could also do this with multiple density plots, remember that we always need the data structure to plot a graph. For this reason we again need data3. The rest stays again the same as with histograms: Note: I just copied the code from above, changed the geom_histogram() to geom_density() and then I just changed the colors, the alpha and the theme. That’s it. And that is mostly how plotting works, just copy and paste from the internet, and adjust what you do not like. ggplot(data2, aes(x=value, fill=type)) + geom_density(color=&quot;#0a0a0a&quot;, alpha = 0.9, position = &quot;identity&quot;) + scale_fill_manual(values = c(&quot;#FDE725FF&quot;, &quot;#440154FF&quot;)) + theme_minimal() 3.2.3 Boxplots 3.2.3.1 Basic Boxplots The last visualization form of distributions are Boxplots. Boxplots are a really interesting form of showing distributions with a lot of information. Let us have a look at their anatomy, before I show you how to program them: Anatomy of a Boxplot The black rectangle represents the Interquartile Range (IQR), thus the difference between the 25th and 75th percentiles of the data The red line in the black rectangle represents the median of the data. The end of the lines show the value at the 0th percentile, respectively 100th percentile, thus the minimum and the maximum value of the IQR, not the data. The dots beyond the black lines are potential outliers and the points at the ends are the minimum value, respectively maximum value in the data. We should be aware of them, because if we ignore them, they could bias our statistical models, but more to that in Chapter 6. Let us implement a boxplot in R. Again the only thing that changes is that we use the standard ggplot() function and go on with the function geom_boxplot(): ggplot(data1, aes(x = value)) + geom_boxplot() We can also make that graph pretty with the same techniques as above: ggplot(data1, aes(x = value)) + geom_boxplot() + labs( x = &quot;Value&quot;, y = &quot;Count&quot;, title = &quot;A Boxplot&quot;) + scale_x_continuous(breaks = seq(-4, 4, 1), limits = c(-4, 4)) + theme_classic() 3.2.3.2 Multiple Boxplots A huge advantage of Boxplots are that it is an easy way to compare the structure of distributions of different groups. Consider following example: We want to compare the income of people with migration background and people without migration background. Let us say we collected a sample of people with 2000 respondents, 1000 with and 1000 without migration background. We further collected the incomes of each respondent. Be aware that we now need to define the y-axis with income. Since we do not look anymore at the count of the distribution, but the distribution over another variable (here:income). Let us look at the plot: # Set seed for reproducibility set.seed(123) # Simulate income data income_18_24 &lt;- rnorm(1000, mean = 40000, sd = 11000) income_25_34 &lt;- rnorm(1000, mean = 55000, sd = 17500) income_35_59 &lt;- rnorm(1000, mean = 70000, sd = 25000) # Combine into a data frame data3 &lt;- data.frame( income = c(income_18_24, income_25_34, income_35_59), age = factor(rep(c(&quot;18-24&quot;, &quot;25-34&quot;, &quot;35-59&quot;), each = 1000)) ) ggplot(data3, aes(x = age, y = income, fill = age)) + geom_boxplot() Before interpreting the plot, let us make it prettier: We change labels of the x-axis, y-axis and give the plot a title with the labs() function. I do not like the colors, we change them with the scale_fill_manual(). Again, we define alpha = 0.5 and also width = 0.5 of the boxes in geom_boxplot(). I also think, we do not need a legend, therefore we can remove it, and use the theme() function. This function is powerful, since its specification gives us a lot of possibilities to design the plot according to our wishes. We specify in the theme() function that legend.position = \"none\", which means that we do not want the legend to be displayed at all: # Create boxplot ggplot(data3, aes(x = age, y = income, fill = age)) + geom_boxplot(alpha = 0.5, width = 0.5) + scale_fill_manual(values = c(&quot;#acf6c8&quot;, &quot;#ecec53&quot; ,&quot;#D1BC8A&quot;)) + labs( title = &quot;Comparison of Income Distribution by Age&quot;, x = &quot;Age&quot;, y = &quot;Income&quot; ) + theme_minimal() + theme(legend.position = &quot;none&quot;) We have a lot of information here. First, we clearly see that the median of people with migration background is lower than the median income of people without migration background. But we further see, that the income distribution of respondents without migration background is more spread out over a higher range. We can see that by the longer lines of the boxplot of respondents without migration background. Also the IQR range of both variables are varying. The box of people without migration background is again smaller, which again is an indicator that respondents without migration background are more spread out. In comparison, we can see that respondents with migration background in the 50th -75th percentile earn as much as respondents without migration background in the 25th to 50th percentile. I could go on the whole day, boxplots are very informative and a nice tool to inspect and compare distribution structures. Note: I used simulated data, therefore this data is fictional. 3.3 Ranking: Barplot 3.3.1 Basic Barplot The most famous, and easiest way of showing values of different groups is the Barplot. A barplot (or barchart) is one of the most common types of graphic. It shows the relationship between a numeric and a categoric variable. Each entity of the categoric variable is represented as a bar. The size of the bar represents its numeric value. In ggplot, we only have to define the x-axis, and y-axis inside the ggplot() function, and add the function geom_bar(). Inside geom_bar() you have to add stat = “identity”, for the simple reason, that we have to tell ggplot2 to display the numbers of the column “strength”, otherwise it will give us an error. # Create data data4 &lt;- data.frame( name=c(&quot;King Kong&quot;,&quot;Godzilla&quot;,&quot;Superman&quot;, &quot;Odin&quot;,&quot;Darth Vader&quot;) , strength=c(10,15,45,61,22) ) #Plotting it ggplot(data4, aes(x = name, y = strength)) + geom_bar(stat = &quot;identity&quot;) Again, we can change the look of our plot. We start by changing the color by setting color within the geom_bar() function, we set a theme, let us do theme_test() this time and we change the names of the columns with the labs() function. Note: I can disable the name of the x-lab by simply adding empty quotation marks in the labs() function ggplot(data4, aes(x = name, y = strength)) + geom_bar(stat = &quot;identity&quot;, fill = &quot;#AE388B&quot;) + labs( x = &quot;&quot;, y = &quot;Strength&quot;, title = &quot;Strength of fictional Characters&quot; ) + theme_test() There is also another possibility to use Barplots. We could use them to count categories. Like we would in a histogram with the difference that we now have not a range of numbers, where we count how many numbers for one variable. We have a groups and want to count how often those groups appear in our dataset. Let us assume we asked 20 kids what their favorite fictional character is among Superman, King Kong and Godzilla. data5 &lt;- data.frame( hero = c(rep(&quot;Superman&quot;, 10), rep(&quot;King Kong&quot;, 3), rep(&quot;Godzilla&quot;, 7)), id = c(seq(1:20)), female = c(rep(&quot;Female&quot;, 7), rep(&quot;Male&quot;, 5), rep(&quot;Female&quot;, 1), rep(&quot;Female&quot;, 3), rep(&quot;Male&quot;, 4)) ) ggplot(data5, aes(x = hero)) + geom_bar(fill = &quot;#AE388B&quot;) + labs( x = &quot;&quot;, y = &quot;Count&quot;, title = &quot;What is your favourite fictional Character?&quot; ) + scale_y_continuous(breaks = seq(0,10,1)) + theme_test() We could also turn around both Barplots to have a vertical Barplot. That is quite easy, we just have to add the coord_flip() function. This function swaps the x-axis and the y-axis. Let us look at the plots: #Plot 1 ggplot(data4, aes(x = name, y = strength)) + geom_bar(stat = &quot;identity&quot;, fill = &quot;#AE388B&quot;) + labs( x = &quot;&quot;, y = &quot;Strength&quot;, title = &quot;Strength of fictional Characters&quot; ) + theme_test() + coord_flip() #Plot 2 ggplot(data5, aes(x = hero)) + geom_bar(fill = &quot;#AE388B&quot;) + labs( x = &quot;&quot;, y = &quot;Count&quot;, title = &quot;What is your favourite fictional Character?&quot; ) + scale_y_continuous(breaks = seq(0,10,1)) + theme_test() + coord_flip() 3.3.2 Reordering them To make a Barplot more intuitive, we can order it so the bar with the highest x-value is at the beginning and then it decreases or vice versa. To do so, we use the forcats package We take the code from above and wrap the x-value in the fct_reorder() command and determine the value it should be reorder based on, in our case the x-value is the name of the fictional characters and the value is the strength or the count: Note: You could also do it in descending order by just wrapping a desc() around the value the variable should be reorder based on thus it would look like this: fct_reorder(name, desc(strength)). #Plot 1 ggplot(data4, aes(x = fct_reorder(name, strength), y = strength)) + geom_bar(stat = &quot;identity&quot;, fill = &quot;#AE388B&quot;) + labs( x = &quot;&quot;, y = &quot;Strength&quot;, title = &quot;Strength of fictional Characters&quot; ) + theme_test() #Plot 2 ggplot(data4, aes(x = fct_reorder(name, strength), y = strength)) + geom_bar(stat = &quot;identity&quot;, fill = &quot;#AE388B&quot;) + labs( x = &quot;&quot;, y = &quot;Strength&quot;, title = &quot;Strength of fictional Characters&quot; ) + theme_test() + coord_flip() 3.3.3 Grouped and Stacked Barplots We can go a step further with barplots and group them. Let us assume we asked respondents to tell us how healthy they feel on a scale from 0-10. But we want to separate respondents older than 40 and younger than 40. And we again separate the group between female and male respondents. Therefore we look at the average answer of 4 groups: Female, older 40, Male, older 40, Female younger 40 and Male younger 40. To see if there are gender differences within these groups. Let us get the data: data6 &lt;- data.frame( female = c(&quot;Female&quot;, &quot;Male&quot;, &quot;Female&quot;, &quot;Male&quot;), age = c(&quot;Old&quot;, &quot;Old&quot;, &quot;Young&quot;, &quot;Young&quot;), value = c(5, 2, 8, 7) ) Now we got the data. We have to define 3 parameters within aes(). The x-axis is the age groups, the y-axis the average value, and we have to define fill = female, since this is our group we want to investigate within the age groups. Inside geom_bar(), we need two arguments stat = “identity” and position = dodge. Et voila we will get our first grouped barplot. ggplot(data6, aes(x = age, y = value, fill = female)) + geom_bar(position = &quot;dodge&quot;, stat=&quot;identity&quot;) We could have also used the a stacked barplot. The difference is, that we have one bar for our x-axis group, in our example the age group, and then the amount of the second group, the gender, is stacked on top of each it other. You could also see that as a normal barplot, where the bar is colored depending on the percentual distribution of the other group. In the code the only thing changing is that we set the position argument in the geom_bar() code to position = “stack”: ggplot(data6, aes(x = age, y = value, fill = female)) + geom_bar(position = &quot;stack&quot;, stat=&quot;identity&quot;) Let us make them pretty with our well-known techniques, it is always the same story. But twonew thing are introduced The argument width = 0.35 is included to the geom_bar() so we can determine the width of the bars I introduce you so-called color palettes. Instead of manually scaling the color, you can use built-in color palettes for different types of plots. For Barplot you can use the scale_fill_brewer, which includes different palettes and colors, which are automatically displayed. Have a look at the palettes of the command here. That can be really helpful, if you have a lot of groups, so you do not have to think about different colors, which look good together. #Plot 1 ggplot(data6, aes(x = age, y = value, fill = female)) + geom_bar(position = &quot;dodge&quot;, stat=&quot;identity&quot;, width = 0.35) + scale_fill_brewer(palette = &quot;Accent&quot;) + scale_y_continuous(breaks = seq(0, 15, 1)) + labs( x = &quot;Age Cohort&quot;, y = &quot;Average Score Well-Being&quot;, title = &quot;Impact of Age on Well-Being by Gender&quot; ) + theme_minimal() + theme(legend.title=element_blank()) #Plot 2 ggplot(data6, aes(x = age, y = value, fill = female)) + geom_bar(position = &quot;stack&quot;, stat=&quot;identity&quot;, width = 0.35) + scale_fill_brewer(palette = &quot;Accent&quot;) + scale_y_continuous(breaks = seq(0, 15, 2)) + labs( x = &quot;Age Cohort&quot;, y = &quot;Average Score Well-Being&quot;, title = &quot;Impact of Age on Well-Being by Gender&quot; ) + theme_minimal() + theme(legend.title=element_blank()) 3.4 Evolution: Line Chart A quite familiar plot is the line chart. A quite popular way of showing the evolution of a variable over a variable on the x-axis. We know them mostly from time series analyses, where a certain period is on the x-axis. Since such line charts with dates are well known, I will stick with them as an example. A line chart or line graph displays the evolution of one or several numeric variables. Data points are connected by straight line segments the measurement points are ordered (typically by their x-axis value) and joined with straight line segments. 3.4.1 Basic Line Plot In ggplot, we stick with the ggplot() function, define our x-axis and our y-axis. We add the function geom_line() to it. # Setting Seed set.seed(500) # create data date &lt;- 2000:2024 y &lt;- cumsum(rnorm(25)) y2 &lt;- cumsum(rnorm(25)) data7 &lt;- data.frame(date,y, y2) ggplot(data7, aes(x = date, y = y)) + geom_line() Normally we would go on and make the plot pretty. But there are additional aesthetics to a line plot. First, we can change the line type. The line type can be straight as in the default layout, but I will change set it in the geom_line() command to line type = \"dashed\". For an overview of all line types look here. Second, I change the size of the line with setting size = 1 in the geom_line() command. The rest of the aesthetics are stay the same, re-scaling axes, coloring, and themes. ggplot(data7, aes(x = date, y = y)) + geom_line(color = &quot;#0F52BA&quot;, linetype = &quot;dashed&quot;, linewidth = 1) + scale_y_continuous(breaks = seq(-1, 6, 1), limits = c(-1, 6)) + scale_x_continuous(breaks = seq(2000, 2024, 2)) + labs( y = &quot;&quot;, x = &quot;Year&quot;, title = &quot;A Line Plot&quot; ) + theme_bw() 3.4.2 Multiple Line Chart In the next step, we want to plot multiple lines in one plot. This is useful when we want to compare the evolution of variables for example over time. In ggplot2 we only need to add another layer with a plus and add another geom_line() command. But now things get a bit complicated: Inside the ggplot() command we only add our dataset with our dataset, nothing more. In the first geom_line() command we add the aes() function and define x and y. Until now, we only wrote the aes() function inside the ggplot() function, but now we have to write it in the geom_line() function, since we add another geom_line() layer. In the second geom_line() command we define the our next layer. This time the x-axis stays the same logically. But now we change y and set it to the second variable we want to inspect. ggplot(data7) + geom_line(aes(x = date, y = y)) + geom_line(aes(x = date, y = y2)) As always, we make the plot pretty in the next step. I will use the same code as above. But regarding the lines itself, we can separate the aesthetics separately: We can set the line type, color and size differently for each layer. We just have to specify it inside the geom_line() command for the respective layer. ggplot(data7) + geom_line(aes(x = date, y = y), linetype = &quot;twodash&quot;, size = 1, color = &quot;#365E32&quot;) + geom_line(aes(x = date, y = y2), linetype = &quot;longdash&quot;, size = 1, color = &quot;#FD9B63&quot;) + scale_y_continuous(breaks = seq(-5, 6, 1), limits = c(-5, 6)) + scale_x_continuous(breaks = seq(2000, 2024, 2)) + labs( y = &quot;&quot;, x = &quot;Year&quot;, title = &quot;A Line Plot&quot; ) + theme_bw() 3.4.3 Grouped Line Charts Another possibility of using line charts is to look at the evolution of groups separately. I introduce you to the babynames dataset, which is a package in R, which loads automatically the dataset about the most popular babynames in the US from 1880 until 2017. Let us have a look at it: ###Looking at the dataset head(babynames) ## # A tibble: 6 × 5 ## year sex name n prop ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; ## 1 1880 F Mary 7065 0.0724 ## 2 1880 F Anna 2604 0.0267 ## 3 1880 F Emma 2003 0.0205 ## 4 1880 F Elizabeth 1939 0.0199 ## 5 1880 F Minnie 1746 0.0179 ## 6 1880 F Margaret 1578 0.0162 Well, let us say we are interested in the popularity of the names Michael, Abby, and Lisa. Let us cut down the dataset to these three names with the filter() function you learned in the previous chapter: babynames_cut &lt;- babynames %&gt;% filter(name %in% c(&quot;Emma&quot;, &quot;Kimberly&quot;, &quot;Ruth&quot;)) %&gt;% filter(sex == &quot;F&quot;) In the next step, let us plot the popularity of these three names over time. We have to specify the x and y-axis and further add a geom_line() layer. So far, so normal. The next thing we do, is to tell ggplot2 that we want groups. We do so, in the ggplot() function by setting group = name. We should also set the colors = name, otherwise all lines will be black and we cannot distinguish, which line belongs to which group. ggplot(babynames_cut, aes(x = year, y = n, group = name, color = name)) + geom_line() Well, that looks good, we can see that Ruth had its peak in the 20s, Kimberly in the 60s and Emma is currently on the rise. Let us design the plot with a theme, remove the legend title, add some meaningful lab names and add a color palette with scale_color_brewer(). Regarding the labs, I will introduce you a way of re-naming the legend, by simply setting color = \"New Name\" in the labs() function ggplot(babynames_cut, aes(x = year, y = n, group = name, color = name)) + geom_line(size = 1) + scale_color_brewer(palette = &quot;Set1&quot;) + labs( x = &quot;Year&quot;, y = &quot;Number of Babies named&quot;, title = &quot;Popularity of Babynames over time&quot;, color = &quot;Name&quot; ) + theme_minimal() 3.5 Correlation: Scatterplots The last type of visualization are scatter plots. A Scatter plot displays the relationship between 2 numeric variables. Each dot represents an observation. Their position on the X (horizontal) and Y (vertical) axis represents the values of the 2 variables. It is a quite popular way in articles to investigate the relationship between two variables. 3.5.1 Basic Scatterplot We want to investigate the relationship between two variables. Let us assume we are the owner of a big choclate company. We want to find the out the relationship of our marketing spendings on the sales of our chocolate. We have the data for each quarter of the year and for years: # Set the seed for reproducibility set.seed(123) # Simulate data n &lt;- 100 marketing_budget &lt;- runif(n, min = 1000, max = 10000) sales &lt;- 2000 + 0.65 * marketing_budget + rnorm(n, mean = 1400, sd = 750) quarters &lt;- rep(c(&quot;Q1&quot;, &quot;Q2&quot;, &quot;Q3&quot;, &quot;Q4&quot;), 25) # Create a data frame data_point &lt;- data.frame(marketing_budget, sales, quarters) #Give it a name data_point$name &lt;- &quot;Chocolate Milk&quot; A scatter plot in R is made with the same logic as always. First, we define our x and y-axis in the ggplot() command. We add a comma and call the geom_point() function ggplot(data_point, aes(x = marketing_budget, y = sales)) + geom_point() Let us make the plot pretty and as always, we define a color for the dots in the layer, thus the geom_point() function, re-scale the axes (in this case I would just re-scale the x-axis), re-name the labels, give a title and define a theme. ggplot(data_point, aes(x = marketing_budget, y = sales)) + geom_point(color = &quot;#99582a&quot;) + scale_x_continuous(breaks = seq(0, 10000, 2000)) + labs( x = &quot;Marketing Budget&quot;, y = &quot;Sales per Unit&quot;, title = &quot;Chocolate Milk Sales and Marketing&quot; ) + theme_classic() 3.5.2 Scatter Plots with multiple Groups Let us go on with our example. We do not only have one sort of chocolate but two. Chocolate milk and dark chocolate. Let us get the data for dark chocolate as well: # Set the seed for reproducibility set.seed(123) # Simulate data n &lt;- 100 marketing_budget &lt;- runif(n, min = 1000, max = 10000) sales &lt;- 1500 + 0.3 * marketing_budget + rnorm(n, mean = 1400, sd = 750) quarters &lt;- rep(c(&quot;Q1&quot;, &quot;Q2&quot;, &quot;Q3&quot;, &quot;Q4&quot;), 25) #Making a df df_dark &lt;- data.frame(marketing_budget, sales, quarters) #Give it a name df_dark$name &lt;- &quot;Dark Chocolate&quot; #rowbind it with the other dataset data8 &lt;- rbind(data_point, df_dark) Now, we could run the same code as above, but we would not be able to distinguish, which dots belong to which chocolate. That is the reason we need to specify in the aes() function the argument color = name. That will color the dots in the group they belong to. I will manually give the colors, since I have to use brown colors for this example. ggplot(data8, aes(x = marketing_budget, y = sales, color = name)) + geom_point() + scale_color_manual(values = c(&quot;#e71d36&quot;, &quot;#260701&quot;))+ scale_x_continuous(breaks = seq(0, 10000, 2000)) + labs( x = &quot;Marketing Budget&quot;, y = &quot;Sales per Unit&quot;, title = &quot;Chocolate Milk Sales and Marketing&quot;, color = &quot;Product&quot; ) + theme_classic() As we can see, in general marketing leads to higher sales of chocolate. Further we can see that Marketing has a higher effect on Chocolate milk than on Dark Chocolate. Using colors is one way to differentiate between groups in scatter plots. Another way is to use different shapes. The only thing we have to change the color argument with a the shape argument. We can also adjust the size and I want to do that, since I want to make the forms more visible. Since this changes the design of the points, we have to set the argument size = 2.5 inside the geom_point() function. In the labs() function we change the argument color = “Product” to shape = “Product”, because we now name the legend of the shape layer, and not the color layer. Let us have a look: ggplot(data8, aes(x = marketing_budget, y = sales, shape = name)) + geom_point(size = 2.5) + scale_x_continuous(breaks = seq(0, 10000, 2000)) + labs( x = &quot;Marketing Budget&quot;, y = &quot;Sales per Unit&quot;, title = &quot;Chocolate Milk Sales and Marketing&quot;, shape = &quot;Product&quot; ) + theme_classic() There are different types of shapes and we can set them manually via numbers. For this purpose we can use the scale_shape_manual() and call the argument size = 4. There are different shapes and they have numbers assigned to them, to call them we have to set size equal to the number of the shape. Check out this website for an overview over the different shapes. We can also combine different colors with different shapes. We just leave the color = name argument in the ggplot() function. In the labs() function we will set the argument to color = \"\" and shape = \"\". So the legend shows the colored shape as the legend. ggplot(data8, aes(x = marketing_budget, y = sales, shape = name, color = name)) + geom_point(size = 2.5) + scale_color_manual(values = c(&quot;#e71d36&quot;, &quot;#260701&quot;)) + scale_x_continuous(breaks = seq(0, 10000, 2000)) + labs( x = &quot;Marketing Budget&quot;, y = &quot;Sales per Unit&quot;, title = &quot;Chocolate Milk Sales and Marketing&quot;, shape = &quot;&quot;, color = &quot;&quot; ) + theme_classic() 3.6 Making Plots with facet_wrap() and facet_grid() Sometimes we do not want to compare the elements in a plot (e.g. dots, lines), but the plot itself with other plots from the same dataset. This can be a powerful tool, in terms of telling a story with data. Further, we can gain several information by splitting the data into graphs and directly comparing them. 3.6.1 The facet_wrap() function That is rather abstract, let us stick with our chocolate company. We want to compare the effect of our marketing budget on sales for different quarters. We want to plot the same scatter plot as before, but this time for each quarter. We could of course split up the data set to each quarter and plot 4 plots. But that is not efficient. Let us copy the code from above for the basic plot, and just add the facet_wrap() function and inside this wave symbol ~ and add the variable we want separate for, in our case the quarters. #Basic facet_wrap() function ggplot(data8, aes(x = marketing_budget, y = sales)) + geom_point() + facet_wrap(~ quarters) As you can see ggplot2 plots 4 graphs for each quarter. Instead of plotting 4 graphs and writing unnecessary long code, we can use the handy facet_warp() function. If we want to make the graph pretty, it is quite easy, since it is identical as if we want to make a single plot pretty. Thus, we can just copy the code from above and include it: ggplot(data8, aes(x = marketing_budget, y = sales)) + geom_point(color = &quot;#99582a&quot;) + scale_x_continuous(breaks = seq(0, 10000, 2000)) + labs( x = &quot;Marketing Budget&quot;, y = &quot;Sales per Unit&quot;, title = &quot;Chocolate Milk Sales and Marketing&quot; ) + theme_classic() + facet_wrap(~ quarters) We can also add a facet_wrap() function for our plot with different shapes and colors for chocolate milk and dark chocolate: ggplot(data8, aes(x = marketing_budget, y = sales, shape = name, color = name)) + geom_point(size = 2.5) + scale_color_manual(values = c(&quot;#e71d36&quot;, &quot;#260701&quot;)) + scale_x_continuous(breaks = seq(0, 10000, 2000)) + labs( x = &quot;Marketing Budget&quot;, y = &quot;Sales per Unit&quot;, title = &quot;Chocolate Milk Sales and Marketing&quot;, shape = &quot;&quot;, color = &quot;&quot; ) + theme_classic() + facet_wrap(~ quarters) 3.6.2 The facet_grid() function The facet grid function does the same as the facet_wrap() function, but it allows to add a second dimension. Image we want to know the development of the temperature for the first four months of the years 2018, 2019, 2020 of the cities London, Paris and Berlin. This time, we decide for a line chart to visualize the evolution of the temperatures. Manually we would have to make nine plots, For each city one plot for each year. Or we just use the facet_grid() function: Since we have two dimensions, we have to define them. We define the row and then we define the column and separate them with this wave symbol ~ , thus facet_wrap(row ~ column) We use the geom_line() function and make the plot pretty by giving meaningful label names, coloring each year with a unique color, giving a title, defining a theme and hiding the legend, since it would only show that the years have unique colors. #Set seed for reproducilty set.seed(123) # Define the cities, years, and months cities &lt;- c(&quot;London&quot;, &quot;Paris&quot;, &quot;Berlin&quot;) years &lt;- 2018:2020 months &lt;- 1:4 # Only the first four months # Create a data frame with all combinations of City, Year, and Month data9 &lt;- expand.grid(City = cities, Year = years, Month = months) # Simulate temperature data with some variation depending on the city data9$Temperature &lt;- round(rnorm(nrow(data9), mean = 15, sd = 10), 1) + with(data9, ifelse(City == &quot;London&quot;, 0, ifelse(City == &quot;Paris&quot;, 5, -5))) # Check the first few rows of the dataset head(data9) ## City Year Month Temperature ## 1 London 2018 1 9.4 ## 2 Paris 2018 1 17.7 ## 3 Berlin 2018 1 25.6 ## 4 London 2019 1 15.7 ## 5 Paris 2019 1 21.3 ## 6 Berlin 2019 1 27.2 # Convert Month to a factor for better axis labeling data9$Month &lt;- factor(data9$Month, levels = 1:4, labels = month.abb[1:4]) # Basic ggplot object p &lt;- ggplot(data9, aes(x = Month, y = Temperature, group = Year, color = factor(Year))) + geom_line() + labs(title = &quot;Average Monthly Temperature (Jan-Apr, 2018-2020)&quot;, x = &quot;Month&quot;, y = &quot;Temperature (°C)&quot;, color = &quot;Year&quot;) + theme_bw() + theme(legend.position = &quot;none&quot;) + facet_grid(Year ~ City) #Printing it p 3.7 Outlook That was a brief introduction to data visualization in R and the basic visualization used in Data Analysis. The start of most visualizations are those basic plots and as you saw it is the same workflow. First, you have to built the basic plot, second you have to add the layers you want. And ggplot2 seems to be complicated at first, but since data visualization is a crucial task in Data Science and Research you will have get very fluent, very fast. I can only encourage you to go on and explore the world of data visualization in R with ggplot2. In this section, I want to give a glimpse of what is possible: 3.7.1 Combining different types of Graphs You can also combine different types of graphs. But be careful! Too much in one graph can be distracting. In the following, I will present a graph with two y-axis, one for a line chart with dots and one for a barplot. The x-axis presents the months of the year # Simulating example data data10 &lt;- data.frame( months = factor(1:12, levels = 1:12, labels = month.abb), avg_temp = c(0.6, 1.8, 4.6, 6.1, 10.4, 19, 18.3, 17.9, 15.2, 9.6, 4.7, 2.6), n_deaths = c(149, 155, 200, 218, 263, 282, 318, 301, 247, 250, 194, 205) ) # Scaling factor to align avg_temp with n_deaths scale_factor &lt;- max(data10$n_deaths) / max(data10$avg_temp) # Create the combined graph with dual y-axes ggplot(data10, aes(x = months)) + geom_bar(aes(y = n_deaths), stat = &quot;identity&quot;, fill = &quot;#FF8080&quot;, alpha = 0.6) + geom_line(aes(y = avg_temp * scale_factor, group = 1), color = &quot;#2c2c2c&quot;, linewidth = 1, linetype = &quot;dashed&quot;) + scale_y_continuous( name = &quot;Number of Traffic Deaths&quot;, sec.axis = sec_axis(~ . / scale_factor, name = &quot;Average Temperature (Celsius)&quot;) ) + labs(x = &quot;&quot;, title = &quot;Number of Traffic Deaths and Average Temperature per Month&quot;) + theme_bw() + theme( axis.title.y.left = element_text(color = &quot;#FF8080&quot;), axis.title.y.right = element_text(color = &quot;#2c2c2c&quot;) ) 3.7.2 Distributions: Ridgeline Chart and Violin Chart Two visualizations, which get more and more popular: The Ridgeline Chart and the Violin Chart. The violin chart displays a density plot horizontally. Moreover, it displays mirrors the density plot and puts it toegether: # Setting seed for reproducibility set.seed(123) # Simulate example sports data sports_data &lt;- data.frame( sport = factor(rep(c(&quot;Basketball&quot;, &quot;Soccer&quot;, &quot;Swimming&quot;, &quot;Gymnastics&quot;, &quot;Tennis&quot;), each = 100)), height = c( rnorm(100, mean = 200, sd = 10), # Basketball players are typically tall rnorm(100, mean = 175, sd = 7), # Soccer players have average height rnorm(100, mean = 180, sd = 8), # Swimmers rnorm(100, mean = 160, sd = 6), # Gymnasts are typically shorter rnorm(100, mean = 170, sd = 9) # Tennis players ) ) # Create the violin plot ggplot(sports_data, aes(x = sport, y = height, fill = sport)) + geom_violin(trim = FALSE) + labs( title = &quot;Distribution of Athletes&#39; Heights by Sport&quot;, x = &quot;Sport&quot;, y = &quot;Height (cm)&quot; ) + theme_bw() + theme( legend.position = &quot;none&quot;, plot.title = element_text(hjust = 0.5, size = 16, face = &quot;bold&quot;), axis.title.x = element_text(size = 14), axis.title.y = element_text(size = 14) ) + scale_fill_brewer(palette = &quot;RdBu&quot;) The Ridgeline chart is a nice way to compare more than 2 distributions. The idea is to plot the scale on the x-axis. On the y-axis the groups you want to compare are plotted: # Setting seed for reproducibility set.seed(123) # Normal distribution normal_data &lt;- rnorm(1000, mean = 50, sd = 10) # Left-skewed distribution (using exponential distribution) left_skewed_data &lt;- rexp(1000, rate = 0.1) # Right-skewed distribution (using log-normal distribution) right_skewed_data &lt;- rlnorm(1000, meanlog = 3, sdlog = 0.5) # Bimodal distribution (combining two normal distributions) bimodal_data &lt;- c(rnorm(500, mean = 35, sd = 5), rnorm(500, mean = 60, sd = 5)) # Combine the data into a data frame example_data &lt;- data.frame( value = c(normal_data, left_skewed_data, right_skewed_data, bimodal_data), distribution = factor(rep(c(&quot;Normal&quot;, &quot;Left-Skewed&quot;, &quot;Right-Skewed&quot;, &quot;Bimodal&quot;), each = 1000)) ) # Create the ridgeline chart ggplot(example_data, aes(x = value, y = distribution, fill = distribution)) + geom_density_ridges() + scale_fill_brewer(palette = &quot;Dark2&quot;) + labs( x = &quot;Values&quot;, y = &quot;Distribution&quot;, title = &quot;A Ridgeline Chart&quot; ) + theme_ridges() + theme(legend.position = &quot;none&quot;) ## Picking joint bandwidth of 2.34 3.7.3 Ranking: Lollipop Charts and Radar Charts 3.7.3.1 Lollipop Charts Lollipop Charts are getting more and more popular, so I want to show them to you. The idea is quite simple, it is a Bar Chart, instead a bar it uses a line and a dot: To implement it, we need to add a geom_point() layer in combination with a geom_segment() layer. We define the axis within ggplot() layer. Lastly, we have to define the aesthetics in the geom_segment() plot. ggplot(data4, aes(x=name, y=strength)) + geom_point() + geom_segment(aes(x=name, xend=name, y=0, yend=strength)) Let us make it pretty. We can give the line different colors and adjust it with the same methods as the line chart. The same goes for the dots we can adjust them as much as we like: ggplot(data4, aes(x=name, y=strength)) + geom_segment(aes(x=name, xend=name, y=0, yend=strength), color = &quot;grey&quot;) + geom_point(size = 4, color = &quot;#74B72E&quot;) + labs(x = &quot;Fictional Character&quot;, y = &quot;Strength&quot;, title = &quot;Strength of fictional Characters&quot;) + theme_light() + theme( panel.grid.major.x = element_blank(), panel.border = element_blank(), axis.ticks.x = element_blank() ) 3.7.4 Maps R also offers a variety of possibilities to work with spatial data. Of course, visualization of maps is an integral part, when working with spatial data. With R you can plot all sorts of maps: Interactive maps with leaflet, shape files of countries and multiple layers with the sf package and standard visualization tools such as connection maps or Cartograms. Here is an example of an interactive map filled with data. To keep the code as simple as possible I used the tmap package. It is a map of the world, which displays via its color, if a country is an high income, upper middle income, lower middle income or low income country: # Get country-level shapefiles world &lt;- ne_countries(scale = &quot;medium&quot;, returnclass = &quot;sf&quot;) world &lt;- world %&gt;% filter(gdp_year == 2019) %&gt;% mutate(`Income Group` = case_when( income_grp %in% c(&quot;1. High income: OECD&quot;, &quot;2. High income: nonOECD&quot;) ~ &quot;1. High Income&quot;, income_grp == &quot;3. Upper middle income&quot; ~ &quot;2. Upper Middle Income&quot;, income_grp == &quot;4. Lower middle income&quot; ~ &quot;3. Lower Middle Income&quot;, income_grp == &quot;5. Low income&quot; ~ &quot;4. Low Income&quot;) ) # Plot using tmap tmap_mode(&quot;view&quot;) ## tmap mode set to interactive viewing tm_shape(world) + tm_polygons(&quot;Income Group&quot;, title = &quot;Income Groups&quot;, palette = &quot;viridis&quot;, style = &quot;cat&quot;, id = &quot;sovereignt&quot;) 3.8 Outlook This chapter was an introduction to one of the most fun part of R, making plots. I introduced you to the standard forms of visualization and gave you a little primer to further visualizations and what is possible in R. The package ggplot2 is one of the most intuitive (although not for beginners) for data visualization. There is only one book I have to recommend regarding data visualization and that is the “R Gallery Book” by Kyle W. Brown. Also check out the website of this book, it is the standard website, where I search for code snippets for graphs, I can only recommend it. 3.9 Exercise Section In this exercise Section, we will work with the iris package. This is a classic built-in package in R, which contains data from the Ronald Fisher’s 1936 Study “The use of multiple measurements in taxonomic problems”. It contains three plant species and four measured features for each species. Let us get an overview of the package: summary(iris) ## Sepal.Length Sepal.Width Petal.Length ## Min. :4.300 Min. :2.000 Min. :1.000 ## 1st Qu.:5.100 1st Qu.:2.800 1st Qu.:1.600 ## Median :5.800 Median :3.000 Median :4.350 ## Mean :5.843 Mean :3.057 Mean :3.758 ## 3rd Qu.:6.400 3rd Qu.:3.300 3rd Qu.:5.100 ## Max. :7.900 Max. :4.400 Max. :6.900 ## Petal.Width Species ## Min. :0.100 setosa :50 ## 1st Qu.:0.300 versicolor:50 ## Median :1.300 virginica :50 ## Mean :1.199 ## 3rd Qu.:1.800 ## Max. :2.500 3.9.1 Exercise 1: Distributions a. Plot a Chart, which shows the distribution of Sepal.Length over the setosa Species. Choose the type of distribution chart for yourself. HINT: Prepare the data first and then plot it. b. Now I want you to add the two other Species to the Plot. Make Sure, that every Species has a unique color. c. Make a nice Plot! Give the Plot a meaningful title, meaningful labels for the x-axis and the y-axis and play around with the colors. d. Interpret the Plot! 3.9.2 Exercise 2: Rankings a. Calculate the average Petal.Length for every Species in a nice Barplot. HINT: You have to prepare the data again before you plot it b. Add the Means of the Petal.Width variable to the plot, so you get a nice grouped Barplot. c. Make a nice Plot! Give the Plot a meaningful title, meaningful labels for the x-axis and the y-axis and play around with the colors. d. Interpret the Plot! 3.9.3 Exercise 3: Correlation a. Make a scatter plot where you plot sepal.length on the x-axis and sepal.width on the y-axis. Make the plot for the species virginica b. Now I want you to add the species versicolor to the plot. The dots of this species should have a different color AND a different form. c. Make a nice plot! Add a theme, labels, and a nice title "],["exploratory-data-analysis-eda.html", "Chapter 4 Exploratory Data Analysis (EDA) 4.1 Standard Descriptive Statistics 4.2 Working with EDA packages 4.3 Conclusion 4.4 Exercise Section", " Chapter 4 Exploratory Data Analysis (EDA) In this chapter, we will start with data analysis. One crucial step when analyzing data is Exploratory Data Analysis. It describes the process of analyzing data sets to summarize their main characteristics. This step can help understanding the data, checking its quality, early detecting patterns and trends and gain first insights! Since R is a software specifically designed for statistics, we have a lot of high value libraries to perform EDA. The dataset we will use for this part is called palmerpenguins. This is a dataset about penguin species and their attributes. pacman::p_load(&quot;summarytools&quot;, &quot;SmartEDA&quot;, &quot;skimr&quot;, &quot;naniar&quot;, &quot;gtsummary&quot;, &quot;dlookr&quot;, &quot;DataExplorer&quot;, &quot;psych&quot;, &quot;ggplot2&quot;, &quot;palmerpenguins&quot;, &quot;dplyr&quot;, &quot;tidyr&quot;, &quot;corrplot&quot;) penguins &lt;- na.omit(penguins) penguins_raw &lt;- penguins_raw 4.1 Standard Descriptive Statistics 4.1.1 Measures of Central Tendency As the name suggests, measures of central tendency are helping us to understand the probability distribution of the data, its center and typical values. The three most common measures of central tendency are the arithmetic mean, the median and the mode. Mode: The most frequent number Mean: The sum of all values divided by the total number of values Median: The middle number in an ordered dataset 4.1.1.1 Mode The mode is probably the easiest measure out of all measures: It is defined as the most frequent number of all observations. We cannot directly calculate the mode, but there is a way to it. First we look at all unique values of or observations with the unique() function, then we count the occurrences of each unique value with tabulate(), and lastly we use the which.max() function to get the most frequent unique value: uniq_vals &lt;- unique(penguins$bill_length_mm) # Get unique values freqs &lt;- tabulate(match(penguins$bill_length_mm, uniq_vals)) # Count occurrences uniq_vals[which.max(freqs)] # Getting the unique value with the most occurrences ## [1] 41.1 4.1.1.2 Mean Let us start by looking at the formula to calculate a mean: \\[ \\bar{x} = \\frac{\\sum{x_i}}{n} \\] whereas: \\(\\bar{x}\\) is our mean \\(\\sum{x_i}\\) is the sum of all our observations. \\(n\\) is the number of all our observations We could do that by hand or we just use the built-in mean() function: mean(penguins$bill_length_mm) ## [1] 43.99279 4.1.1.3 Median Image sorting all your data from the lowest to highest and then pointing at the value, which has exactly 50% of all values to its left and the other 50% to its right, this would be the median value. Well, at least you will point at a value if your distribution has an yeven number of observations. But you can also calculate the value for an uneven number of observations, let us have a look at both formulas: \\(X_{(\\frac{n+1}{2})}\\) for an even number of n \\(\\frac{1}{2}X_{(\\frac{n}{2})} + X_{(\\frac{n}{2} + 1)}\\) for an uneven number of n Again we could calculate that by hand or we just use the median() function: median(penguins$bill_length_mm) ## [1] 44.5 4.1.2 Measures of Dispersion In statistics, measures of dispersion describe the extent to which a distribution of a variable is stretched or squeezed. In other words, they help to gauge the spread of our distributions. 4.1.2.1 Interquartile Range (IQR) You remember the boxplot from the data visualization chapter? It is supposed to show the so-called interquartile range (IQR). It is defined as the difference between the 75th percentile (or third quartile) and the 25th percentile (or the first quartile). Basically the distribution is spread into four equally big areas, which are separated by three points, the first quartile denoted by \\(Q_1\\) (also called the lower quartile), the second quartile is the median and denoted as \\(Q_2\\), and the third quartile is measures of dispersiondenoted by \\(Q_3\\) (also called the upper quartile), thus the formula is: \\[ IQR = Q_3 - Q_1 \\] In R, we can calculate the Interquartile Range (IQR) using the IQR() function. By hand, we would: Sort the data in ascending order. Split the data into four equal parts (quartiles). Identify Q1 (first quartile, 25th percentile) and Q3 (third quartile, 75th percentile). Note that in some cases the data cannot be divided into four even parts ddue to their size. In such cases, different statistical methods (e.g. Tukey’s Hinges) approximate quartiles in such cases. IQR(penguins$bill_length_mm) ## [1] 9.1 4.1.2.2 Variance In statistics, the variance is the expected value of the squared deviation from the mean of our random variable. The concept of the mean gets clear if we break down its formula: \\[ s² = \\frac{\\sum(x_i - \\bar{x})²}{n-1} \\] where the index i runs over the observations (Respondents, Countries,…), i = 1,…,n \\(x_i\\) are our observations \\(\\bar{x}\\) is the mean of our distribution \\(n\\) is the number of observations Especially the nominator of the formula \\(\\sum(x_i - \\bar{x})²\\) is quite interesting because it uses an interesting technique: Image our data is aligned on one dimension and somewhere in the middle there is our mean: Now, image we would calculate the differences and sum them up without squaring. You see that distance 1 would be -3 and distance 2 would be 4 an in sum that makes 0,8, but that is for sure not the distance between those two points. Here comes the squared part into action, every squared number is positive, this ensures that -3 becomes 9 and 4 becomes 16, thus the differences can be summed and result in 24. In R, we can implement this by simply calling the var() function: var(penguins$bill_length_mm) ## [1] 29.90633 Well we get 29.9 and the problem with variance is, that the squaring technique I showed you earlier leads to a problem: We cannot really interpret the data because it loses its unit, for example if our data is in meter, the variance would be in square meters. But we can solve this problem with the next measure: standard deviation. 4.1.2.3 Standard Deviation The standard deviation is the square root of the variance. It describes the amount of variation of the values of a variables about its mean. A low standard deviation indicate closeness of the values to the mean, and a high standard deviation indicates that the values are spread out over a wider range. The standard deviation is the square root of the variance: \\[ s = \\sqrt{s²} = \\sqrt\\frac{\\sum(x_i - \\bar{x})}{n-1} \\] Remember our example?: We calculated the variance the distance of the squared sums to be 24 (9 + 16). However, what happens if we take the square root of the squared values before adding them? Well, we get 3 for distance 1 and 4 for distance 2. For distance 2 nothing changed, it came back to its original value 4, but distance 1 has become positive from -3 to 3. Now we can add it together and get the distance 7. Let us apply it in R, with the function sd(): sd(penguins$bill_length_mm) ## [1] 5.468668 The huge advantage of the standard deviation in comparison to the variance is that it can be interpreted in the original unit and is thus a more intuitive measure of dispersion to work with. 4.1.3 Relationships between Variables It is necessary to understand one key difference in EDA. There are values you simply look at and interpret such as the measures shown before. But on the other hand there are measures which look at the relationship between variables, thus analyzing how they interact which each other. In the following, we will look at two methods to do so. 4.1.3.1 Crosstables / Contingency Tables table(penguins$species, penguins$island) ## ## Biscoe Dream Torgersen ## Adelie 44 55 47 ## Chinstrap 0 68 0 ## Gentoo 119 0 0 summarytools::ctable(penguins$species, penguins$island) ## Cross-Tabulation, Row Proportions ## species * island ## Data Frame: penguins ## ## ----------- -------- -------------- -------------- ------------ -------------- ## island Biscoe Dream Torgersen Total ## species ## Adelie 44 ( 30.1%) 55 ( 37.7%) 47 (32.2%) 146 (100.0%) ## Chinstrap 0 ( 0.0%) 68 (100.0%) 0 ( 0.0%) 68 (100.0%) ## Gentoo 119 (100.0%) 0 ( 0.0%) 0 ( 0.0%) 119 (100.0%) ## Total 163 ( 48.9%) 123 ( 36.9%) 47 (14.1%) 333 (100.0%) ## ----------- -------- -------------- -------------- ------------ -------------- gtsummary::tbl_cross(data = penguins, row = species, col = island) #qipzyayjvz table { font-family: system-ui, 'Segoe UI', Roboto, Helvetica, Arial, sans-serif, 'Apple Color Emoji', 'Segoe UI Emoji', 'Segoe UI Symbol', 'Noto Color Emoji'; -webkit-font-smoothing: antialiased; -moz-osx-font-smoothing: grayscale; } #qipzyayjvz thead, #qipzyayjvz tbody, #qipzyayjvz tfoot, #qipzyayjvz tr, #qipzyayjvz td, #qipzyayjvz th { border-style: none; } #qipzyayjvz p { margin: 0; padding: 0; } #qipzyayjvz .gt_table { display: table; border-collapse: collapse; line-height: normal; margin-left: auto; margin-right: auto; color: #333333; font-size: 16px; font-weight: normal; font-style: normal; background-color: #FFFFFF; width: auto; border-top-style: solid; border-top-width: 2px; border-top-color: #A8A8A8; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #A8A8A8; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; } #qipzyayjvz .gt_caption { padding-top: 4px; padding-bottom: 4px; } #qipzyayjvz .gt_title { color: #333333; font-size: 125%; font-weight: initial; padding-top: 4px; padding-bottom: 4px; padding-left: 5px; padding-right: 5px; border-bottom-color: #FFFFFF; border-bottom-width: 0; } #qipzyayjvz .gt_subtitle { color: #333333; font-size: 85%; font-weight: initial; padding-top: 3px; padding-bottom: 5px; padding-left: 5px; padding-right: 5px; border-top-color: #FFFFFF; border-top-width: 0; } #qipzyayjvz .gt_heading { background-color: #FFFFFF; text-align: center; border-bottom-color: #FFFFFF; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #qipzyayjvz .gt_bottom_border { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #qipzyayjvz .gt_col_headings { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #qipzyayjvz .gt_col_heading { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 6px; padding-left: 5px; padding-right: 5px; overflow-x: hidden; } #qipzyayjvz .gt_column_spanner_outer { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; padding-top: 0; padding-bottom: 0; padding-left: 4px; padding-right: 4px; } #qipzyayjvz .gt_column_spanner_outer:first-child { padding-left: 0; } #qipzyayjvz .gt_column_spanner_outer:last-child { padding-right: 0; } #qipzyayjvz .gt_column_spanner { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 5px; overflow-x: hidden; display: inline-block; width: 100%; } #qipzyayjvz .gt_spanner_row { border-bottom-style: hidden; } #qipzyayjvz .gt_group_heading { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; text-align: left; } #qipzyayjvz .gt_empty_group_heading { padding: 0.5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: middle; } #qipzyayjvz .gt_from_md > :first-child { margin-top: 0; } #qipzyayjvz .gt_from_md > :last-child { margin-bottom: 0; } #qipzyayjvz .gt_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; margin: 10px; border-top-style: solid; border-top-width: 1px; border-top-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; overflow-x: hidden; } #qipzyayjvz .gt_stub { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 5px; padding-right: 5px; } #qipzyayjvz .gt_stub_row_group { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 5px; padding-right: 5px; vertical-align: top; } #qipzyayjvz .gt_row_group_first td { border-top-width: 2px; } #qipzyayjvz .gt_row_group_first th { border-top-width: 2px; } #qipzyayjvz .gt_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #qipzyayjvz .gt_first_summary_row { border-top-style: solid; border-top-color: #D3D3D3; } #qipzyayjvz .gt_first_summary_row.thick { border-top-width: 2px; } #qipzyayjvz .gt_last_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #qipzyayjvz .gt_grand_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #qipzyayjvz .gt_first_grand_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-top-style: double; border-top-width: 6px; border-top-color: #D3D3D3; } #qipzyayjvz .gt_last_grand_summary_row_top { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-bottom-style: double; border-bottom-width: 6px; border-bottom-color: #D3D3D3; } #qipzyayjvz .gt_striped { background-color: rgba(128, 128, 128, 0.05); } #qipzyayjvz .gt_table_body { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #qipzyayjvz .gt_footnotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #qipzyayjvz .gt_footnote { margin: 0px; font-size: 90%; padding-top: 4px; padding-bottom: 4px; padding-left: 5px; padding-right: 5px; } #qipzyayjvz .gt_sourcenotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #qipzyayjvz .gt_sourcenote { font-size: 90%; padding-top: 4px; padding-bottom: 4px; padding-left: 5px; padding-right: 5px; } #qipzyayjvz .gt_left { text-align: left; } #qipzyayjvz .gt_center { text-align: center; } #qipzyayjvz .gt_right { text-align: right; font-variant-numeric: tabular-nums; } #qipzyayjvz .gt_font_normal { font-weight: normal; } #qipzyayjvz .gt_font_bold { font-weight: bold; } #qipzyayjvz .gt_font_italic { font-style: italic; } #qipzyayjvz .gt_super { font-size: 65%; } #qipzyayjvz .gt_footnote_marks { font-size: 75%; vertical-align: 0.4em; position: initial; } #qipzyayjvz .gt_asterisk { font-size: 100%; vertical-align: 0; } #qipzyayjvz .gt_indent_1 { text-indent: 5px; } #qipzyayjvz .gt_indent_2 { text-indent: 10px; } #qipzyayjvz .gt_indent_3 { text-indent: 15px; } #qipzyayjvz .gt_indent_4 { text-indent: 20px; } #qipzyayjvz .gt_indent_5 { text-indent: 25px; } #qipzyayjvz .katex-display { display: inline-flex !important; margin-bottom: 0.75em !important; } #qipzyayjvz div.Reactable > div.rt-table > div.rt-thead > div.rt-tr.rt-tr-group-header > div.rt-th-group:after { height: 0px !important; } island Total Biscoe Dream Torgersen species     Adelie 44 55 47 146     Chinstrap 0 68 0 68     Gentoo 119 0 0 119 Total 163 123 47 333 4.1.3.2 Correlation Correlation is an umbrella term for any statistical relationship, whether causal or not, between two random variables or bivariate data. The three main measures of correlations are named after their inventors: Pearsons Correlation (or simply Pearson’s r), Spearman’s Rank Correlation (or simply Spearman’s rho) and Kendall’s Tau. Before going on let us make clear what a linear relationship means. It means that if an observation increases or decreases the corresponding variables changes in a proportional and predictable way. 4.1.3.2.1 Pearsons Correlation Pearsons Correlation measures the linear relationship between two continous variables. To do so it assumes a normal distribution between two variables, a linear relationship and no major outliers. Normally, you should test those before running a correlation, but we skip that part and look at the implementation in R: cor(penguins$bill_length_mm, penguins$body_mass_g, method = &quot;pearson&quot;) ## [1] 0.5894511 The interpretation of pearson’s r is quite straightforward: The result always ranges between +1 and -1, where +1 means a perfect linear relationship, 0 means no relationship at all, -1 means a perfect linear relationship. In our case there is a strong positive, linear relationship of both variables with a r = 0.59. 4.1.3.2.2 Spearman’s Rank Correlation Spearman’s Rank Correlation, or simply Spearman’s Rho shows if ordinal or continuous variables have a Monotonic Relationship. This means that if one variables increases, the other always increases or decreases but not necessarily at a constant way, as it would be with a linear relationship. It has different assumptions like outliers are allowed, the relationship is non-linear, the data contains rank and the variables are not normally distributed. Let us have a look, how it is calculated in R: cor(penguins$bill_length_mm, penguins$body_mass_g, method = &quot;spearman&quot;) ## [1] 0.5764804 The interpretation is analog to Pearson’s R, +1 means a perfect positive, monotonic relationship, 0 means no monotonic relationship and -1 means perfect negative, monotonic relationship. A spearman’s rho of 0.58 indicates a strong, positive, monotonic relationship between the two variables. 4.1.3.2.3 Kendalls Tau Kendalls Tau measures the strength and direction of association between two ranked variables. We differentiate between two types of relationships: Concordant and Discordant relationships. A concordant relationship means both data points move in the same direction A discordant relationship if one data point increases, the other decreases (or vice versa). Let us calculate it: cor(penguins$bill_length_mm, penguins$body_mass_g, method = &quot;kendall&quot;) ## [1] 0.4277598 Kendalls Tau can be interpreted wiht +1 as a perfect rank agreement, 0 means no association at all, and -1 means a perfect rank reversal. Our kendall’s tau indicates a high rank agreement between both our variables. 4.1.3.3 Correlation Graphically The part before was quite theoretical, but there are also nice approaches to look at correlations graphically. we start with a simple scatterplot: ggplot(penguins, aes(x = bill_length_mm, y = body_mass_g)) + geom_point(color = &quot;#0077b6&quot;) + labs(x = &quot;Length in mm&quot;, y = &quot;Body Mass in g&quot;, title = &quot;Relationship between Length (in mm) and Body Mass (in g)&quot;) + theme_bw() When looking at relationships between two variables, scatterplots are the standard visualization to do so. As we can see, there is a clear trend that the length in mm could be positively related to the body mass in g. In the end of the day, there are three types of linear relationships and the scatterplots always look accordingly: set.seed(123) n &lt;- 100 df_cor &lt;- data.frame( x = rep(1:n, 3), relationship = rep(c(&quot;Positive&quot;, &quot;Negative&quot;, &quot;None&quot;), each = n), y = c( (1:n) + rnorm(n, sd = 15), # strong positive correlation (n:1) + rnorm(n, sd = 15), # strong negative correlation rnorm(n, mean = 50, sd = 20) # no correlation ) ) # Reorder factor levels df_cor$relationship &lt;- factor(df_cor$relationship, levels = c(&quot;Positive&quot;, &quot;None&quot;, &quot;Negative&quot;), labels = c(&quot;Positive&quot;, &quot;No Correlation&quot;, &quot;Negative&quot;)) # Plot ggplot(df_cor, aes(x = x, y = y)) + geom_point(color = &quot;steelblue&quot;, size = 2) + facet_wrap(~relationship, nrow = 1) + labs(title = &quot;Strong Positive, Negative, and No Correlation&quot;, x = &quot;X&quot;, y = &quot;Y&quot;) + theme_bw(base_size = 18) With scatterplots in combination with facet_wrap() you can show several correlations graphically, but there is a way to calculate the correlation coefficient and to show it graphically with a correlation plot. A correlation plot combines the logic of contingency tables, heat maps and the correlation coefficients. First a correlation matrix is created. A table that shows the pairwise correlation coefficients (typically Pearson) between several numerical variables. Each cell in the matrix represents the strength and direction of the linear relationship between two variables. A correlation plot is then a visual representation of this matrix, often using color gradients or circle sizes to show the strength and direction of correlations, making it easier to spot patterns. Let us compute it in R: We will use the corrplot package in R (There are other ways to compute it, which I will show later). First, we cut down our dataset to the variables you want to correlate with each other Second, compute a correlation matrix with the cor() command Third, call the corrplot variable, and take in the dataset, define the method (we will use the color to display the strength of the correlation), the type ('full' (default), 'upper' or 'lower', display full matrix, lower triangular or upper triangular matrix). # Step 1: Prepare numeric data penguins_numeric &lt;- penguins %&gt;% select(bill_length_mm, bill_depth_mm, flipper_length_mm, body_mass_g) %&gt;% drop_na() # Step 2: Compute correlation matrix corr_matrix &lt;- cor(penguins_numeric) # Step 3: Plot the correlation matrix corrplot(corr_matrix, method = &quot;color&quot;) Now, we can see in an elegant way the correlation between the four selected variables, which are displayed through the color. Every cell displays the correlation coefficient of the variable on its respective column and on its respective row. The corrplot() function allows for further aesthetics: corrplot(corr_matrix, method = &quot;color&quot;, type = &quot;upper&quot;, addCoef.col = &quot;black&quot;, tl.col = &quot;black&quot;, tl.srt = 45) And finally, we can change the method to circular. This changes the plot insofar that it does not fill the cells with the color of the correlation coefficient, but makes a circle around the numbers and fills it then with the color. The size of the circles, thus the radius is determined but the strength of the correlation, meaning that the closer the coefficient to zero the smaller the circle: corrplot(corr_matrix, method = &quot;circle&quot;, type = &quot;upper&quot;, addCoef.col = &quot;black&quot;, tl.col = &quot;black&quot;, tl.srt = 45) If I use the circular method, I like to display it without the numbers, looks better in my opinion, and is more intuitive then filling the whole row: corrplot(corr_matrix, method = &quot;circle&quot;, type = &quot;upper&quot;, tl.col = &quot;black&quot;, tl.srt = 45) 4.2 Working with EDA packages There are several other measures for EDA, and of course you do not have to calculate every single measure by hand, although you could. In the following, I will introduce you to the most popular packages for EDA. From my point of view, they are all basically the same, with some nuances and when talking to other R-users I noticed that everyone somehow established his or her own routine of EDA. Therefore I suggest that you get an overview of all those packages and find your own “EDA-Routine” so to speak. 4.2.1 psych The psych package is a crucial package for psychologists and I love to use its “describe()” function, which shows a bunch of descriptive statistics with one line of code: #Applying describe to the whole dataset psych::describe(penguins) ## vars n mean sd median trimmed ## species* 1 333 1.92 0.89 2.0 1.90 ## island* 2 333 1.65 0.71 2.0 1.57 ## bill_length_mm 3 333 43.99 5.47 44.5 43.98 ## bill_depth_mm 4 333 17.16 1.97 17.3 17.19 ## flipper_length_mm 5 333 200.97 14.02 197.0 200.36 ## body_mass_g 6 333 4207.06 805.22 4050.0 4159.46 ## sex* 7 333 1.50 0.50 2.0 1.51 ## year 8 333 2008.04 0.81 2008.0 2008.05 ## mad min max range skew kurtosis ## species* 1.48 1.0 3.0 2.0 0.16 -1.72 ## island* 1.48 1.0 3.0 2.0 0.62 -0.85 ## bill_length_mm 6.97 32.1 59.6 27.5 0.04 -0.90 ## bill_depth_mm 2.22 13.1 21.5 8.4 -0.15 -0.91 ## flipper_length_mm 16.31 172.0 231.0 59.0 0.36 -0.98 ## body_mass_g 889.56 2700.0 6300.0 3600.0 0.47 -0.75 ## sex* 0.00 1.0 2.0 1.0 -0.02 -2.01 ## year 1.48 2007.0 2009.0 2.0 -0.08 -1.49 ## se ## species* 0.05 ## island* 0.04 ## bill_length_mm 0.30 ## bill_depth_mm 0.11 ## flipper_length_mm 0.77 ## body_mass_g 44.13 ## sex* 0.03 ## year 0.04 #You can apply describe() also for single variables #psych::describe(penguins$bill_length_mm) The describe() function output is a summary table of the basic summary statistics as mean, standard deviation, median, and a lot more and it is a shortcut, so you do not have to calculate every measure on its own. corr.test(penguins_numeric) ## Call:corr.test(x = penguins_numeric) ## Correlation matrix ## bill_length_mm bill_depth_mm ## bill_length_mm 1.00 -0.23 ## bill_depth_mm -0.23 1.00 ## flipper_length_mm 0.65 -0.58 ## body_mass_g 0.59 -0.47 ## flipper_length_mm body_mass_g ## bill_length_mm 0.65 0.59 ## bill_depth_mm -0.58 -0.47 ## flipper_length_mm 1.00 0.87 ## body_mass_g 0.87 1.00 ## Sample Size ## [1] 333 ## Probability values (Entries above the diagonal are adjusted for multiple tests.) ## bill_length_mm bill_depth_mm ## bill_length_mm 0 0 ## bill_depth_mm 0 0 ## flipper_length_mm 0 0 ## body_mass_g 0 0 ## flipper_length_mm body_mass_g ## bill_length_mm 0 0 ## bill_depth_mm 0 0 ## flipper_length_mm 0 0 ## body_mass_g 0 0 ## ## To see confidence intervals of the correlations, print with the short=FALSE option In the next step, I want to show you the pairs.panel() function. It displays a really interesting visualization regarding correlations between the variables: pairs.panels(penguins_numeric) So this is an interesting grid consisting of different types of data visualization, all related to correlations. Let us start with the easiest one, the diagonal. The diagonal shows us a histogram of the distributions of our input variables, additionally it includes a line, so we can check if the distributions look right. The upper right visualizations display the correlation coefficients of the respective two variables, like a correlation matrix. The bottom left visualizations are scatterplots between the two variables, where a line is fitted between the data points. In addition, the shape of the red line (a loess smoother) can reveal whether the relationship between the two variables is linear or more complex, such as curved or s-shaped. The scatterplots also include correlation ellipses, which visually represent the strength and direction of the relationship: narrow, tilted ellipses indicate strong correlations, while rounder shapes indicate weaker or no correlations. Together, this grid gives a comprehensive overview of both the distributions of individual variables and the pairwise relationships between them. 4.2.2 skimr The skimr package is a wonderful way to get an overview of our datasets structure and basic statistics, even with a visualization of the distribution of the variable. The main function is skim(): skimr::skim(penguins) Table 4.1: Data summary Name penguins Number of rows 333 Number of columns 8 _______________________ Column type frequency: factor 3 numeric 5 ________________________ Group variables None Variable type: factor skim_variable n_missing complete_rate ordered n_unique top_counts species 0 1 FALSE 3 Ade: 146, Gen: 119, Chi: 68 island 0 1 FALSE 3 Bis: 163, Dre: 123, Tor: 47 sex 0 1 FALSE 2 mal: 168, fem: 165 Variable type: numeric skim_variable n_missing complete_rate mean sd p0 p25 p50 p75 p100 hist bill_length_mm 0 1 43.99 5.47 32.1 39.5 44.5 48.6 59.6 ▃▇▇▆▁ bill_depth_mm 0 1 17.16 1.97 13.1 15.6 17.3 18.7 21.5 ▅▆▇▇▂ flipper_length_mm 0 1 200.97 14.02 172.0 190.0 197.0 213.0 231.0 ▂▇▃▅▃ body_mass_g 0 1 4207.06 805.22 2700.0 3550.0 4050.0 4775.0 6300.0 ▃▇▅▃▂ year 0 1 2008.04 0.81 2007.0 2007.0 2008.0 2009.0 2009.0 ▇▁▇▁▇ #If you do not want to see the distribution #skimr::skim_without_charts(penguins) The advantage of the skimr package is that it directly calculates descriptive statistics and shows the missing values for every variable in your dataset. I often use this command to have a look at datasets I am not familiar with. I think it is more a function for exploring the dataset rather than EDA it can be a useful first step for conducting EDA. 4.2.3 summarytools We will dive into the summarytools package with the classic dfSummary command which summarizes the structure of our dataset: dfSummary(penguins) ## Data Frame Summary ## penguins ## Dimensions: 333 x 8 ## Duplicates: 0 ## ## ---------------------------------------------------------------------------------------------------------------------- ## No Variable Stats / Values Freqs (% of Valid) Graph Valid Missing ## ---- ------------------- ---------------------------- --------------------- --------------------- ---------- --------- ## 1 species 1. Adelie 146 (43.8%) IIIIIIII 333 0 ## [factor] 2. Chinstrap 68 (20.4%) IIII (100.0%) (0.0%) ## 3. Gentoo 119 (35.7%) IIIIIII ## ## 2 island 1. Biscoe 163 (48.9%) IIIIIIIII 333 0 ## [factor] 2. Dream 123 (36.9%) IIIIIII (100.0%) (0.0%) ## 3. Torgersen 47 (14.1%) II ## ## 3 bill_length_mm Mean (sd) : 44 (5.5) 163 distinct values . . : 333 0 ## [numeric] min &lt; med &lt; max: . : : : : : (100.0%) (0.0%) ## 32.1 &lt; 44.5 &lt; 59.6 : : : : : : ## IQR (CV) : 9.1 (0.1) : : : : : : . ## . : : : : : : : . ## ## 4 bill_depth_mm Mean (sd) : 17.2 (2) 79 distinct values : 333 0 ## [numeric] min &lt; med &lt; max: : : (100.0%) (0.0%) ## 13.1 &lt; 17.3 &lt; 21.5 : . : : : . ## IQR (CV) : 3.1 (0.1) . : : : : : : ## : : : : : : : . . ## ## 5 flipper_length_mm Mean (sd) : 201 (14) 54 distinct values : 333 0 ## [integer] min &lt; med &lt; max: . : (100.0%) (0.0%) ## 172 &lt; 197 &lt; 231 : : : . . ## IQR (CV) : 23 (0.1) . : : : : : : ## : : : : : : : : : ## ## 6 body_mass_g Mean (sd) : 4207.1 (805.2) 93 distinct values : 333 0 ## [integer] min &lt; med &lt; max: . : (100.0%) (0.0%) ## 2700 &lt; 4050 &lt; 6300 : : : : ## IQR (CV) : 1225 (0.2) : : : : : . ## . : : : : : : ## ## 7 sex 1. female 165 (49.5%) IIIIIIIII 333 0 ## [factor] 2. male 168 (50.5%) IIIIIIIIII (100.0%) (0.0%) ## ## 8 year Mean (sd) : 2008 (0.8) 2007 : 103 (30.9%) IIIIII 333 0 ## [integer] min &lt; med &lt; max: 2008 : 113 (33.9%) IIIIII (100.0%) (0.0%) ## 2007 &lt; 2008 &lt; 2009 2009 : 117 (35.1%) IIIIIII ## IQR (CV) : 2 (0) ## ---------------------------------------------------------------------------------------------------------------------- The huge advantage of this command is that if we wrap it around the dfSummary function and then it gives us a formatted in a nice table: view(dfSummary(penguins)) ## Output file written: /tmp/RtmpnHp0t6/file2df97173c4c0.html # Or if you are a good R user, then you can use also a pipe # dfSummary(penguins) %&gt;% # view() The package also includes nice ways of showing frequency tables for single variables with more information than the standard table() command: freq(penguins$species) ## Frequencies ## penguins$species ## Type: Factor ## ## Freq % Valid % Valid Cum. % Total % Total Cum. ## --------------- ------ --------- -------------- --------- -------------- ## Adelie 146 43.84 43.84 43.84 43.84 ## Chinstrap 68 20.42 64.26 20.42 64.26 ## Gentoo 119 35.74 100.00 35.74 100.00 ## &lt;NA&gt; 0 0.00 100.00 ## Total 333 100.00 100.00 100.00 100.00 We can also use the descr() function rom the package which shows us the most common descriptive statistics of our variables in our dataset: descr(penguins) ## Non-numerical variable(s) ignored: species, island, sex ## Descriptive Statistics ## penguins ## N: 333 ## ## bill_depth_mm bill_length_mm body_mass_g flipper_length_mm year ## ----------------- --------------- ---------------- ------------- ------------------- --------- ## Mean 17.16 43.99 4207.06 200.97 2008.04 ## Std.Dev 1.97 5.47 805.22 14.02 0.81 ## Min 13.10 32.10 2700.00 172.00 2007.00 ## Q1 15.60 39.50 3550.00 190.00 2007.00 ## Median 17.30 44.50 4050.00 197.00 2008.00 ## Q3 18.70 48.60 4775.00 213.00 2009.00 ## Max 21.50 59.60 6300.00 231.00 2009.00 ## MAD 2.22 6.97 889.56 16.31 1.48 ## IQR 3.10 9.10 1225.00 23.00 2.00 ## CV 0.11 0.12 0.19 0.07 0.00 ## Skewness -0.15 0.04 0.47 0.36 -0.08 ## SE.Skewness 0.13 0.13 0.13 0.13 0.13 ## Kurtosis -0.91 -0.90 -0.75 -0.98 -1.49 ## N.Valid 333.00 333.00 333.00 333.00 333.00 ## N 333.00 333.00 333.00 333.00 333.00 ## Pct.Valid 100.00 100.00 100.00 100.00 100.00 Lastly, I want to show you how you can make cross tables with the summarytools package (you already saw it above): ctable(penguins$species, penguins$island) ## Cross-Tabulation, Row Proportions ## species * island ## Data Frame: penguins ## ## ----------- -------- -------------- -------------- ------------ -------------- ## island Biscoe Dream Torgersen Total ## species ## Adelie 44 ( 30.1%) 55 ( 37.7%) 47 (32.2%) 146 (100.0%) ## Chinstrap 0 ( 0.0%) 68 (100.0%) 0 ( 0.0%) 68 (100.0%) ## Gentoo 119 (100.0%) 0 ( 0.0%) 0 ( 0.0%) 119 (100.0%) ## Total 163 ( 48.9%) 123 ( 36.9%) 47 (14.1%) 333 (100.0%) ## ----------- -------- -------------- -------------- ------------ -------------- 4.2.4 naniar Naniar is one of the most powerful packages for working with missing data. At first glance, dealing with missing values may seem straightforward — as covered in the “Data Manipulation” chapter, it’s common to simply remove rows with missing values using functions like na.omit() or drop_na(). However, as you progress in data analysis, handling missing data becomes much more important and nuanced. Here’s why: Dropping missing values can lead to a small sample size (n), reducing statistical power. If a large portion of data is missing, removing it may introduce bias, especially if the missingness is not random. In such situations, advanced techniques like multiple imputation become valuable. These methods estimate missing values using mathematical models that consider patterns in the data. But there’s a catch: These models have assumptions (e.g., data are Missing at Random — MAR). Violating these assumptions can lead to misleading results. That’s why it’s crucial to explore and understand the structure of missingness before choosing a strategy. Let us start with the basic miss_var_summary() function, it shows the number of missing values, and calculates the percentages of missing values: naniar::miss_var_summary(penguins_raw) ## # A tibble: 17 × 3 ## variable n_miss pct_miss ## &lt;chr&gt; &lt;int&gt; &lt;num&gt; ## 1 Comments 290 84.3 ## 2 Delta 15 N (o/oo) 14 4.07 ## 3 Delta 13 C (o/oo) 13 3.78 ## 4 Sex 11 3.20 ## 5 Culmen Length (mm) 2 0.581 ## 6 Culmen Depth (mm) 2 0.581 ## 7 Flipper Length (mm) 2 0.581 ## 8 Body Mass (g) 2 0.581 ## 9 studyName 0 0 ## 10 Sample Number 0 0 ## 11 Species 0 0 ## 12 Region 0 0 ## 13 Island 0 0 ## 14 Stage 0 0 ## 15 Individual ID 0 0 ## 16 Clutch Completion 0 0 ## 17 Date Egg 0 0 The function that made naniar famous is the gg_miss_upset() function, which shows us the structure of the missing values graphically: naniar::gg_miss_upset(penguins_raw) naniar::vis_miss(penguins_raw) 4.2.5 gtsummary The gtsummary package is the package for data reporting, because it automatically creates data tables ready for publication with one line of code. Let us start with the tbl_summary() function: gtsummary::tbl_summary(penguins) #chnioqghai table { font-family: system-ui, 'Segoe UI', Roboto, Helvetica, Arial, sans-serif, 'Apple Color Emoji', 'Segoe UI Emoji', 'Segoe UI Symbol', 'Noto Color Emoji'; -webkit-font-smoothing: antialiased; -moz-osx-font-smoothing: grayscale; } #chnioqghai thead, #chnioqghai tbody, #chnioqghai tfoot, #chnioqghai tr, #chnioqghai td, #chnioqghai th { border-style: none; } #chnioqghai p { margin: 0; padding: 0; } #chnioqghai .gt_table { display: table; border-collapse: collapse; line-height: normal; margin-left: auto; margin-right: auto; color: #333333; font-size: 16px; font-weight: normal; font-style: normal; background-color: #FFFFFF; width: auto; border-top-style: solid; border-top-width: 2px; border-top-color: #A8A8A8; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #A8A8A8; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; } #chnioqghai .gt_caption { padding-top: 4px; padding-bottom: 4px; } #chnioqghai .gt_title { color: #333333; font-size: 125%; font-weight: initial; padding-top: 4px; padding-bottom: 4px; padding-left: 5px; padding-right: 5px; border-bottom-color: #FFFFFF; border-bottom-width: 0; } #chnioqghai .gt_subtitle { color: #333333; font-size: 85%; font-weight: initial; padding-top: 3px; padding-bottom: 5px; padding-left: 5px; padding-right: 5px; border-top-color: #FFFFFF; border-top-width: 0; } #chnioqghai .gt_heading { background-color: #FFFFFF; text-align: center; border-bottom-color: #FFFFFF; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #chnioqghai .gt_bottom_border { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #chnioqghai .gt_col_headings { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #chnioqghai .gt_col_heading { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 6px; padding-left: 5px; padding-right: 5px; overflow-x: hidden; } #chnioqghai .gt_column_spanner_outer { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; padding-top: 0; padding-bottom: 0; padding-left: 4px; padding-right: 4px; } #chnioqghai .gt_column_spanner_outer:first-child { padding-left: 0; } #chnioqghai .gt_column_spanner_outer:last-child { padding-right: 0; } #chnioqghai .gt_column_spanner { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 5px; overflow-x: hidden; display: inline-block; width: 100%; } #chnioqghai .gt_spanner_row { border-bottom-style: hidden; } #chnioqghai .gt_group_heading { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; text-align: left; } #chnioqghai .gt_empty_group_heading { padding: 0.5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: middle; } #chnioqghai .gt_from_md > :first-child { margin-top: 0; } #chnioqghai .gt_from_md > :last-child { margin-bottom: 0; } #chnioqghai .gt_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; margin: 10px; border-top-style: solid; border-top-width: 1px; border-top-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; overflow-x: hidden; } #chnioqghai .gt_stub { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 5px; padding-right: 5px; } #chnioqghai .gt_stub_row_group { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 5px; padding-right: 5px; vertical-align: top; } #chnioqghai .gt_row_group_first td { border-top-width: 2px; } #chnioqghai .gt_row_group_first th { border-top-width: 2px; } #chnioqghai .gt_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #chnioqghai .gt_first_summary_row { border-top-style: solid; border-top-color: #D3D3D3; } #chnioqghai .gt_first_summary_row.thick { border-top-width: 2px; } #chnioqghai .gt_last_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #chnioqghai .gt_grand_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #chnioqghai .gt_first_grand_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-top-style: double; border-top-width: 6px; border-top-color: #D3D3D3; } #chnioqghai .gt_last_grand_summary_row_top { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-bottom-style: double; border-bottom-width: 6px; border-bottom-color: #D3D3D3; } #chnioqghai .gt_striped { background-color: rgba(128, 128, 128, 0.05); } #chnioqghai .gt_table_body { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #chnioqghai .gt_footnotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #chnioqghai .gt_footnote { margin: 0px; font-size: 90%; padding-top: 4px; padding-bottom: 4px; padding-left: 5px; padding-right: 5px; } #chnioqghai .gt_sourcenotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #chnioqghai .gt_sourcenote { font-size: 90%; padding-top: 4px; padding-bottom: 4px; padding-left: 5px; padding-right: 5px; } #chnioqghai .gt_left { text-align: left; } #chnioqghai .gt_center { text-align: center; } #chnioqghai .gt_right { text-align: right; font-variant-numeric: tabular-nums; } #chnioqghai .gt_font_normal { font-weight: normal; } #chnioqghai .gt_font_bold { font-weight: bold; } #chnioqghai .gt_font_italic { font-style: italic; } #chnioqghai .gt_super { font-size: 65%; } #chnioqghai .gt_footnote_marks { font-size: 75%; vertical-align: 0.4em; position: initial; } #chnioqghai .gt_asterisk { font-size: 100%; vertical-align: 0; } #chnioqghai .gt_indent_1 { text-indent: 5px; } #chnioqghai .gt_indent_2 { text-indent: 10px; } #chnioqghai .gt_indent_3 { text-indent: 15px; } #chnioqghai .gt_indent_4 { text-indent: 20px; } #chnioqghai .gt_indent_5 { text-indent: 25px; } #chnioqghai .katex-display { display: inline-flex !important; margin-bottom: 0.75em !important; } #chnioqghai div.Reactable > div.rt-table > div.rt-thead > div.rt-tr.rt-tr-group-header > div.rt-th-group:after { height: 0px !important; } Characteristic N = 3331 species     Adelie 146 (44%)     Chinstrap 68 (20%)     Gentoo 119 (36%) island     Biscoe 163 (49%)     Dream 123 (37%)     Torgersen 47 (14%) bill_length_mm 44.5 (39.5, 48.6) bill_depth_mm 17.30 (15.60, 18.70) flipper_length_mm 197 (190, 213) body_mass_g 4,050 (3,550, 4,775) sex     female 165 (50%)     male 168 (50%) year     2007 103 (31%)     2008 113 (34%)     2009 117 (35%) 1 n (%); Median (Q1, Q3) We get a nice table which splits up the categorical data in its categories and displays the absolute and relative frequencies (in the brackets) and for numeric variables we get the median value with first quartile and the third quartile. We can also group by certain variables to get a more detailed overview: penguins %&gt;% tbl_summary(by = sex) #owwxhaeyqf table { font-family: system-ui, 'Segoe UI', Roboto, Helvetica, Arial, sans-serif, 'Apple Color Emoji', 'Segoe UI Emoji', 'Segoe UI Symbol', 'Noto Color Emoji'; -webkit-font-smoothing: antialiased; -moz-osx-font-smoothing: grayscale; } #owwxhaeyqf thead, #owwxhaeyqf tbody, #owwxhaeyqf tfoot, #owwxhaeyqf tr, #owwxhaeyqf td, #owwxhaeyqf th { border-style: none; } #owwxhaeyqf p { margin: 0; padding: 0; } #owwxhaeyqf .gt_table { display: table; border-collapse: collapse; line-height: normal; margin-left: auto; margin-right: auto; color: #333333; font-size: 16px; font-weight: normal; font-style: normal; background-color: #FFFFFF; width: auto; border-top-style: solid; border-top-width: 2px; border-top-color: #A8A8A8; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #A8A8A8; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; } #owwxhaeyqf .gt_caption { padding-top: 4px; padding-bottom: 4px; } #owwxhaeyqf .gt_title { color: #333333; font-size: 125%; font-weight: initial; padding-top: 4px; padding-bottom: 4px; padding-left: 5px; padding-right: 5px; border-bottom-color: #FFFFFF; border-bottom-width: 0; } #owwxhaeyqf .gt_subtitle { color: #333333; font-size: 85%; font-weight: initial; padding-top: 3px; padding-bottom: 5px; padding-left: 5px; padding-right: 5px; border-top-color: #FFFFFF; border-top-width: 0; } #owwxhaeyqf .gt_heading { background-color: #FFFFFF; text-align: center; border-bottom-color: #FFFFFF; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #owwxhaeyqf .gt_bottom_border { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #owwxhaeyqf .gt_col_headings { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #owwxhaeyqf .gt_col_heading { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 6px; padding-left: 5px; padding-right: 5px; overflow-x: hidden; } #owwxhaeyqf .gt_column_spanner_outer { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; padding-top: 0; padding-bottom: 0; padding-left: 4px; padding-right: 4px; } #owwxhaeyqf .gt_column_spanner_outer:first-child { padding-left: 0; } #owwxhaeyqf .gt_column_spanner_outer:last-child { padding-right: 0; } #owwxhaeyqf .gt_column_spanner { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 5px; overflow-x: hidden; display: inline-block; width: 100%; } #owwxhaeyqf .gt_spanner_row { border-bottom-style: hidden; } #owwxhaeyqf .gt_group_heading { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; text-align: left; } #owwxhaeyqf .gt_empty_group_heading { padding: 0.5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: middle; } #owwxhaeyqf .gt_from_md > :first-child { margin-top: 0; } #owwxhaeyqf .gt_from_md > :last-child { margin-bottom: 0; } #owwxhaeyqf .gt_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; margin: 10px; border-top-style: solid; border-top-width: 1px; border-top-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; overflow-x: hidden; } #owwxhaeyqf .gt_stub { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 5px; padding-right: 5px; } #owwxhaeyqf .gt_stub_row_group { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 5px; padding-right: 5px; vertical-align: top; } #owwxhaeyqf .gt_row_group_first td { border-top-width: 2px; } #owwxhaeyqf .gt_row_group_first th { border-top-width: 2px; } #owwxhaeyqf .gt_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #owwxhaeyqf .gt_first_summary_row { border-top-style: solid; border-top-color: #D3D3D3; } #owwxhaeyqf .gt_first_summary_row.thick { border-top-width: 2px; } #owwxhaeyqf .gt_last_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #owwxhaeyqf .gt_grand_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #owwxhaeyqf .gt_first_grand_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-top-style: double; border-top-width: 6px; border-top-color: #D3D3D3; } #owwxhaeyqf .gt_last_grand_summary_row_top { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-bottom-style: double; border-bottom-width: 6px; border-bottom-color: #D3D3D3; } #owwxhaeyqf .gt_striped { background-color: rgba(128, 128, 128, 0.05); } #owwxhaeyqf .gt_table_body { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #owwxhaeyqf .gt_footnotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #owwxhaeyqf .gt_footnote { margin: 0px; font-size: 90%; padding-top: 4px; padding-bottom: 4px; padding-left: 5px; padding-right: 5px; } #owwxhaeyqf .gt_sourcenotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #owwxhaeyqf .gt_sourcenote { font-size: 90%; padding-top: 4px; padding-bottom: 4px; padding-left: 5px; padding-right: 5px; } #owwxhaeyqf .gt_left { text-align: left; } #owwxhaeyqf .gt_center { text-align: center; } #owwxhaeyqf .gt_right { text-align: right; font-variant-numeric: tabular-nums; } #owwxhaeyqf .gt_font_normal { font-weight: normal; } #owwxhaeyqf .gt_font_bold { font-weight: bold; } #owwxhaeyqf .gt_font_italic { font-style: italic; } #owwxhaeyqf .gt_super { font-size: 65%; } #owwxhaeyqf .gt_footnote_marks { font-size: 75%; vertical-align: 0.4em; position: initial; } #owwxhaeyqf .gt_asterisk { font-size: 100%; vertical-align: 0; } #owwxhaeyqf .gt_indent_1 { text-indent: 5px; } #owwxhaeyqf .gt_indent_2 { text-indent: 10px; } #owwxhaeyqf .gt_indent_3 { text-indent: 15px; } #owwxhaeyqf .gt_indent_4 { text-indent: 20px; } #owwxhaeyqf .gt_indent_5 { text-indent: 25px; } #owwxhaeyqf .katex-display { display: inline-flex !important; margin-bottom: 0.75em !important; } #owwxhaeyqf div.Reactable > div.rt-table > div.rt-thead > div.rt-tr.rt-tr-group-header > div.rt-th-group:after { height: 0px !important; } Characteristic female N = 1651 male N = 1681 species     Adelie 73 (44%) 73 (43%)     Chinstrap 34 (21%) 34 (20%)     Gentoo 58 (35%) 61 (36%) island     Biscoe 80 (48%) 83 (49%)     Dream 61 (37%) 62 (37%)     Torgersen 24 (15%) 23 (14%) bill_length_mm 42.8 (37.6, 46.2) 46.8 (41.0, 50.4) bill_depth_mm 17.00 (14.50, 17.80) 18.45 (16.05, 19.30) flipper_length_mm 193 (187, 210) 201 (193, 219) body_mass_g 3,650 (3,350, 4,550) 4,300 (3,900, 5,325) year     2007 51 (31%) 52 (31%)     2008 56 (34%) 57 (34%)     2009 58 (35%) 59 (35%) 1 n (%); Median (Q1, Q3) The gtsummary package gives you many options to customize your table, which can be regarding the content (mean instead of median, displaying p-value…) but you can also customize its appearance for example customizing the font. It also includes a nice option to compute publish-ready cross tables: penguins %&gt;% tbl_cross( row = species, col = island ) #ewrxhtxjvr table { font-family: system-ui, 'Segoe UI', Roboto, Helvetica, Arial, sans-serif, 'Apple Color Emoji', 'Segoe UI Emoji', 'Segoe UI Symbol', 'Noto Color Emoji'; -webkit-font-smoothing: antialiased; -moz-osx-font-smoothing: grayscale; } #ewrxhtxjvr thead, #ewrxhtxjvr tbody, #ewrxhtxjvr tfoot, #ewrxhtxjvr tr, #ewrxhtxjvr td, #ewrxhtxjvr th { border-style: none; } #ewrxhtxjvr p { margin: 0; padding: 0; } #ewrxhtxjvr .gt_table { display: table; border-collapse: collapse; line-height: normal; margin-left: auto; margin-right: auto; color: #333333; font-size: 16px; font-weight: normal; font-style: normal; background-color: #FFFFFF; width: auto; border-top-style: solid; border-top-width: 2px; border-top-color: #A8A8A8; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #A8A8A8; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; } #ewrxhtxjvr .gt_caption { padding-top: 4px; padding-bottom: 4px; } #ewrxhtxjvr .gt_title { color: #333333; font-size: 125%; font-weight: initial; padding-top: 4px; padding-bottom: 4px; padding-left: 5px; padding-right: 5px; border-bottom-color: #FFFFFF; border-bottom-width: 0; } #ewrxhtxjvr .gt_subtitle { color: #333333; font-size: 85%; font-weight: initial; padding-top: 3px; padding-bottom: 5px; padding-left: 5px; padding-right: 5px; border-top-color: #FFFFFF; border-top-width: 0; } #ewrxhtxjvr .gt_heading { background-color: #FFFFFF; text-align: center; border-bottom-color: #FFFFFF; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #ewrxhtxjvr .gt_bottom_border { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #ewrxhtxjvr .gt_col_headings { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #ewrxhtxjvr .gt_col_heading { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 6px; padding-left: 5px; padding-right: 5px; overflow-x: hidden; } #ewrxhtxjvr .gt_column_spanner_outer { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; padding-top: 0; padding-bottom: 0; padding-left: 4px; padding-right: 4px; } #ewrxhtxjvr .gt_column_spanner_outer:first-child { padding-left: 0; } #ewrxhtxjvr .gt_column_spanner_outer:last-child { padding-right: 0; } #ewrxhtxjvr .gt_column_spanner { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 5px; overflow-x: hidden; display: inline-block; width: 100%; } #ewrxhtxjvr .gt_spanner_row { border-bottom-style: hidden; } #ewrxhtxjvr .gt_group_heading { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; text-align: left; } #ewrxhtxjvr .gt_empty_group_heading { padding: 0.5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: middle; } #ewrxhtxjvr .gt_from_md > :first-child { margin-top: 0; } #ewrxhtxjvr .gt_from_md > :last-child { margin-bottom: 0; } #ewrxhtxjvr .gt_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; margin: 10px; border-top-style: solid; border-top-width: 1px; border-top-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; overflow-x: hidden; } #ewrxhtxjvr .gt_stub { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 5px; padding-right: 5px; } #ewrxhtxjvr .gt_stub_row_group { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 5px; padding-right: 5px; vertical-align: top; } #ewrxhtxjvr .gt_row_group_first td { border-top-width: 2px; } #ewrxhtxjvr .gt_row_group_first th { border-top-width: 2px; } #ewrxhtxjvr .gt_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #ewrxhtxjvr .gt_first_summary_row { border-top-style: solid; border-top-color: #D3D3D3; } #ewrxhtxjvr .gt_first_summary_row.thick { border-top-width: 2px; } #ewrxhtxjvr .gt_last_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #ewrxhtxjvr .gt_grand_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #ewrxhtxjvr .gt_first_grand_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-top-style: double; border-top-width: 6px; border-top-color: #D3D3D3; } #ewrxhtxjvr .gt_last_grand_summary_row_top { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-bottom-style: double; border-bottom-width: 6px; border-bottom-color: #D3D3D3; } #ewrxhtxjvr .gt_striped { background-color: rgba(128, 128, 128, 0.05); } #ewrxhtxjvr .gt_table_body { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #ewrxhtxjvr .gt_footnotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #ewrxhtxjvr .gt_footnote { margin: 0px; font-size: 90%; padding-top: 4px; padding-bottom: 4px; padding-left: 5px; padding-right: 5px; } #ewrxhtxjvr .gt_sourcenotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #ewrxhtxjvr .gt_sourcenote { font-size: 90%; padding-top: 4px; padding-bottom: 4px; padding-left: 5px; padding-right: 5px; } #ewrxhtxjvr .gt_left { text-align: left; } #ewrxhtxjvr .gt_center { text-align: center; } #ewrxhtxjvr .gt_right { text-align: right; font-variant-numeric: tabular-nums; } #ewrxhtxjvr .gt_font_normal { font-weight: normal; } #ewrxhtxjvr .gt_font_bold { font-weight: bold; } #ewrxhtxjvr .gt_font_italic { font-style: italic; } #ewrxhtxjvr .gt_super { font-size: 65%; } #ewrxhtxjvr .gt_footnote_marks { font-size: 75%; vertical-align: 0.4em; position: initial; } #ewrxhtxjvr .gt_asterisk { font-size: 100%; vertical-align: 0; } #ewrxhtxjvr .gt_indent_1 { text-indent: 5px; } #ewrxhtxjvr .gt_indent_2 { text-indent: 10px; } #ewrxhtxjvr .gt_indent_3 { text-indent: 15px; } #ewrxhtxjvr .gt_indent_4 { text-indent: 20px; } #ewrxhtxjvr .gt_indent_5 { text-indent: 25px; } #ewrxhtxjvr .katex-display { display: inline-flex !important; margin-bottom: 0.75em !important; } #ewrxhtxjvr div.Reactable > div.rt-table > div.rt-thead > div.rt-tr.rt-tr-group-header > div.rt-th-group:after { height: 0px !important; } island Total Biscoe Dream Torgersen species     Adelie 44 55 47 146     Chinstrap 0 68 0 68     Gentoo 119 0 0 119 Total 163 123 47 333 4.2.6 dlookr dlookr is a nice package with different functions that can support us with EDA. Let us start with the diagnose() function, which is a gelps us identify missing values and unique observations in the dataset: diagnose(penguins) %&gt;% print() ## # A tibble: 8 × 6 ## variables types missing_count missing_percent unique_count ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; ## 1 species fact… 0 0 3 ## 2 island fact… 0 0 3 ## 3 bill_lengt… nume… 0 0 163 ## 4 bill_depth… nume… 0 0 79 ## 5 flipper_le… inte… 0 0 54 ## 6 body_mass_g inte… 0 0 93 ## 7 sex fact… 0 0 2 ## 8 year inte… 0 0 3 ## # ℹ 1 more variable: unique_rate &lt;dbl&gt; We can also generate an output for summary statistics with an old friend, the describe() function, but this time from the dlookr package: dlookr::describe(penguins) ## # A tibble: 5 × 26 ## described_variables n na mean sd se_mean ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 bill_length_mm 333 0 44.0 5.47 0.300 ## 2 bill_depth_mm 333 0 17.2 1.97 0.108 ## 3 flipper_length_mm 333 0 201. 14.0 0.768 ## 4 body_mass_g 333 0 4207. 805. 44.1 ## 5 year 333 0 2008. 0.813 0.0445 ## # ℹ 20 more variables: IQR &lt;dbl&gt;, skewness &lt;dbl&gt;, ## # kurtosis &lt;dbl&gt;, p00 &lt;dbl&gt;, p01 &lt;dbl&gt;, p05 &lt;dbl&gt;, ## # p10 &lt;dbl&gt;, p20 &lt;dbl&gt;, p25 &lt;dbl&gt;, p30 &lt;dbl&gt;, p40 &lt;dbl&gt;, ## # p50 &lt;dbl&gt;, p60 &lt;dbl&gt;, p70 &lt;dbl&gt;, p75 &lt;dbl&gt;, p80 &lt;dbl&gt;, ## # p90 &lt;dbl&gt;, p95 &lt;dbl&gt;, p99 &lt;dbl&gt;, p100 &lt;dbl&gt; The dlookr package has one special feature: It can generate an EDA report with one line of code, I introduce you the eda_report() function: dlookr::eda_paged_report(penguins, output_format = \"html\") 4.2.7 DataExplorer DataExplorer is a powerful all-in-one EDA package, that helps us to explore our data with a few line of code. It also includes a function that generates an EDA report. But let us start by getting basic information about our data with the introduce() function: introduce(penguins) ## # A tibble: 1 × 9 ## rows columns discrete_columns continuous_columns ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 333 8 3 5 ## # ℹ 5 more variables: all_missing_columns &lt;int&gt;, ## # total_missing_values &lt;int&gt;, complete_rows &lt;int&gt;, ## # total_observations &lt;int&gt;, memory_usage &lt;dbl&gt; We can also plot missing values with DataExplorer by using the plot_missing() function: plot_missing(penguins_raw) Further, we can plot correlations with DataExplorer: plot_correlation(penguins_numeric) And finally, we can use DataExplorer to generate an automated Data Report: create_report(penguins) 4.2.8 smartEDA The last package in this chapter is smartEDA. It is a powerful package designed to quickly create descriptive statistics and visualizations for numeric and categorical data. The first function is ExpData(), it gives us the structure, missing values and variable types: ExpData(penguins, type = 1) ## Descriptions ## 1 Sample size (nrow) ## 2 No. of variables (ncol) ## 3 No. of numeric/interger variables ## 4 No. of factor variables ## 5 No. of text variables ## 6 No. of logical variables ## 7 No. of identifier variables ## 8 No. of date variables ## 9 No. of zero variance variables (uniform) ## 10 %. of variables having complete cases ## 11 %. of variables having &gt;0% and &lt;50% missing cases ## 12 %. of variables having &gt;=50% and &lt;90% missing cases ## 13 %. of variables having &gt;=90% missing cases ## Value ## 1 333 ## 2 8 ## 3 5 ## 4 3 ## 5 0 ## 6 0 ## 7 0 ## 8 0 ## 9 0 ## 10 100% (8) ## 11 0% (0) ## 12 0% (0) ## 13 0% (0) We can also let smartEDA calculate summary statistics such as mean, standard deviation, skewness, etc. ExpNumStat(penguins) ## Vname Group TN nNeg nZero nPos NegInf PosInf ## 2 bill_depth_mm All 333 0 0 333 0 0 ## 1 bill_length_mm All 333 0 0 333 0 0 ## 4 body_mass_g All 333 0 0 333 0 0 ## 3 flipper_length_mm All 333 0 0 333 0 0 ## NA_Value Per_of_Missing sum min max mean ## 2 0 0 5715.9 13.1 21.5 17.165 ## 1 0 0 14649.6 32.1 59.6 43.993 ## 4 0 0 1400950.0 2700.0 6300.0 4207.057 ## 3 0 0 66922.0 172.0 231.0 200.967 ## median SD CV IQR Skewness Kurtosis ## 2 17.3 1.969 0.115 3.1 -0.149 -0.897 ## 1 44.5 5.469 0.124 9.1 0.045 -0.888 ## 4 4050.0 805.216 0.191 1225.0 0.470 -0.740 ## 3 197.0 14.016 0.070 23.0 0.359 -0.965 Lastly, we can again generate an automatized EDA report with ExpReport(): ExpReport(data = penguins, Target = \"species\", label = \"Penguin Species\", op_file=\"Samp1.html\", Rc=3 ) 4.3 Conclusion And that is it - at least for now - the possibilities and functions of the packages presented could fill easily an own course and as I already said, at one point everyone gets its own EDA routine and has its own packages they want to work with. The important point is to always get an overview over your data and to always check for interesting patterns in your data before conducting substantial analysis. 4.4 Exercise Section 4.4.1 Exercise 1: Standard Descriptive Statistics In this exercise we will work with the built-in iris package in R: a. Calculate the mode, mean and the median for the iris$Sepal.Length variable b. Calculate the interquartile range, variance and the standard deviation for iris$Sepal.Length c. Calculate all five measures at once by using a function that does so (Choose by yourself, which one you want to use) 4.4.2 Exercise 2: Contingency Tables and Correlations a. Make a Contingency Table for esoph$agegp and esoph$alcgp b. Cut down the iris dataset to Sepal.Length, Sepal.Width, Petal.Length and Petal.Width and save it in an object called iris_numeric. c. Make a correlation matrix with iris_numeric d. Make the correlation matrix prettyChapter 4: Exploratory Data Analysis 4.4.3 Exercise 3: Working with packages a. Use a function to get an overview of the dataset mtcars b. Have a look at the structure of the missing values in mtcars c. Make an automatized EDA report for mtcars! "],["data-analysis.html", "Chapter 5 Data Analysis 5.1 Linear Regression 5.2 Hypothesis Testing in R 5.3 Multivariate Regression 5.4 Categorical Variables 5.5 Outlook 5.6 Exercise Section", " Chapter 5 Data Analysis In this chapter you I introduce you to the basic ideas of statistical analysis and hypothesis testing. For me, it is important that you get an intuition about what is going on rather than terrorizing you with complicated math. Although, I cannot and will not leave out central formulas, you will be fine with the maths. There extensively commented and in the end of the day not that hard to understand. You will notice that the programming in this chapter is quite easy compared to before. In the end, you only need formulas, who do everything you need to analyse data, that is also a strength of R. Please concentrate more on the concepts and get an idea what is going on. The goal of this chapter is that you get to know linear regression and how to check if the model fits and the results are robust and significant. pacman::p_load(&quot;tidyverse&quot;, &quot;ggpubr&quot;,&quot;gapminder&quot;, &quot;sjPlot&quot;, &quot;GGally&quot;, &quot;car&quot;, &quot;margins&quot;, &quot;plotly&quot;) 5.1 Linear Regression 5.1.1 Terminology The classical (bivariate) linear regression can be expressed in the following equation (systematic component): \\[ Y_i = \\beta_0 + \\beta_1X_i + e_i \\] where the index i runs over the observations (Respondents, Countries,…), i = 1,…,n \\(Y_i\\) is the dependent variable, the variable we want to explain \\(X_i\\) is the independent variable or explanatory variable. \\(\\beta_0\\) is the intercept of the regression line \\(\\beta_1\\) is the slope of the regression line \\(\\epsilon_i\\) is the error term, thus how our observed data differs from actual population data (e.g. Measurement Error). 5.1.2 Estimating the Ordinary Least Squares Estimator To get the idea of linear regression, let us look at an example. To do so, let us simulate some data and plot it.You now should have the data frame df in your environment. It contains a variable X, which is our independent variable. Y is also included, which is your dependent variable. You want to explain Y with your X. Let us plot the variables with a scatterplot: ggplot(df, aes(x, y)) + geom_point() + theme_bw() + scale_x_continuous(breaks = seq(0, 10, by = 1)) + scale_y_continuous(breaks = seq(0, 20, by = 2)) We can see that there has to be some relationship between both those variables: The higher x gets the higher y gets. Linear regression can help us investigate the relationship. We just have to take the formula and estimated \\(\\beta_0\\) and \\(\\beta_1\\) . To do so, we estimate the ordinary least square (OLS) estimator. To understand what the OLS estimator does, look at the scatter plot again: The OLS estimator fits a line through all the dots, that minimizes the distance to the dots as much as possible. Afterward, we only extract the intercept \\(\\beta_0\\) (that is the point, where the line crosses the y-axis), and \\(\\beta_1\\) (that is the slope of the line). However, since these are estimated values for our model we have to call them by convention \\(\\hat{\\beta_0}\\) and \\(\\hat{\\beta_1}\\) . We will denote them in the following as such. 5.1.2.1 Visualization The visual estimation is good to get an intuition with the data, but we will see later, that it does not work with multiple independent variables. Further, it does not give much information, at least not as much as we would like to have. ggplot(df, aes(x, y)) + geom_point() + theme_bw() + scale_x_continuous(breaks = seq(0, 10, by = 1)) + scale_y_continuous(breaks = seq(0, 30, by = 5)) + geom_smooth(method = &quot;lm&quot;, se = FALSE) + geom_segment(aes(x = x, y = y, xend = x, yend = predict(lm(y ~ x, data = df))), linewidth = 0.5) ## `geom_smooth()` using formula = &#39;y ~ x&#39; 5.1.2.2 Calculation per Hand We could also just calculate \\(\\hat{\\beta_0}\\) and \\(\\hat{\\beta_1}\\) by hand. The formula for both are as follows: \\[ \\hat{\\beta_1} = \\frac{\\sum_{i=1}^n(x_i - \\bar{x})(y_i - \\bar{y})}{\\sum_{i=1}^n(x_i - \\bar{x})^2} \\] where \\(x_i\\) is the answer of respondent i for our independent variable x, \\(\\bar{x}\\) is the average answer of the respondents. Those two values are subtracted, thus the deviation from the mean is calculated. The same goes with our dependent variable y, where \\(y_i\\) is the answer of respondent i, and \\(\\bar{y}\\) is the average answer of all respondents. his is done for every respondent i, thus n - times. This is displayed with the sum symbol. We just calculated the so-called covariance. The covariance is then divided by the squared deviation to the average answer of our independent variable x. This will get us the estimated coefficient for our model \\(\\hat{\\beta_1}\\) . Sounds complicated but lets do it in R: #Let us first get the covariance cov &lt;- sum((df$x - mean(df$x)) * (df$y - mean(df$y))) #Now we get the variance of x x_sq &lt;- sum((df$x - mean(df$x))^2) x_sq ## [1] 246.1869 # We just have to divide them slope &lt;- cov/x_sq #printing it print(slope) ## [1] 1.539357 To get the intercept \\(\\hat{\\beta_0}\\) we have to take the result of the slope and implement it into this formula: \\[ \\hat{\\beta_0} = \\bar{y} - \\hat{\\beta_1}*\\bar{x} \\] We multiply \\(\\hat{\\beta_1}\\) with the average of our independent variable X. The result is subtracted from the average of our dependent variable y. #calculating the intercept beta_0 &lt;- mean(df$y) - (slope * mean(df$x)) #printing it beta_0 ## [1] 1.682133 Now we estimated our parameters and can display the for our model by simply putting it into the systematic component: \\[ Y_i = 1.68 + 1.54 * X_i \\] 5.1.2.3 Automated Calculation This procedure by hand is way to time-wasting. R has a built in function to calculate the parameter for us called lm() : #running a linear regression model1 &lt;- lm(y ~ x, data = df) #Printing a summary of the model results summary(model1) ## ## Call: ## lm(formula = y ~ x, data = df) ## ## Residuals: ## Min 1Q Median 3Q Max ## -5.1985 -1.4215 -0.4309 1.5505 5.9720 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1.6821 1.0376 1.621 0.116 ## x 1.5394 0.1621 9.496 2.97e-10 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 2.543 on 28 degrees of freedom ## Multiple R-squared: 0.7631, Adjusted R-squared: 0.7546 ## F-statistic: 90.18 on 1 and 28 DF, p-value: 2.974e-10 When we check the results, we see that we did everything right and get the exact same values. The interpretation of the coefficient is with a one-unit increase in the independent variable X, the dependent increases on average about 0.68 units, holding all else constant. But you noticed that you get more information on the model, than just the coefficients. We will get to that later. 5.1.3 Predictions with Linear Regression We could calculated predictions based on our OLS calculations, reconsider the systematic component we calculated: \\[ \\hat{y_i} = 1.68 + 1.54x_i \\] We just have to put in x-values and according to our model we would get a prediction of the respondents y-value \\(\\hat{y_i}\\). We can do that for all x-values in our dataset and get a column with all the predicted values, let us call it (y_hat) : #First, we calculate the predictions for y df$y_hat &lt;- 1.6821 + 1.5394*df$x #We could also do it automatically via the predict() function df$auto_y_hat &lt;- predict(model1) #Checking it head(df) ## x y categorical_variable y_hat ## 1 2.875775 6.680633 0 6.109068 ## 2 7.883051 12.527668 1 13.817269 ## 3 4.089769 10.029008 0 7.977891 ## 4 8.830174 17.562679 1 15.275270 ## 5 9.404673 18.312220 1 16.159653 ## 6 0.455565 3.594825 0 2.383397 ## auto_y_hat ## 1 6.108979 ## 2 13.816967 ## 3 7.977750 ## 4 15.274927 ## 5 16.159286 ## 6 2.383411 5.2 Hypothesis Testing in R We are not only interested if our model fits or not. We want to investigate real world phenomena. Well, and if our model does not fit well, we have to make adjustments so it does. What comes next is that we want to know more about the world. Researcher do so by formulating hypotheses. These are nothing more than assumptions you make theoretically about the world. Let us say you think you assume that in our world education has an impact on income. This would be your Alternative Hypothesis \\(H_A\\) . What you know want to do is to test it against the so-called Null Hypothesis \\(H_0\\). That is nothing more than the opposite of our alternative hypothesis, thus that education has no impact on income. Let us formulate both to have an overview: \\(H_0\\) = Education does not impact Income. \\(H_A\\) = The more educated a person is, the higher the income. The advantage of this approach is obvious, one of those will be true. Therefore statistical testing is needed. But before introducing it to you, we have to decide how we want to test our alternative hypothesis. More specifically, how do we want to measure our variables. For our example, we decide to conduct a survey and to get data by asking a random sample of 1000 people over 18 living in Germany (N=1000). We decide to measure our independent variable Education, by asking the respondents about the years they invested in their education. To get their income, you just ask them about it to fill it in. Can you think about possible critics about our data collection strategy? 5.2.1 Standard Error 5.2.1.1 Root Mean Square Error (RMSE) Before moving on to Standard Errors, I will introduce another metric, the root mean square error. It is the average difference between the actual values \\(y_i\\) and our predicted values \\(\\hat{y_i}\\). To calculate it, we square the residuals, to get only positive values. Then we take the mean, and lastly we take the square root. #Getting the the sum of squared residuals (SSR) SSR &lt;- df$y_hat^2 #Calculating the mean of the squared residuals mean_SSR &lt;- mean(SSR) #Calculatin the RSME rsme &lt;- sqrt(mean_SSR) #Printing it print(rsme) ## [1] 11.38257 The rule of thumb is that the lower the RSME, the better. It is a non-standardized goodness of fit measure. Its counterpart is the R-squared measure, which is a standardized measure. You can use both, and should use both to get a metric about how close the predicted values are distributed around the actual values. 5.2.1.2 Standard Error of the Estimate The standard error of a coefficient estimate is the metric directly presented next to the coefficient in the regression output. It is the square root of the variance of the regression coefficient. #calculating standard error by hand se &lt;- SSR/(nrow(df) - 2 * (sum(df$x - mean(x)))) #Printing it print(se) ## [1] 1.2440239 6.3638977 2.1215580 7.7777957 8.7044799 ## [6] 0.1893527 3.2090185 7.9258787 3.4482339 2.5295160 ## [11] 8.9780123 2.5002725 4.8905207 3.6730533 0.3556723 ## [16] 8.0435110 0.9974999 0.1808957 1.5098134 8.9388169 ## [21] 7.8803725 5.0817088 4.4406398 9.6196124 4.6225001 ## [26] 5.2829470 3.3717449 3.9084188 1.2539634 0.5192325 5.2.2 T-Value or T-Statistic The formula for calculating the t-value is simple: \\[ t_i = \\frac{\\beta_i}{SE(\\beta_i)} \\] where \\(\\beta_i\\) is the coefficient calculated by the linear regression, and \\(SE(\\beta_i)\\) is the standard error. Let us calculate it manually by hand for our example: #t value by hand t_value_intercept &lt;- -0.32773/0.30271 t_value_x &lt;- 0.67949/0.04813 #printing it print(t_value_intercept) #-1.082653 ## [1] -1.082653 print(t_value_x) #-14.11781 ## [1] 14.11781 Well, that is the t-value on its own does not tell us about statistical significance. It is used to calculate the p-value, which tells us about the significance of the value. But before we calculate it, we have to understands some key concepts before. Degrees of Freedom is the first concept. Let us say, we have a sample of me and my sister (N = 2). We collected the numbers of books each of us has, and calculated a mean of 30. I have 20 books. How many books does my sister have? Obviously, she has 40, if we have a mean of 30. Given my value, and the mean of the sample, the value of my sister had to be 20. That is what the degrees of freedom tells us. Given the mean of a sample, the values, which can freely be chosen. Or to put it more technically, the maximum number of logically independent values. The rule is that the larger the sample, the higher the degrees of freedom and the lower the sample, the lower the degrees of freedom. T-distributions are the distribution, we will use to calculate the p-value. I will talk about that in detail in a minute. But they are connected to the degrees of freedom, because degrees of freedom determine the tail behavior and shape of the curve until the point, where the t-distribution looks like a standard normal distribution. Let us visualize that: # Generate data x &lt;- seq(-5, 5, length.out = 100) # Calculate densities densities &lt;- data.frame( x = rep(x, 4), density = c(dt(x, df = 1), dt(x, df = 2), dt(x, df = 10), dnorm(x, mean = 0, sd = 1)), distribution = rep(c(&quot;t(df=1)&quot;, &quot;t(df=2)&quot;, &quot;t(df=10)&quot;, &quot;Normal&quot;), each = 100) ) densities$distribution &lt;- factor(densities$distribution, levels = c(&quot;Normal&quot;, &quot;t(df=10)&quot;, &quot;t(df=2)&quot;, &quot;t(df=1)&quot;)) # Plot plotly::ggplotly(ggplot(densities, aes(x = x, y = density, color = distribution)) + geom_line() + theme_minimal() + labs(x = &quot;x&quot;, y = &quot;Density&quot;, title = &quot;t-distributions with different degrees of freedom&quot;) + scale_color_manual(values = c(&quot;black&quot;, &quot;red&quot;, &quot;green&quot;, &quot;blue&quot;)) + scale_x_continuous(&quot;X&quot;, seq(-5,5,1), limits = c(-5,5))) Alright, before we find out, how to determine if a value allows us to reject the null hypothesis, we have to determine a so-called critical value. This is no math or anything, it is a probability we decide about. More precisely, the probability that our t-value and thus our coefficient occured by chance alone. If we say the probability that it occured by chance alone is 10%, then this is our critical value. The rule of thumb is, that a probability lower than 5% that the coefficient occured by chance alone indicates statistical significance. Keep in mind, that the critical value can vary depending on the sample size, the degrees of freedom, the field you are working in etc. #setting seed set.seed(42) # Generate data x &lt;- seq(-5, 5, length.out = 100) t_density &lt;- function(x) dt(x, df = 28) # Calculate densities t_value_data &lt;- data.frame( x = rep(x, 1), density = dt(x, df = 28), distribution = rep(&quot;t(df=28)&quot;, 100) ) # Plot plotly:: ggplotly(ggplot(t_value_data, aes(x = x, y = density)) + geom_line(lineend = &quot;round&quot;) + stat_function(fun = t_density, geom = &quot;area&quot;, fill = &quot;gray&quot;, alpha = 0.75, xlim = c(-5, -1.701), n = 10000) + stat_function(fun = t_density, geom = &quot;area&quot;, fill = &quot;gray&quot;, alpha = 0.75, xlim = c(5, 1.701), n = 10000) + geom_vline(xintercept = -1.701, linetype = &quot;dashed&quot;, colour = &quot;red&quot;) + geom_vline(xintercept = 1.701, linetype = &quot;dashed&quot;, colour = &quot;red&quot;) + ggtitle(&quot;t-distribution with 28 df&quot;, subtitle = &quot;The pink area marks the interval of significant values on a 95% level&quot;) + geom_segment(x = -1.082653, xend = -1.082653, yend = dt(-1.082653, df = 28), y = -1, color = &quot;pink&quot;, linetype = &quot;dashed&quot;, linewidth = 0.2) + annotate(&quot;point&quot;, x = -1.082653, y = dt(-1.082653, df = 28), color = &quot;pink&quot;) + scale_x_continuous(&quot;X&quot;, seq(-5,5,1), limits = c(-5,5)) + theme_classic() + theme(legend.position = &quot;none&quot;) ) As we can see, the blue line representing the value of the intercept is not in the area it would have to be, for us to reject the null hypothesis. However, the t-value of our coefficient from variable is with about 14 far away from the threshold, so we can reject the null hypothesis. This is how the t-value works. The problem is, that the threshold varies with the degrees of freedom, and you will not have the threshold value in your head for every degree of freedom. You could look it up every time or you use p-values, which directly tell you the probability of the coefficient occuring by chance alone. 5.2.3 p-values The p-value is a statistics that shows us the probability that a statistical measure (in our example 0) is greater/less than an observed value (in our example, the estimated coefficient). The null hypothesis would tell us that our independent variable X has no impact on Y. Statistically speaking, that would mean that our coefficient has a high probability to be zero, because zero means no effect, thus we cannot reject the null hypothesis. But if the probability that our estimated coefficient is 0 is low, we may reject the null hypothesis and would found an effect. Before showing you two ways to find the p-value, we have to determine a critical value. This is no math or anything, it is a probability we decide about. The rule of thumb is that if the p-value is smaller than 5%, we can reject the null hypothesis. However, depending on various factors, it could also be different, that depends on your data, sample size, degrees of freedom etc. For our example, we follow the rule of thumb and set the significance level to 0.05, thus if the coefficient has a smaller probability (p-value &lt; 0.05) than 5% to be zero we can reject the null hypothesis, otherwise we cannot reject it. Since the math is complicated and thus not help to understand the intuition, I directly show you how to calculate the p-value by hand: #calculating the p values by hand p_value_1 &lt;- 2 * pt(-abs(t_value_intercept), 28) p_value_2 &lt;- 2 * pt(-abs(t_value_x), 28) #printing it print(p_value_1) ## [1] 0.2881981 print(p_value_2) ## [1] 2.941061e-14 We get the same values as in our regression, and we decided before that our critical value (denoted as \\(\\alpha\\)) should be less than 0.05 to reject the null hypothesis. Therefore we can reject the null hypothesis for the coefficient of our variable x. Since the coefficient is positive, the interpretation would be that x has on average a positive effect on y, since the probability that the value is different than 0 is lower than the critical value of 5%. 5.2.4 Confidence Interval The last way of testing hypothesis are confidence intervals. I recommend to use them, since they are intuitive and easier to interpret than p-values. To understand confidence intervals, we must remember the difference between a population and a sample. The population are all people we want to infer to, for example in an election, the population are all citizens eligible to vote in that election. Let us assume, all else equal, that the the vote share in the population for Party A is 24%. This is what we call the true population parameter. And our goal is to estimate this parameter with statistical methods, since it is more efficient. Think about Germany, we cannot ask 80 million people before the election to give us their thoughts, so what we do instead, is to draw a representative sample of less citizens from that sample to infer to the population. The problem is that even if the sample is representative, it could be that our sample today gives us a vote share of Party A of 23%, but tomorrow we would get 25% (This could have several reasons, can you think of some?). In the following, let us assume we draw 1000 samples before the election, to get the vote share of Party A. We know that our true population parameter is 24%. But before we start, we have to set a rule again. This rule is basically the definition of confidence intervals: In 95% of all samples, that could be drawn, the confidence intervals will cover the true population parameter. So if we draw 1000 samples, in 950 the confidence intervals have to cover the true population parameter: #Since this is a simulation we need to set a seed set.seed(187) #We will need to have vectors for the upper confidence interval and the lower one lower_ci &lt;- numeric(100) upper_ci &lt;- numeric(100) estimates &lt;- numeric(100) #This loop represents for(i in 1:length(lower_ci)) { Y &lt;- rnorm(100, mean = 24, sd = 2) estimates[i] &lt;- Y[i] lower_ci[i] &lt;- Y[i] - 1.96 * 24 / 10 upper_ci[i] &lt;- Y[i] + 1.96 * 24 / 10 } #Let us bind both vectors together CIs &lt;- data.frame(estimates, lower_ci, upper_ci) #Print it head(CIs) ## estimates lower_ci upper_ci ## 1 22.86723 18.16323 27.57123 ## 2 25.36001 20.65601 30.06401 ## 3 26.32276 21.61876 31.02676 ## 4 23.37235 18.66835 28.07635 ## 5 22.71026 18.00626 27.41426 ## 6 21.89276 17.18876 26.59676 Now we have drawn our 1000 samples and computed our estimates as well as their corresponding intervals. Thus, 1000 samples about the vote share of Party A. Let us check, if the true population parameter is 95% of times within our computed confidence intervals: #Getting the true mean true_mean &lt;- 24 #First, we identify those who are not including 24 our true population parameter CIs$missed &lt;- ifelse(CIs$lower_ci &gt; true_mean | CIs$upper_ci &lt; true_mean, &quot;Out&quot;, &quot;In&quot;) #Let us give every sample an identification number CIs$id &lt;- 1:nrow(CIs) #Plotting it ggplot(data = CIs) + geom_pointrange( aes( x = estimates, # point value xmin = lower_ci, # lower CI xmax = upper_ci, # upper CI y = id, # y axis - just observation number color = missed ) # color varies by missed variable ) + geom_vline( aes(xintercept = true_mean), # add vertical line at true_mean ) + scale_color_manual(values = c(&quot;azure4&quot;, &quot;red&quot;)) + theme_minimal() + labs( title = &quot;Confidence Interval for Mean&quot;, subtitle = &quot;Population mean equals 24&quot;, x = &quot;Estimates&quot;, y = &quot;Sample&quot;, color = &quot;Is true population parameter inside the CI?&quot; ) + theme(legend.position = &quot;top&quot;) + # switch the legend to the top scale_x_continuous(breaks = c(seq(15, 30, by = 1))) Well, we can see that the majority of the computed intervals include the true population parameter. But there are three, which are not. That is within our definition. What is now with confidence intervals for coefficients of linear regression? Well, it is the same story, we compute an estimate, in this case a coefficient. The true population parameter is unknown, but we know that the true population parameter is within 95% of the intervals. Its calculation is fairly easy and you already saw it: \\[ CI_{lower} = \\beta_i - 1.96 * SE(\\beta_i) \\\\\\ CI_{upper} = \\beta_i + 1.96 * SE(\\beta_i) \\] #Let us look at the confindence intervals of model 1 confint(model1) ## 2.5 % 97.5 % ## (Intercept) -0.4432173 3.807484 ## x 1.2073136 1.871401 #Let us compute the confidence values by hand for the intercept and x ci_lower_int &lt;- model1$coefficients[1] - 1.96 * summary(model1)$coef[, &quot;Std. Error&quot;][1] ci_upper_int &lt;- model1$coefficients[1] + 1.96 * summary(model1)$coef[, &quot;Std. Error&quot;][1] #Print it print(ci_lower_int) ## (Intercept) ## -0.3514894 print(ci_upper_int) ## (Intercept) ## 3.715756 #Estimate X ci_lower_est &lt;- model1$coefficients[2] - 1.96 * summary(model1)$coef[, &quot;Std. Error&quot;][2] ci_upper_est &lt;- model1$coefficients[2] + 1.96 * summary(model1)$coef[, &quot;Std. Error&quot;][2] #Print it print(ci_lower_est) ## x ## 1.221644 print(ci_upper_est) ## x ## 1.857071 5.3 Multivariate Regression The world is a complex place and of course if we have a dependent variable Y, let us say for example income, we cannot explain it exclusively by the years of education or exclusively by the profession or any other single factor. Rather it is plausible that all these factors matter. The Multivariate Linear Regression Model allows us that we explain the variation in our dependent variable Y with multiple independent variables X. Mathematically, the systematic component changes like this: \\[ Y_i = \\beta_0 + \\beta_1X_{1i} + \\beta_2X_{2i} + ... + \\beta_kX_{ki} + \\epsilon_i \\] where the index i runs over the observations (Respondents, Countries,…), i = 1,…,n \\(Y_i\\) is the dependent variable, the variable we want to explain \\(X_{1i}\\) is the first independent variable or explanatory variable. \\(X_{2i}\\) is the second independent variable or explanatory variable. \\(X_{ki}\\) is the k-th independent variable or explanatory variable, where k is the number of all our independent variables in our model. \\(\\beta_0\\) is the intercept of the regression line \\(\\beta_1\\) is the slope of the regression line of the first explanatory variable. \\(\\beta_2\\) is the slope of the regression line of the second explanatory variable. \\(\\beta_k\\) is the slope of the regression line of the k-th explanatory variable, where k is the number of all our independent variables in our model. \\(\\epsilon_i\\) is the error term, thus how our observed data differs from actual population data (e.g. Measurement Error). In R, we can implement a multivariate model very easy with the already known lm() function. Let us run a multiple regression with our independent Variable X and our categorical Variable Z, the systematic component for this model looks like this: \\[ Y_i = \\beta_0 + \\beta_1X_i + \\beta_2Z_i + \\epsilon_i \\] Let us compute the slopes: lm(y ~ x + categorical_variable, data = df) %&gt;% summary() ## ## Call: ## lm(formula = y ~ x + categorical_variable, data = df) ## ## Residuals: ## Min 1Q Median 3Q Max ## -4.5183 -1.3840 -0.5014 1.2393 5.5808 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1.9799 1.0679 1.854 0.0747 ## x 1.5557 0.1621 9.596 3.42e-10 ## categorical_variable1 -1.0667 0.9637 -1.107 0.2781 ## ## (Intercept) . ## x *** ## categorical_variable1 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 2.533 on 27 degrees of freedom ## Multiple R-squared: 0.7734, Adjusted R-squared: 0.7566 ## F-statistic: 46.07 on 2 and 27 DF, p-value: 1.982e-09 We can see, that this works fine, we only had to add the independent variable in the command such as in our systematic component. Regarding the interpretation it is analogous to the normal linear regression one. For a one-unit increase in our independent variable x, the dependent variable y increases 0.67 units on average, holding all else constant. For category 1 in comparison to category 0, the dependent variable y increases 0.52 units on average, holding all else equal. Now, we can put the calculated coefficients into our systematic component and make predictions: \\[ Y_i = -0.54 + 0.67X_i + 0.52Z_i + \\epsilon_i \\] If we want to visualize this, we would need a three-dimensional coordinate system. Why? Because every independent variable is one-dimension if you want. I said that k is the number of our independent variables, technically it is the number of dimensions our model has. I am not a fan of plotting graphs more than three dimensions and I do not recommend it to you either. 5.4 Categorical Variables As I mentioned in the first chapter, there are different types of variables, let us reconsider them: Numeric Numbers c(1, 2.4, 3.14, 4) Character Text c(\"1\", \"blue\", \"fun\", \"monster\") Logical True or false c(TRUE, FALSE, TRUE, FALSE) Factor Category c(\"Strongly disagree\", \"Agree\", \"Neutral\") Dependent variables must be numeric, when conducting linear regression. However, independent variables can take be scaled differently. Numeric variables are the easiest case, the interpretation is as mentioned in the previous examples. But categorical variables are differently to interpret. Let us inspect the categorical variable in our dataset, called categorical_variable: table(df$categorical_variable) ## ## 0 1 ## 19 11 As we can see our data set now contains a categorical variable with two categories, named “A” and “B”. Those categories could be anything: Female and Male, bought a product or did not buy a product, vaccine or placebo and so on. What happens, when we now run a model? #running a model with a categorical variable model2 &lt;- lm(y ~ categorical_variable, data = df) #Getting the summary summary(model2) ## ## Call: ## lm(formula = y ~ categorical_variable, data = df) ## ## Residuals: ## Min 1Q Median 3Q Max ## -10.2770 -3.7683 0.1863 4.0996 10.2377 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 10.5765 1.1985 8.825 1.41e-09 ## categorical_variable1 -0.2265 1.9792 -0.114 0.91 ## ## (Intercept) *** ## categorical_variable1 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 5.224 on 28 degrees of freedom ## Multiple R-squared: 0.0004673, Adjusted R-squared: -0.03523 ## F-statistic: 0.01309 on 1 and 28 DF, p-value: 0.9097 We see a coefficient of 0.64 that is fine. But as you see the category “category_A” is not display, why? Categorical variables are calculated based on so-called “reference categories”. R determines the reference category based on alphabetical order. In this case the category “A” is the reference category. The reference category is named like this, since the computed coefficients refer to it. The reference category “A” takes on the value 0. The coefficient of the category B is “0.92”. That means that category “B” if the a respondent is part of category “B” than the dependent variable Y increases on average 0.64 units in comparison to category “A”, holding all else equal. That sounds technocratic, let us get an intuition. We add a zero to our code and look at the output: #running the model model3 &lt;- lm(y ~ categorical_variable + 0, data = df) #getting a summary summary(model3) ## ## Call: ## lm(formula = y ~ categorical_variable + 0, data = df) ## ## Residuals: ## Min 1Q Median 3Q Max ## -10.2770 -3.7683 0.1863 4.0996 10.2377 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## categorical_variable0 10.576 1.198 8.825 1.41e-09 ## categorical_variable1 10.350 1.575 6.571 3.99e-07 ## ## categorical_variable0 *** ## categorical_variable1 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 5.224 on 28 degrees of freedom ## Multiple R-squared: 0.8122, Adjusted R-squared: 0.7987 ## F-statistic: 60.53 on 2 and 28 DF, p-value: 6.812e-11 The coefficients changed, but not really. Let us subtract the coefficient from category A from category B: #results result &lt;- coefficients(model3)[2] - coefficients(model3)[1] #coefficents can be extracted this way #printing it result ## categorical_variable1 ## -0.2264615 We get the same coefficient as above and that is what R does automatically when computing coefficients of categorical variables. The reference category is scaled to 0 and the other categories are following. 5.4.1 Interaction Effects One technique, which is important and widely used in statistics are interaction effects. To keep things simple concentrate on our independent variable X and our categorical variable Z. As mentioned, Z shows us the coefficients for our categories “A” and “B”. But what if we could further investigate the dynamics of the group? Interaction effects allow us to multiply our variable X by our categorical variable Z. Mathematically our systematic component looks like this: \\[ Y_i = \\beta_0 + \\beta_1X_i + \\beta_2Z_i + \\beta_3X_i*Z_i + e_i \\] where the index i runs over the observations (Respondents, Countries,…), i = 1,…,n \\(Y_i\\) is the dependent variable, the variable we want to explain \\(X_i\\) is the independent variable or explanatory variable. \\(Z_i\\) is our categorical variable \\(\\beta_0\\) is the intercept of the regression line \\(\\beta_1\\) is the slope of the regression line \\(\\epsilon_i\\) is the error term, thus how our observed data differs from actual population data (e.g. Measurement Error). In the following, I will show you an example how to use interaction effects. Let’s say that I conducted a survey among 1000 respondents and asked them about how many hours they spent on R online courses such as this one. Then I asked about their coding ability. Lastly, I asked if they learned with this course or with other courses (0 = other courses, 1 = this course). Now we want to find out if spending more hours on a course lead to a higher coding ability in R. Furthermore, we want to find out if this course in comparison to other courses lead to a higher ability. To do so, we interact hours spent on courses with the variable indicating if the respondent worked through other courses or this course. Interactions effects can be implemented in R, by simply writing it explicitly into the function lm(), either with an asterisks * or a double point :, let us have a look it: # Setting seed for reproducibility set.seed(123) # Generate hours spent on a course hours_spent &lt;- runif(100, min = 0, max = 10) # Generate the course dummy (0 = other courses, 1 = this course) this_course = sample(c(0, 1), 100, replace = TRUE) # Generate y with interaction effect coding_ability &lt;- 2 + 0.5 * hours_spent + 0 * this_course + 1.5 * hours_spent * this_course + rnorm(100) # Create a data frame df_int &lt;- data.frame(hours_spent, this_course, coding_ability) # Fit the interaction model model_interaction &lt;- lm(coding_ability ~ hours_spent * this_course, data = df_int) # Summarizing models summary(model_interaction) ## ## Call: ## lm(formula = coding_ability ~ hours_spent * this_course, data = df_int) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.9122 -0.7295 -0.0765 0.5901 3.3882 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1.82617 0.32506 5.618 1.88e-07 ## hours_spent 0.52184 0.05380 9.699 6.59e-16 ## this_course 0.03336 0.41030 0.081 0.935 ## hours_spent:this_course 1.47571 0.07068 20.877 &lt; 2e-16 ## ## (Intercept) *** ## hours_spent *** ## this_course ## hours_spent:this_course *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.9799 on 96 degrees of freedom ## Multiple R-squared: 0.9696, Adjusted R-squared: 0.9687 ## F-statistic: 1022 on 3 and 96 DF, p-value: &lt; 2.2e-16 Interaction effects on their own are not intuitive. To get an intuition we have to graphically plot it. The sjPlot package offers the plot_model() command, which automatically plots so called predicted probabilities. It would be too much to go into detail about predicted probabilities. What is more important is, that we get a graph, which shows the effect of of the interaction: We call plot_model() and include our model with the interaction effect model_interaction Then we have to call the type and set it to type=\"int\", which explicitly plots interaction effects. plot_model(model_interaction, type = &quot;int&quot;) + scale_x_continuous(breaks = seq(0,10, 1)) + labs(title = &quot;Coding Ability after this Course in Comparison&quot;, x = &quot;Hours spent&quot;, y = &quot;Coding Ability in R&quot;) + scale_color_manual( values = c(&quot;red&quot;, &quot;blue&quot;), labels = c(&quot;Other Courses&quot;, &quot;This Course&quot;) ) + theme_sjplot() + theme(legend.position = &quot;bottom&quot;, legend.title = element_blank()) On the x-axis, we have our independent variable and on the y-axis we do have our dependent variable. So far, so normal. But we see two lines: One line for all observations in category “Other Courses” (red line) and one line for all observations with “This Course” (blue line). That is exactly what an interaction does. For every point in our independent variable a probability for y is computed in one category, and one line for every observation in the other category. To interpret it we follow two steps: First, looking at the direction of the lines. We can see that X has a positive effect on Y. Both lines are increasing with higher X-values. Now, the interaction allows us to compare the effect from X on Y for all groups of the categorical variable. In our example that means, that observations in group 1 display a higher effect than observations in group 0. That could be for several reasons. Let us fill this example with life: We are studying the effect of study hours (x) on exam scores (y) for two different groups of students: those who attend a preparatory course (group 1) and those who do not (group 0). Interpretation: Direction of the Lines: Both lines (for group 0 and group 1) are increasing, indicating that more hours spent on courses lead to higher coding ability in R, on average. Effect Comparison: The slope for group 1 ( attended this course) is steeper compared to group 0 (attended to other courses). This means that for students who attended this course, each additional hour of study has a larger positive impact on their coding ability in R compared to those who did attend another course By visualizing and analyzing the interaction effect, we can draw meaningful conclusions about how different factors (like attending a preparatory course) modify the relationship between study efforts and performance outcomes. 5.5 Outlook This chapter was an introduction to hypothesis testing and basic statistical applications. Further, you were introduced in the most popular and important model, linear regression. The chapter showed not only how linear regression in R is computed, but also how to check if effects are robust and how the models fits. Also extensions to linear regression, namely multivariate regression and categorical variable were introduced. Linear regression is the basis of nearly everything. Advanced modelling of different classes of dependent variables to even machine learning techniques are based on linear regression. This was the reason, I did not show just linear regression, but also how vulnerable the model is. Poor modelling will always lead to poor results, so we have to aim to check how good our model fits our data and thus research interest. Further Links: A course I can recommend if you want to have a detailed deep dive into statistics is “Introduction to Econometrics with R” by Christoph Hanck, Martin Arnold, Alexander Gerber, and Martin Schmelzer. 5.6 Exercise Section To understand linear regression, we do not have to load any complicated data. Let us assume, you are a market analyst and your customer is the production company of a series called “Breaking Thrones”. The production company wants to know how the viewers of the series judge the final episode. You conduct a survey and ask people on how satisfied they were with the season final and some social demographics. Here is your codebook: Variable Description id The id of the respondent satisfaction The answer to the question “How satisfied were you with the final episode of Breaking Throne?”, where 0 is completely dissatisfied and 10 completely satisfied age The age of the respondent female The Gender of the respondent, where 0 = Male, 1 = Female Let us generate the data: #setting seed for reproduciability set.seed(123) #Generating the data final_BT &lt;- data.frame( id = c(1:10), satisfaction = round(rnorm(10, mean = 6, sd = 2.5)), age = round(rnorm(10, mean = 25, sd = 5)), female = rbinom(10, 1, 0.5) ) print(final_BT) ## id satisfaction age female ## 1 1 5 31 0 ## 2 2 5 27 0 ## 3 3 10 27 0 ## 4 4 6 26 0 ## 5 5 6 22 0 ## 6 6 10 34 0 ## 7 7 7 27 0 ## 8 8 3 15 0 ## 9 9 4 29 0 ## 10 10 5 23 1 5.6.1 Exercise 1: Linear Regression with two variables You want to know if age has an impact on the satisfaction with the last episode. You want to conduct a linear regression. a. Calculate \\(\\beta_0\\) and \\(\\beta_1\\) by hand b. Calculate \\(\\beta_0\\) and \\(\\beta_1\\) automatically with R c. Interpret all quantities of your result: Standard Error, t-statistic, p-value, confidence intervals and the \\(R^2\\). d. Check for influential outliers 5.6.2 Exercise 2: Multivariate Regression a. Add the variable female to your regression b. Interpret the Output. What has changed? What stays the same? c. Make an interaction effect between age and gender and interpret it! d. Plot the interaction and make the plot nice "],["loops-and-functions.html", "Chapter 6 Loops and Functions 6.1 Loops 6.2 apply() Function Family 6.3 Writing your own functions 6.4 Outlook 6.5 Exercise Section", " Chapter 6 Loops and Functions In this chapter I want to introduce to a way to work more efficient. R is a programming language for statistical analysis, but it also includes classical elements of programming. Two main operations are loops and functions. We can automate tasks and the earlier you learn about it the faster you can advance and understand the logic of R. The goal of this chapter is to make you familiar with functions and loops so you know them when you see them. 6.1 Loops For example, you can use a loop to iterate through a list of numbers and perform calculations on each number, or to go through the rows of a dataset and apply certain operations to each row. Loops provide a way to streamline your code and avoid writing repetitive instructions manually. There are different type of Loops, but for this course we focus just on the for loops, since you will see them also in the QM Tutorial. 6.1.1 For loops Do you remember my grade example from the first chapter? grade &lt;- 4.0 if (grade == 1.0) { print(&quot;Amazing&quot;) } else if (grade &gt; 1.0 &amp; grade &lt;= 2.0) { print(&quot;Good Job&quot;) } else if (grade &gt; 2.0 &amp; grade &lt;= 3.0) { print(&quot;OK&quot;) } else if (grade &gt; 3.0 &amp; grade &lt;= 4.0) { print(&quot;Life goes on&quot;) } ## [1] &quot;Life goes on&quot; grade &lt;- 3.3 if (grade == 1.0) { print(&quot;Amazing&quot;) } else if (grade &gt; 1.0 &amp; grade &lt;= 2.0) { print(&quot;Good Job&quot;) } else if (grade &gt; 2.0 &amp; grade &lt;= 3.0) { print(&quot;OK&quot;) } else if (grade &gt; 3.0 &amp; grade &lt;= 4.0) { print(&quot;Life goes on&quot;) } ## [1] &quot;Life goes on&quot; grade &lt;- 4.0 if (grade == 1.0) { print(&quot;Amazing&quot;) } else if (grade &gt; 1.0 &amp; grade &lt;= 2.0) { print(&quot;Good Job&quot;) } else if (grade &gt; 2.0 &amp; grade &lt;= 3.0) { print(&quot;OK&quot;) } else if (grade &gt; 3.0 &amp; grade &lt;= 4.0) { print(&quot;Life goes on&quot;) } ## [1] &quot;Life goes on&quot; grade &lt;- 2.3 if (grade == 1.0) { print(&quot;Amazing&quot;) } else if (grade &gt; 1.0 &amp; grade &lt;= 2.0) { print(&quot;Good Job&quot;) } else if (grade &gt; 2.0 &amp; grade &lt;= 3.0) { print(&quot;OK&quot;) } else if (grade &gt; 3.0 &amp; grade &lt;= 4.0) { print(&quot;Life goes on&quot;) } ## [1] &quot;OK&quot; grade &lt;- 1.7 if (grade == 1.0) { print(&quot;Amazing&quot;) } else if (grade &gt; 1.0 &amp; grade &lt;= 2.0) { print(&quot;Good Job&quot;) } else if (grade &gt; 2.0 &amp; grade &lt;= 3.0) { print(&quot;OK&quot;) } else if (grade &gt; 3.0 &amp; grade &lt;= 4.0) { print(&quot;Life goes on&quot;) } ## [1] &quot;Good Job&quot; I could now write down all my grades and assign them as I did in the first chapter, but there is a way to automatize this process. For that I will use the For loop. First, let us make a vector with grades: grades &lt;- c(1.7, 3.3, 4.0, 2.3, 1.0) Now, we can directly dive into the loop. Write down for and in brackets you define the loop iterator, this is the i in the loop. Then you define in which object of interest you want to iterate. In our case, the operation should be iterated in the grades vector. I could also write down the number 5, but it is convention to define an object. Why? After closing the brackets you open fancy brackets and write down your function, as you would normally, but this time you need to define how the iterator is used. Since I use the numbers in grades, my iterator needs to be put in brackets, after the name of the grades. Why? And that’s it basically for (i in 1:length(grades)) { if (grades[i] == 1.0) { print(&quot;Amazing&quot;) } else if (grades[i] &gt; 1.0 &amp; grades[i] &lt;= 2.0) { print(&quot;Good Job&quot;) } else if (grades[i] &gt; 2.0 &amp; grades[i] &lt;= 3.0) { print(&quot;OK&quot;) } else if (grades[i] &gt; 3.0 &amp; grades[i] &lt;= 4.0) { print(&quot;Life goes on&quot;) } } ## [1] &quot;Good Job&quot; ## [1] &quot;Life goes on&quot; ## [1] &quot;Life goes on&quot; ## [1] &quot;OK&quot; ## [1] &quot;Amazing&quot; Loops can look differently: In this example I have a number vector and my let the console print a sentence, where I vary the number and therefore the sentence changes over every loop # creating num vector num &lt;- c(1, 2, 3, 4, 5, 249) # looping through vector for (i in num) { print(stringr::str_c(&quot;This is the &quot;, i, &quot;th Iteration&quot;)) } ## [1] &quot;This is the 1th Iteration&quot; ## [1] &quot;This is the 2th Iteration&quot; ## [1] &quot;This is the 3th Iteration&quot; ## [1] &quot;This is the 4th Iteration&quot; ## [1] &quot;This is the 5th Iteration&quot; ## [1] &quot;This is the 249th Iteration&quot; 6.1.2 Nested Loops Because you will eventually encounter them, I will show you shortly nested loops: First, let us play a game of tic tac toe: #Defining a matrix ttt &lt;- matrix(c(&quot;X&quot;, &quot;O&quot;, &quot;X&quot;, &quot;O&quot;, &quot;X&quot;, &quot;O&quot;, &quot;O&quot;, &quot;X&quot;, &quot;O&quot;), nrow = 3, ncol = 3, byrow = TRUE) We define a loop with an iterator i for the rows of the matrix, and we define another one for the columns with the iterator j. Afterwords, we built up the body, in which aim to get information about the matrix and its content. The sentence shows, which rows and columns contain which values. for (i in 1:nrow(ttt)) { for (j in 1:ncol(ttt)) { print(paste(&quot;On row&quot;, i, &quot;and column&quot;, j, &quot;the board contains&quot;, ttt[i,j])) } } ## [1] &quot;On row 1 and column 1 the board contains X&quot; ## [1] &quot;On row 1 and column 2 the board contains O&quot; ## [1] &quot;On row 1 and column 3 the board contains X&quot; ## [1] &quot;On row 2 and column 1 the board contains O&quot; ## [1] &quot;On row 2 and column 2 the board contains X&quot; ## [1] &quot;On row 2 and column 3 the board contains O&quot; ## [1] &quot;On row 3 and column 1 the board contains O&quot; ## [1] &quot;On row 3 and column 2 the board contains X&quot; ## [1] &quot;On row 3 and column 3 the board contains O&quot; 6.2 apply() Function Family 6.2.1 apply() apply() takes a data frame or matrix as an input and gives output in vector, list or array. Apply function in R is primarily used to avoid explicit uses of loop constructs. The idea is to apply a function repeatedly to a matrix or data frame: apply(X, MARGIN, FUNCTION) #Let us create a matrix with random numbers mat &lt;- matrix(1:10, nrow = 5, ncol = 6) #Checking it head(mat) ## [,1] [,2] [,3] [,4] [,5] [,6] ## [1,] 1 6 1 6 1 6 ## [2,] 2 7 2 7 2 7 ## [3,] 3 8 3 8 3 8 ## [4,] 4 9 4 9 4 9 ## [5,] 5 10 5 10 5 10 Assume you now want to calculate the mean of every column: apply(mat, 2, mean) #calculating mean ## [1] 3 8 3 8 3 8 apply(mat, 2, sum) #calculating sum ## [1] 15 40 15 40 15 40 apply(mat, 2, sd) #calculating sd ## [1] 1.581139 1.581139 1.581139 1.581139 1.581139 1.581139 #The corresponding Loop would look like this: for (i in 1:ncol(mat)) { mean_col &lt;- mean(mat[, i]) print(mean_col) } ## [1] 3 ## [1] 8 ## [1] 3 ## [1] 8 ## [1] 3 ## [1] 8 The apply() function is useful especially if you are working with dimensional bodies and want to calculate anything. However, they can not keep up with the flexibility of loops, you should be aware of that. 6.2.2 Notes Loops and the apply function are widely used in programming. However, this is no programming course, it is an introduction, so now you have an idea what is happening, if you are seeing those two things in the scripts. But if you are interested in this topic, please read into while loops and repeat loops. The apply() function is part of a family: sapply(), lapply(), tapply() are also in that family. 6.3 Writing your own functions We can again safe a lot of time and be more efficient by writing our own function. First, you need to define a name for your function# Afterwards, you write down the command with the function() command. In to your brackets you put your variables. Later your input follows those variables. After the fancy brackets, you define your operation with your predefined variables. Lastly, you want the function to return your quantity of interest and close the fancy brackets Afterwards you have a function saved and can operate with it #My function is just a sum add &lt;- function(x, y) { result &lt;- x + y return(result) } add(2,7) #Now I can use my function ## [1] 9 Let us calculate the area of a circle aoc &lt;- function(radius) { pi &lt;- 3.14159 area &lt;- pi * radius^2 return(area) } aoc(5) ## [1] 78.53975 Let us combine what we have learned in this chapter with a meeting of animals. You do not have to understand the code, and I also do not want to explain it, but just to make clear what loops and functions are able to do. And all packages are written based on functions and loops, so it is useful to get an overview how it could look like. In this loop, we let R print out what persons in a classroom are doing. Before that we identify where the individuals are sitting. And the seating order in a class, is nothing more that a matrix right? We have rows and columns. This loops identifies in which row (i) and in which column (j) an individual sits. Moreover, the values in the matrix are student ids and the loop knows the id of students and prints out where which student sits. So this is an unnecessary loop, but I think a good example. Imagine the class are 1000 of people, then this loop would be necessary. As I said you do not need to completely understand it. Its purpose is to illustrate what you can do in R. # The function print_classroom &lt;- function(x) { for (i in 1:length(x)) { # Outer loop iterates over rows for (j in 1:length(x[[i]])) { # Inner loop iterates over columns student &lt;- x[[i]][j] if (student == 1) { comment &lt;- &quot;Alice&quot; } else if (student == 2) { comment &lt;- &quot;Bob&quot; } else if (student == 3) { comment &lt;- &quot;Cathy&quot; } else if (student == 4) { comment &lt;- &quot;David&quot; } else if (student == 5) { comment &lt;- &quot;Eva&quot; } else { comment &lt;- paste(&quot;Unknown student&quot;, student, &quot;is doing something interesting.&quot;) } cat(&quot;At row&quot;, i, &quot;column&quot;, j, &quot;:&quot;, comment, &quot;\\n&quot;) } } } # Example usage seating_order &lt;- list( c(1, 5, 2), c(4, 3, 7) ) #Checking it print_classroom(seating_order) ## At row 1 column 1 : Alice ## At row 1 column 2 : Eva ## At row 1 column 3 : Bob ## At row 2 column 1 : David ## At row 2 column 2 : Cathy ## At row 2 column 3 : Unknown student 7 is doing something interesting. 6.4 Outlook This course was a short introduction to automatize work and write efficient code: Loops and Functions. Since both concepts can get really complicated really fast, and in the beginning of your R-journey you will be less likely to use loops and functions. At some point however will you be confronted with loops and functions. And the earlier you see them, the better. For further information on programming in R, I recommend “Hands-On Programming with R” by Garret Grolemund. In this book, the authors shows projects where he uses functions and loops, so it is a nice illustration of the usage of loops and functions. 6.5 Exercise Section 6.5.1 Exercise 1: Writing a loop Write a for loop that prints the square of each number from 1 to 10 #Assigning an object for a better workflow number &lt;- 10 #The Loop 6.5.2 Exercise 2: Writing a function Write a function that takes the input x and squares it: #Defining a function for squaring sq &lt;- function (x) { } #Defining a vector containing a vector from 1 to 10 numbers &lt;- c(1:10) #Applying the number sq(numbers) 6.5.3 Exercise 3: The midnight Formula This is the midnight formula separated in two equations: \\(x_{1,2} = \\frac{-b \\pm \\sqrt{b^2 - 4ac}}{2a}\\) Make one function for the midnight formula, so the output are \\(x_1\\) and \\(x_2\\). Test it with a = 2, b = -6, c = -8 Hint: You need two split up the formula into two equations with two outputs. mnf &lt;- mnf(2, 6, 8) "],["further-explanations.html", "Chapter 7 Further Explanations 7.1 Probability Theory 7.2 Regression Diagnostics", " Chapter 7 Further Explanations In this Chapter we start into the main strength of R: Data Analysis. After cleaning our data we are ready to analyze it, but it is always recommend to get an overview of our data with exploratory data analysis. This yields the advantage that we can detect first trends and potential problems with our data. Furthermore, we will get a better understand of the Data Generating Process and how data is generated. Let us tart with loading packages and you will see, that we are loading a lot of packages due to the fact, that R is a statistical analysis software and therefore it is only natural that Statisticians and Data Scientists, who use R, are also writing packages to (1) make their own life easier, but also (2) further developing R. pacman::p_load(&quot;dplyr&quot;, &quot;tidyr&quot;, &quot;ggpubr&quot;, &quot;gapminder&quot;, &quot;kableExtra&quot;, &quot;car&quot;) 7.1 Probability Theory Before diving into Exploratory Data Analysis and Linear Regression, it’s worth taking a moment to look at Probability Theory. This is crucial because probability forms the foundation of statistics: it allows us to make statements that include uncertainty. Instead of saying something definite like “It will rain tomorrow,” probability allows us to say, “There is a 70% chance of rain tomorrow.” That means you might want to take an umbrella — but there’s still a 30% chance you won’t need it. 7.1.1 Random Variable We are interested in Data! And Data is nothing more than a bunch of Random Variables. Technically speaking, a Random Variable is a numerical outcome of a random process or experiment. Informally, let us say you want to collect Data about your family, just image all of them and now randomly pick some information about them, their height, their age, gender, glasses or not, cooking abilities, anything, and now you just need to find a system of assigning a number (e.g. height in cm, age in years,…) and et violà you collected a random variable. Note that a random variable is the mutually exclusive outcome of a random process. Rolling a dice and getting the result of the dice could be a random variable, let us role the dice and say we got a 3: #rolling one dice dice_role &lt;- 3 print(dice_role) ## [1] 3 We now collected our first random variable. On its own, a random variable is not really insightful, but what if we do not collect one random variable, but two? Or three? Or thousand? 7.1.1.1 Discrete and Continuous Variables Before answering the question above, there is an important distinction to make between the types of random variables we can collect. There are discrete and continuous variables: A discrete variable can take on a finite or countably infinite set of distinct values, usually integers. These values are separated and cannot take on values in between. For example: Number of children, coin flips, number of goals. A continuous variable can take on any value within a given range, including fractions and decimals. Between any two values, there are infinitely many possible values. For example: Height, weight, temperature, time. 7.1.1.2 Population vs Sample Distribution Another thing to clarify before going on with Probability Distributions, is the difference between Populations and Samples. Population: The population is the entire group we are theoretically interested in studying, based on our research question. For example, if we want to know the average height of German women, then the population consists of all German women. The population mean is the true average height of all German women. Sample: Asking every German woman (42.3 million, according to the Federal Office of Statistics, 2023) would indeed be impractical. Instead, we use statistical sampling to gather data from a smaller, more manageable subset of the population. Probability theory helps ensure that this sample is representative of the entire population. For the sample to be representative of the population, it needs to meet two key criteria: Sample Size: The sample must be large enough to accurately reflect the population’s characteristics. Randomness: Every individual in the population must have an equal chance of being selected for the sample. This ensures that the sample is random and not biased in any way. When these conditions are met, we can confidently generalize the findings from the sample to the larger population. For example, we calculate the sample mean (the average of the sample). This is the true value for the sample itself, but we use it to make an inference about the population mean. However, the sample mean will not always be exactly equal to the population mean. This is because of sampling variability — there’s always some random variation in the sample that can cause deviations from the population mean. Despite this, if the sample is large enough and chosen randomly, the sample mean will tend to be close to the population mean on average, and the difference between them decreases as the sample size increases. The difference of population and sample is crucial and will be important in the later section of this chapter! 7.1.2 Probability Distributions 7.1.2.1 Probability Mass Functions (PMF) 7.1.2.1.1 Uniform Distributions We can let R roll the dice for us using the sample() function. This function takes in the range of possible outcomes, the number of observations to draw, and some additional options (which we won’t worry about for now): dice_rolls &lt;- data.frame( roll = sample(c(1:6), 1000, replace = TRUE) ) This simulates rolling a fair six-sided die 1,000 times. Let us plot it: # Plot with ggplot ggplot(dice_rolls, aes(x = factor(roll))) + geom_bar(fill = &quot;#89CFF0&quot;, color = &quot;gray&quot;) + labs( title = &quot;Distribution of 1,000 Dice Rolls&quot;, x = &quot;Dice Face&quot;, y = &quot;Frequency&quot; ) + theme_minimal() What we’ve created here is called a distribution of a discrete variable. A discrete variable has a countable number of distinct outcomes. In this case, our die can only land on one of six values: 1 through 6 — nothing in between. This explicit type of distribution is called uniform distribution. A uniform distribution is one in which all outcomes are equally likely — in our case, each face of the die (1 through 6) has the same probability of occurring. Thus, the probability of one outcome is just 1/n, thus 1/6 for each outcome. Let us plot that as well: # Define the outcomes and their corresponding probabilities uniform_dis &lt;- data.frame( Outcome = factor(1:6), Probability = rep(1/6, 6) ) # Plot it ggplot(uniform_dis, aes(x = Outcome, y = Probability)) + geom_point() + labs( title = &quot;Uniform Probability Distribution of a Fair Die&quot;, x = &quot;Die Face&quot;, y = &quot;Probability&quot; ) + theme_minimal() This a so-called Probability Mass Function (PMF). This is the term for probability distributions of discrete variables. It is useful since we can determine the probability of an event with it, for example the probability of getting 3: \\[ P(X = 3) = \\frac{1}{6} \\] What if we start to cumulate the probabilities, like in the following table: Outcome 1 2 3 4 5 6 Probability 1/6 1/6 1/6 1/6 1/6 1/6 Cumulative Probability 1/6 2/6 3/6 4/6 5/6 1 We can plot the cumulative Probability by calculating it with the cumsum() function: # Calculating the cumsum uniform_dis$cumsum &lt;- cumsum(uniform_dis$Probability) # Plot it ggplot(uniform_dis, aes(x = Outcome, y = cumsum)) + geom_point() + labs( title = &quot;Cumulative Distribution of a Fair Die&quot;, x = &quot;Die Face&quot;, y = &quot;Probability&quot; ) + ylim(0,1) + theme_minimal() With this graph we can directly see the cumulative distribution for several outcomes, for example the cumulative probability for the outcome 3 is 0.5, thus 50%, so with a 50% probability I will get either 1, 2 or 3. 7.1.2.1.2 Bernoulli Distributions In this part I want to introduce you a new type of distribution: The bernoulli distribution. It is super easy it takes on the value 1 with probability \\(p\\) and the value 0 with the probability \\(q = 1 -p\\). In more simple terms, it displays the outcomes for any experiment that has yes/no answers. The classical example is head and tails, you throw a coin and it either shows head or tails. Let us simulate data and plot it: # Simulate 1000 tosses of a fair coin (1 = Head, 0 = Tail) set.seed(101) # for reproducibility fair_coin &lt;- data.frame( outcome = factor(rbinom(10, 1, 0.5), levels = c(0, 1), labels = c(&quot;Tail&quot;, &quot;Head&quot;)) ) # Plot the results ggplot(fair_coin, aes(x = outcome)) + geom_bar(fill = &quot;#89CFF0&quot;, width = 0.35) + labs( title = &quot;Simulation of 10 Tosses of a Fair Coin&quot;, x = &quot;Outcome&quot;, y = &quot;Count&quot; ) + theme_minimal() As we probably expected, if we throw a fair coin ten times we will get approximately 5 Tails and 5 Heads. Furthermore, we plotted our first Bernoulli Distribution. This is an example for a fair coin, what if our coin was “not fair” or “unfair”, thus a “biased” coin, we could plot that as well: # Simulate 1000 tosses of an unfair coin (1 = Head, 0 = Tail) set.seed(102) # for reproducibility unfair_coin &lt;- data.frame( outcome = factor(rbinom(10, 1, 0.28), levels = c(0, 1), labels = c(&quot;Tail&quot;, &quot;Head&quot;)) ) # Plot the results ggplot(unfair_coin, aes(x = outcome)) + geom_bar(fill = &quot;#89CFF0&quot;, width = 0.35) + labs( title = &quot;Simulation of 1000 Tosses of an unfair Coin&quot;, x = &quot;Outcome&quot;, y = &quot;Count&quot; ) + theme_minimal() We can see that the bins are not equal the coin has a tendency for Tail. The Probability for getting Tail is higher than getting Head with this Unfair Coin. How does the Probability Distribution of a Bernoulli Distribution looks like? Before answering this question, we have to calculate the probability of getting exact 7 Tails and 3 Heads as we got. I leave the maths out, thankfully R can do that with the dbinom() function, it takes in “x” which is the number of outcomes we got, thus our Tail = 7. Then we have to put in the size, which is the number of trials, thus 10 and lastly the probability, if it is a fair coin 0.5. I computed the biased coin with a probability of 0.28. Let us calculate for the fair and the biased coin, the probability of our results: # fair coin dbinom( x = 5, size = 10, prob = 0.5 ) ## [1] 0.2460938 # biased coin dbinom( x = 3, size = 10, prob = 0.28 ) ## [1] 0.2642304 What you can see here is the probability of getting exact 5 heads for the fair coin and 3 heads for the unfair coin. So to get exact 5 heads with the fair coin has a probability of 24.6% and the probability of getting exact 3 heads has a probability of 26.4%. 7.1.2.1.3 Binomial Distribution How does it come, that although we have compute a probability of 50% of a success in each trial, that we only get a probability of 5 heads with 24.6% probability and 26.4% of getting 3 heads with a biased coin? We can solve this problem by plotting a so-called Binomial Distribution. It displays the number of successes in a fixed number of independent yes/no, thus head and tails (Bernoulli) trials. It gets clear, when we plot all Probabilities for both coins for each possible outcome for head, thus 10, since we have 10 trials: # Create theoretical distributions theoretical_probs &lt;- data.frame( heads = rep(0:10, 2), coin_type = rep(c(&quot;unbiased&quot;, &quot;biased&quot;), each = 11), prob = c(dbinom(0:10, size = 10, prob = 0.5), dbinom(0:10, size = 10, prob = 0.28)) ) # Plot ggplot(theoretical_probs, aes(x = heads, y = prob)) + geom_point(size = 1.5, color = &quot;black&quot;) + facet_wrap(~ coin_type, labeller = as_labeller(c( biased = &quot;Biased Coin (p = 0.28)&quot;, unbiased = &quot;Unbiased Coin (p = 0.5)&quot;))) + labs(title = &quot;Theoretical Binomial Distribution of Number of Heads in 10 Flips&quot;, x = &quot;Number of Heads&quot;, y = &quot;Probability&quot;) + scale_x_continuous(breaks = 0:10) + theme_bw() Now, the reason behind the probabilities for the outcome for heads becomes clear, the computed probability deviates from the outcome probability because in some cases the trials follow the computed probability but in some cases it deviates and head becomes 2 or 4 for the biased coin and 4 or 6 for the unbiased coin. At this point, one thing becomes clear: the computed probability represents the expected likelihood of obtaining a specific number of heads on average over a large number of repeated experiments. Why “on average”? Because probability theory accounts for randomness — meaning that in any single simulation (say, flipping a coin 10 times), we might observe different outcomes: sometimes 5 heads, sometimes 2, or even 9. For a fair coin, the theoretical expectation is 5 heads in 10 flips. However, this doesn’t mean we will always get exactly 5. The actual results vary due to chance. In the case of a biased coin with a 0.28 probability of heads, we might expect around 2.8 heads on average — but again, we will only ever see whole numbers like 2 or 3 in practice. In short, these deviations from the expected value arise from randomness. While the true probability (e.g., 0.5 or 0.28) describes the long-term pattern, it does not guarantee specific outcomes in short runs. Instead, it tells us what to expect most frequently over a large number of repetitions. This concept will become important later again, but before that let us have a look at the cumulated distribution function (CDF) of the probability distribution function (PDF): # Fix cumulative sum: compute it within each coin_type group theoretical_probs &lt;- theoretical_probs %&gt;% group_by(coin_type) %&gt;% arrange(heads, .by_group = TRUE) %&gt;% mutate(cumsum_prob = cumsum(prob)) %&gt;% ungroup() # Plot cumulative probabilities ggplot(theoretical_probs, aes(x = heads, y = cumsum_prob)) + geom_point(size = 1.5, color = &quot;black&quot;) + facet_wrap(~ coin_type, labeller = as_labeller(c( biased = &quot;Biased Coin (p = 0.28)&quot;, unbiased = &quot;Unbiased Coin (p = 0.5)&quot;)), nrow = 2) + labs( title = &quot;Cumulative Distribution of Number of Heads in 10 Flips&quot;, x = &quot;Number of Heads&quot;, y = &quot;Cumulative Probability&quot;) + scale_x_continuous(breaks = 0:10) + theme_bw() The CDF is quite interesting, we see that in round about 70% of all experiments of the biased coin will result in 3 or less heads and on the other side the unbiased coin will result in round about 80% in 6 or less heads. 7.1.2.2 Probability Density Functions (PDF) 7.1.2.2.1 Normal Distributions Thus far we looked at the two typical distributions for discrete variables. In the following, we will look at distributions for continuous variables. The probability distributions of the Bernoulli trials we saw in the part follow the form of the arguably most important distribution: The normal distribution. This distribution is also called a Gaussian distribution after its inventor Carl-Friedrich Gauss, or Bell Curve due to its shape. Let us have a look at it at the Probability Distribution Function (PDF): # simulating the data set.seed(123) snd &lt;- data.frame( sample = rnorm(1000000, mean = 0, sd = 1) ) # Plot it ggplot(snd, aes(x = sample)) + geom_density() + labs(title = &quot;Normal Distribution&quot;, x = &quot;Value of Random Variable&quot;, y = &quot;Density&quot;) + scale_x_continuous(breaks = -5:5, limits = c(-5,5)) + theme_minimal() The normal distribution has a lot of properties: The distribution is perfectly symmetrical around the mean The mean equals the median equals the mode The shape is bell-like, with most values clustered around the mean and tails tapering off in both directions. About 68% of the data fall within 1 standard deviation of the mean, about 95% fall within 2 standard deviations and around 99.7% fall within 3 standard deviations. The distribution is defined by its mean (\\(\\mu\\)) and its standard deviation (\\(\\sigma\\)). The tails extend infinitely in both directions but never touch the x-axis (approach zero asymptotically). The area under the density curve represents probability, and it sums up to exactly 1. That is the reason the reason why we talk about Probability Density Functions, because we can determine the probability for each data point by calculating the density of the curve. It is unimodal, thus has only one peak. As Standard Normal Distributions is a special case where the mean, \\(\\mu\\) = 0 and the standard deviation, \\(\\sigma\\) = 1 We already saw the Cumulative Density Function in the Bernoulli Trials, but let us have a look again: # Plot empirical CDF ggplot(snd, aes(x = sample)) + stat_ecdf(geom = &quot;step&quot;, color = &quot;black&quot;) + labs(title = &quot;Cumulative Distribution Function (CDF)&quot;, x = &quot;Value of Random Variable&quot;, y = &quot;Cumulative Probability&quot;) + theme_minimal() We can use the cumulative distribution function (CDF) to determine the probability that a random variable takes on a value less than or equal to a specific value. For example, with a standard normal distribution, the CDF tells us there’s a 50% chance that the random variable will take on a value less than or equal to 0. 7.1.2.2.2 Central Limit Theorem (CTL) In the examples of the Chapter 7.1.3 we will use normal distributions due to its significance for natural and social sciences, here is why: When we collect random variables such as height, IQ, sizes of snowflakes, milk production of cows follows a normal distribution. Due to its elegant mathematical properties like the symmetry around the mean, the 68-95-99.7 rule, and it is defined by only two parameters, the mean and the standard deviation. Statistical Analysis assumes in many models a normal distributions, because its simplicity, for example the it is the building block for machine learning, finance, etc., and for statistical models such as regression analysis assumes also normality. But besides these, let’s say “convenient reasons”, there is also one mathematical reason why it is so frequently used in statistics, the central limit theorem. I will not dive into its maths, but it is quite easy to understand: The Central Limit Theorem states that, under certain conditions, the distribution of the sample means of a large number of independent and identically distributed random variables will approximate a normal distribution—regardless of the shape of the original population distribution. In other words, as the sample size increases, the distribution of the means of these samples becomes increasingly normal. Let us visualize that! Remember the sample of our dice? What if we draw many, many samples and collect their sample means, and then plot the sample means? We would expect a uniform distribution, right? Because every outcome has the same probability to happen, exactly 1/6? Let us draw samples and look what happens. In the following I first define a function that rolls the dice 1000 times. In the next step, I use the function to draw a sample. I first take one sample, thus role the dice 1000 times and save the result, and plot it, I do the same for 2, 3, and 4 samples: set.seed(123) # For reproducibility # Function to simulate rolling a die n_rolls times, repeated n_sim times simulate_dice_means &lt;- function(n_rolls, n_sim = 1000) { replicate(n_sim, mean(sample(1:6, n_rolls, replace = TRUE))) } # Simulate sample means for different numbers of rolls means_1 &lt;- simulate_dice_means(1) means_2 &lt;- simulate_dice_means(2) means_3 &lt;- simulate_dice_means(3) means_4 &lt;- simulate_dice_means(4) df &lt;- bind_rows( data.frame(mean = means_1, rolls = &quot;1000 Rolls&quot;), data.frame(mean = means_2, rolls = &quot;2000 Rolls&quot;), data.frame(mean = means_3, rolls = &quot;3000 Rolls&quot;), data.frame(mean = means_4, rolls = &quot;4000 Rolls&quot;) ) # Plotting ggplot(df, aes(x = mean)) + geom_density(fill = &quot;skyblue&quot;, alpha = 0.6) + facet_wrap(~ rolls, scales = &quot;free&quot;, ncol = 2) + labs(title = &quot;Central Limit Theorem Demonstration with Dice Rolls&quot;, x = &quot;Sample Mean&quot;, y = &quot;Density&quot;) + scale_x_continuous(breaks = 1:6, limits = c(1,6)) + theme_minimal() Here we can clearly see how the distribution of the sample means converge to the form of a normal distribution. That leaves only one question, why does the mean of the sample means converge to 3.5? 7.1.2.2.3 Law of Large Numbers The phenomenon that the sample converges to 3.5 is due to the Law of large numbers. It is a mathematical law that states that if the sample size increases, the sample mean gets closer population mean. The true population mean for our dice example is 3.5 (1+2+3+4+5+6/6 = 3.5), thus our expected value is our true population mean. According to the Law of large numbers, the more the dice is rolled the more the sample mean converges to 3.5 and that is what we see in the density plot in the bottom, right corner. When the dice is rolled 4000 times in total the curve gets closer to the sample mean. 7.1.2.3 Different Types of Distributions Alright, that was a lot of theory, but it is crucial to understand, because probability distributions are the basic ground for the statistical models to analyse our data. And here is the deal, knowing the distributions of our data leads to configuration possibilities later in the models, so that our models can better fit our data! In the next chapter, I hope that will become clear! To conclude this chapter, I will briefly introduce you in different types of distributions and their interpretations: # Create a data frame with different distributions x_vals &lt;- seq(0, 10, by = 0.1) dist_df &lt;- data.frame( x = rep(x_vals, 4), # only 4 now, Poisson removed from here distribution = rep(c(&quot;Exponential (λ=1)&quot;, &quot;Gamma (shape=2, rate=1)&quot;, &quot;Chi-Square (df=3)&quot;, &quot;t-Distribution (df=10)&quot;), each = length(x_vals)), y = c( dexp(x_vals, rate = 1), dgamma(x_vals, shape = 2, rate = 1), dchisq(x_vals, df = 3), dt(x_vals - 5, df = 10) # shift t-distribution for better display ) ) # Add Poisson and F-distribution poisson_vals &lt;- data.frame( x = 0:10, y = dpois(0:10, lambda = 3), distribution = &quot;Poisson (λ=3)&quot; ) f_vals &lt;- data.frame( x = x_vals, y = df(x_vals, df1 = 5, df2 = 10), distribution = &quot;F-Distribution (df1=5, df2=10)&quot; ) # Combine all distributions plot_df &lt;- bind_rows(dist_df, poisson_vals, f_vals) # Plot using facet_wrap ggplot(plot_df, aes(x = x, y = y)) + geom_line(data = filter(plot_df, !distribution %in% c(&quot;Poisson (λ=3)&quot;)), color = &quot;steelblue&quot;, linewidth = 1) + geom_point(data = filter(plot_df, distribution == &quot;Poisson (λ=3)&quot;), color = &quot;steelblue&quot;, size = 1) + facet_wrap(~ distribution, scales = &quot;free&quot;, ncol = 3) + labs(title = &quot;Overview of Common Statistical Distributions&quot;, x = &quot;x&quot;, y = &quot;Density / Probability&quot;) + theme_minimal(base_size = 14) Let us go shortly through these distributions and their meanings: Chi-Square: This distribution arises when squaring standard normal values. It’s fundamental in hypothesis testing (e.g., the Chi-Square Test for independence) and constructing confidence intervals for population variances. Exponential: It describes the time between two independent events in a Poisson process. Essential for survival analysis and modeling waiting times. F-Distribution: It describes the ratio of two sample variances, each from normally distributed populations. Used in comparing model fits. Gamma: A flexible distribution that models the waiting time until multiple independent events occur. It generalizes the exponential distribution and is widely used in Bayesian statistics, reliability engineering, and insurance modeling. Poisson: Models the number of events occurring in a fixed time or space interval when events happen independently at a constant average rate—like the number of emails per hour. Commonly used for count data and in event-based modeling. t-Distribution: Used when estimating the mean of a normally distributed population in small samples. It accounts for the added uncertainty from estimating the standard deviation. Essential in hypothesis testing and regression analysis. 7.1.3 Working with Distributions After the fairly theoretical part before, let us work with distributions hands-on in R: For this purpose let us simulate data. For the following part, I will simulate a normal distribution and introduce you to the first central function rnorm(). This functions takes in three arguments, the number of observations n, the mean and the standard deviation, sd. And again, the beauty of the normal distribution becomes clear, we do not need to tell R more than that, since normal distributions can be generated with only the mean and the standard deviation and on top we can define the number of observations. We will simulate the normal distribution for IQ. Let the mean be 100 and the standard deviation 15. These are not randomly selected, most samples of the IQ result in this mean and sd: #Setting seed for reproduciability set.seed(123) #Simulating sample sample_iq &lt;- data.frame( height = rnorm(10000, mean = 100, sd = 15)) #Plotting it ggplot(sample_iq, aes(x=height)) + geom_density(linewidth = 1, color = &quot;#E35335&quot;) + labs( x = &quot;IQ&quot;, y = &quot;Frequency&quot; ) + scale_x_continuous(breaks = seq(40, 160, 20), limits = c(40, 160)) + theme_minimal() 7.1.3.1 The 68-95-99.7 Rule Remember this Rule? Without any complex mathematics, we can determine, where the most values are located, just by by subtracting the standard deviation from the mean: 68% of all values falls within one standard deviation of the mean: \\[ \\mu + \\sigma² = 100 + 15 = 115 \\] \\[ \\mu - \\sigma² = 100 + 15 = 85 \\] We can interpret these values as follows, when your IQ falls between 115 and 115 you share the fate of 68% of our sample. 95% of all values falls within two standard deviations of the mean: \\[ \\mu + 2\\sigma² = 100 + 2 \\cdot 15 = 130 \\] \\[ \\mu - 2\\sigma² = 100 - 2 \\cdot 15 = 70 \\] If an IQ is not within 70 and 130 then it is either in the 2.5 % lowest or 2.5 highest values. An IQ smaller than 70 means being among the 2.5% lowest IQs and on the other side, if the IQ is above 130 than your IQ is higher than 97.5% ! 99.7% of all values falls within three standard deviations of the mean: \\[ \\mu + 3\\sigma² = 100 + 3 \\cdot 15 = 145 \\] \\[ \\mu + 3\\sigma² = 100 - 3 = 55 \\] If the IQ value is outside of 145 or 55, the value is either at the top 0.15 or if it is below 55 it is at the bottom 0.15. 7.1.3.2 Determining the probability of single values? The 68-95-99.7 rule is a helpful rule of thumb for understanding normal distributions. However, it’s not very precise. If we want to determine the exact probability of a value in our distribution, we need to use the Probability Density Function (PDF) of the normal distribution. Now, because a continuous distribution has infinitely many possible values, the probability of observing any exact value (like exactly 100) is actually zero. That’s why we work with probability densities, not exact probabilities at a single point. Instead of asking “What is the probability of exactly 100?”, we ask “What is the density at 100?” — which reflects how likely it is to observe values around 100. In R, we can use the dnorm() function to compute this density. It takes three arguments: x: the value we’re interested in mean: the mean of the distribution sd: the standard deviation Let us calculate the probability of having an IQ of exactly 100, 87 and 140: dnorm(x = 100, mean = 100, sd = 15) #Probability of 100 IQ ## [1] 0.02659615 dnorm(x = 87, mean = 100, sd = 15) #Probability of 65 IQ ## [1] 0.0182691 dnorm(x = 140, mean = 100, sd = 15) #Probability of 135 IQ ## [1] 0.0007597324 So the probability of having an IQ of 100 is 2.6%, getting an IQ of 87 is 1.8% and lastly the probability of having an IQ of 140 is 0.07%. 7.1.3.3 Getting the probabilities until a certain value The function pnorm() in R gives the cumulative probability up to a certain value under the normal distribution. In simpler terms, it tells you how likely it is that a value drawn from a normal distribution is less than or equal to a given number. It takes the following arguments: q: the value you are interested in (the quantile) mean: the mean (μ) of the distribution sd: the standard deviation (σ) of the distribution pnorm(q = 100, mean = 100, sd = 15) #Probability of 100 IQ ## [1] 0.5 pnorm(q = 87, mean = 100, sd = 15) #Probability of 65 IQ ## [1] 0.1930623 pnorm(q = 140, mean = 100, sd = 15) #Probability of 135 IQ ## [1] 0.9961696 Now what do those values mean? The interpretation is quite easy, 0.5 means that an IQ of 100 is higher than 0.5, thus 50% of our distribution has lower values than 100. In simpler terms. If a person has an IQ of 100, the person has an IQ smarter than 50% of all other people. If a person has an IQ of 87, the person has an IQ higher than 19.3% of the sample/population. For an IQ of 140 we can say that the person has a higher IQ than 99.6% of all other persons in the sample. In other ways, with an IQ of 140, a person is part of the top 1%. What if we want to know which value we need, to be in a certain percentile? Well, for this case R has the qnorm() function. It takes in the argument p, which takes probability, we want the corresponding value and of course our two parameters mean and standard deviation qnorm(p = 0.5, mean = 100, sd = 15) #Probability of 100 IQ ## [1] 100 qnorm(p = 0.1930623, mean = 100, sd = 15) #Probability of 65 IQ ## [1] 87 qnorm(p = 0.9961696, mean = 100, sd = 15) #Probability of 135 IQ ## [1] 140 We see that we get our estimated values before. This function is especially useful when setting thresholds (e.g., what IQ is needed to qualify as a “genius”) or determining cutoffs in standardized testing. 7.1.3.4 Conclusion This concludes our chapter on probability theory. We explored different types of variables, learned about Probability Mass Functions (PMFs) and Probability Density Functions (PDFs), and finally saw how to work with probability distributions in R. At first glance, this might seem like basic theory. However, a solid understanding of distributions is essential: it allows us to detect and correct mistakes when building statistical models, which often rely on distributional assumptions. Moreover, understanding distributions shows us why statistical analysis works in the first place—even in the presence of uncertainty. It allows us to assign probabilities to predictions, interpret results meaningfully, and quantify how likely certain outcomes are. 7.2 Regression Diagnostics 7.2.1 Model Fit: Bivariate Regression Now the linear regression has a lot of assumptions, it is not like we can run the model every time how we want. Since it is a model, it makes assumptions and instead of just assuming them to be right, we can test them. To techniques to test them are called Measures of Fit. Because they test how much our data fits the data. Let us have a look at the assumptions and how we can test them: 7.2.1.1 Measures of Fit 7.2.1.1.1 Residuals 1.) Calculating Residuals: \\[ Residuals = y_i - \\hat{y_i} = y_i - (\\hat{\\beta_0} - \\hat{\\beta_1}x_i) \\] where \\(y_i\\) = our actual observed values of our dependent variable (df$y) \\(\\hat{y_i}\\) = are our predicted values based on our OLS estimator Reconsider the graph at 2.2.1, the residuals are basically the red lines, thus the distance from the line to the point. We can calculate the residuals for our graph: #Getting the data set.seed(123) # For reproducibility n &lt;- 30 x &lt;- runif(n) * 10 categorical_variable &lt;- factor(sample(c(0, 1), n, replace = TRUE)) y &lt;- 0.8 + 1.6 * x + rnorm(n, 0, 3) df &lt;- data.frame(x,y, categorical_variable) # And or Model #running a linear regression model1 &lt;- lm(y ~ x, data = df) #First, we calculate the predictions for y df$y_hat &lt;- 1.6821 + 1.5394*df$x #We get the Residuals by subtracting our actual y from y_hat df$residuals &lt;- df$y - df$y_hat #cheking it head(df) ## x y categorical_variable y_hat ## 1 2.875775 6.680633 0 6.109068 ## 2 7.883051 12.527668 1 13.817269 ## 3 4.089769 10.029008 0 7.977891 ## 4 8.830174 17.562679 1 15.275270 ## 5 9.404673 18.312220 1 16.159653 ## 6 0.455565 3.594825 0 2.383397 ## residuals ## 1 0.5715646 ## 2 -1.2896015 ## 3 2.0511170 ## 4 2.2874090 ## 5 2.1525664 ## 6 1.2114280 #We could have done that automatically with R as well ! df$residuals_auto &lt;- residuals(model1) #Checking it head(df) ## x y categorical_variable y_hat ## 1 2.875775 6.680633 0 6.109068 ## 2 7.883051 12.527668 1 13.817269 ## 3 4.089769 10.029008 0 7.977891 ## 4 8.830174 17.562679 1 15.275270 ## 5 9.404673 18.312220 1 16.159653 ## 6 0.455565 3.594825 0 2.383397 ## residuals residuals_auto ## 1 0.5715646 0.5716538 ## 2 -1.2896015 -1.2892991 ## 3 2.0511170 2.0512579 ## 4 2.2874090 2.2877518 ## 5 2.1525664 2.1529337 ## 6 1.2114280 1.2114141 Let us have a look at a so-called Residual Plot: On the x-axis you plot the fitted values, thus our y_hat. On the y-axis you plot the residuals, thus \\(y-\\hat{y}\\). Then you plot a horizontal line at y = 0. All dots on those lines show us the values correctly predicted by our model. ggplot(df, aes(x, residuals_auto)) + geom_point() + geom_hline(yintercept = 0) + scale_y_continuous(&quot;Residuals&quot;, seq(-6, 6, 1), limits = c(-6, 6)) + scale_x_continuous(&quot;Fitted Values&quot;, seq(0, 10, 1), limits = c(0, 10)) + theme_bw() 7.2.1.1.2 Homoskedasticity and Heteroskedasticity One assumption of linear regression is that the variance of the error term is not correlated with our independent variable. Well, that is quite technocratic and means basically, that the residuals are distributed equally over the independent variables. Let us plot it to get a visual intuition: #setting seed for reproduciability set.seed(123) # Generate some data x &lt;- runif(150, 0.05, 1) e &lt;- rnorm(150, 0, 0.5) #homoskedastic data y_homo &lt;- 2 * x + e #heteroskedastic data y_hetero &lt;- 2 * x + e*x^2 #making a data frame with both data df_homo_hetero &lt;- data.frame(x, y_homo, y_hetero) # Scatterplot with homoscedasticity homoskedastic_plot &lt;- ggplot(df_homo_hetero, aes(x = x, y = y_homo)) + geom_point() + geom_smooth(method = &quot;lm&quot;, se = FALSE) +# Add linear regression line scale_y_continuous(&quot;Y&quot;, seq(-0.5, 3.5, 0.5), limits = c(-0.5, 3.5)) + labs(title = &quot;Homoskedastic Plot&quot;) + theme_minimal() # Scatterplot with heteroscedasticity heteroskedastic_plot &lt;- ggplot(df_homo_hetero, aes(x = x, y = y_hetero)) + geom_point() + geom_smooth(method = &quot;lm&quot;, se = FALSE) +#Add linear regression line labs(title = &quot;Heteroskedastic Plot&quot;) + scale_y_continuous(&quot;Y&quot;, seq(-0.5, 3.5, 0.5), limits = c(-0.5, 3.5)) + theme_minimal() # Combine plots using facet_wrap facet_plots &lt;- ggarrange(homoskedastic_plot, heteroskedastic_plot, nrow = 1) # Print the combined plots print(facet_plots) In the left plot, you see the homoskedastic data. The dots are equally and constantly distributed around the fitted line. However, the right plot shows that the more the independent variable x increases, the more the observations are increasing. The dots are not constantly distributed over the line. In the case, that the data is heteroskedastic, then this is a problem. You could try to transform the independent variable by taking the logarithm (We will look into that later). You could also use so-called heteroskedastic regression, but this an advanced model. 7.2.1.2 TSS, ESS and \\(R^2\\) Now that we have the Residuals, we can calculate the Total Sum of Square (TSS), the Explained Sum of Squared ESS, and \\(R^2\\): TSS (Variation in the DV): \\(TSS =\\sum(y_i - \\bar{y})^2\\) , we just subtract our actual values (df$y) from its mean and square it to avoid negative numbers. This gives us the total variation of our dependent variable. ESS (Variation we explain in the DV): \\(ESS = \\sum(\\hat{y_i} - \\bar{y})^2\\) , now we use our predicted values (df$y_hat) instead of our actual values. That gives us the variation in the dependent variable, we can explain with our model. \\(R^2\\) (The Variation we can predict from our model): \\(R^2 = \\frac{ESS}{TSS}\\) , well to get the proportion we just divide the variation we can explain from our DV from the actual variation through the total variation in the DV. If these two values are the same, thus our model predicts all the variation in our dependent variable and this \\(R^2\\) is 1. If our model could not explain anything the variation would be 0, since the values of both cannot be negative. Let us calculate them: #total sum of squares tss &lt;- sum((df$y - mean(df$y))^2) #explained sum of squares ess &lt;- sum((df$y_hat - mean(df$y))^2) #caculating r squared r_squared &lt;- ess/tss #Printing it r_squared ## [1] 0.7631199 #Summarizing it summary(model1)$r.squared ## [1] 0.7630777 7.2.1.3 Influential Outliers Outliers are extremely deviating values, which can impact our analysis and bias it. Therefore, we have to check, if our data contains such values. But first let us see how they can impact our data: #set seed set.seed(069) #generate fake data with outlier x1 &lt;- sort(runif(10, min = 30, max = 70)) y1 &lt;- rnorm(10 , mean = 200, sd = 50) y1[9] &lt;- 2000 data_outlier &lt;- data.frame(x1, y1) #Model with Outlier model_outlier &lt;- lm(y1 ~ x1) #Model without Outlier model_without_outlier &lt;- lm(y1[-9] ~ x1[-9]) #Plotting the Data # Scatter plot with points ggplot(data_outlier, aes(x = x1, y = y1)) + geom_point(shape = 20, size = 3) + # Regression line for the model with outlier geom_abline(aes(slope = model_outlier$coefficients[2], intercept = model_outlier$coefficients[1], color = &quot;Model with Outlier&quot;), linewidth = 0.75, show.legend = TRUE) + # Regression line for the model without outlier geom_abline(aes(slope = model_without_outlier$coefficients[2], intercept = model_without_outlier$coefficients[1], color = &quot;Model without Outlier&quot;), linewidth = 0.75, show.legend = TRUE) + xlab(&quot;Independent Variable&quot;) + # Adding legend theme_classic() + theme(legend.position = c(0.15,0.9), legend.title = element_blank()) We see that this one observation completely biases our sample. But how do we find out, which observation is an influential outlier? There is a metric called Cook’s Distance, we can use. Let us do it and plot it in R. #Cooks Distance can be calculated with a built-in function data_outlier$cooks_distance &lt;- cooks.distance(model_outlier) #Plotting it ggplot(data_outlier, aes(x = x1, y = cooks_distance)) + geom_point(colour = &quot;darkgreen&quot;, size = 3, alpha = 0.5) + labs(y = &quot;Cook&#39;s Distance&quot;, x = &quot;Independent Variables&quot;) + geom_hline(yintercept = 1, linetype = &quot;dashed&quot;) + theme_bw() We can clearly see that Cook’s Distance detected the outlier. The rule is that values with a cooks distance bigger than 1 have to be eliminated. You can do that with the filter() function and run the model afterward again without the outliers. 7.2.1.4 Functional Form First let us get our data: #Simulate further data X_quadratic &lt;- X &lt;- runif(50, min = -5, max = 5) u &lt;- rnorm(50, sd = 1) #True relation Y_quadratic &lt;- X^2 + 2 * X + u #Making a data frame out of it df2 &lt;- data.frame(X_quadratic, Y_quadratic) Linear regression is a mathematical model. Therefore it is based on assumptions. But we should not just assume them, we should test them! One assumption is that linearity is assumed between X and Y. But that can be problematic consider following example: # estimate a simple regression model model_simple &lt;- lm(Y_quadratic ~ X_quadratic, data = df2) # Summarize it summary(model_simple) ## ## Call: ## lm(formula = Y_quadratic ~ X_quadratic, data = df2) ## ## Residuals: ## Min 1Q Median 3Q Max ## -8.703 -6.508 -1.478 5.277 17.464 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 7.5912 1.0914 6.955 8.61e-09 *** ## X_quadratic 2.0220 0.3948 5.122 5.32e-06 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 7.709 on 48 degrees of freedom ## Multiple R-squared: 0.3534, Adjusted R-squared: 0.3399 ## F-statistic: 26.23 on 1 and 48 DF, p-value: 5.321e-06 #Plot it ggplot(df2, aes(x = X_quadratic, y = Y_quadratic)) + geom_point(shape = 20, size = 3) + geom_smooth(method = &quot;lm&quot;, se = FALSE) + theme_bw() ## `geom_smooth()` using formula = &#39;y ~ x&#39; As you can see something look wrong. There seems to be a correlation between the two variables, but it does not seem linear. In such a case it does make sense to square the independent variable and run the regression again: # estimate a simple regression model model_quadratic &lt;- lm(Y_quadratic ~ X_quadratic^2, data = df2) #Summarize it summary(model_quadratic) ## ## Call: ## lm(formula = Y_quadratic ~ X_quadratic^2, data = df2) ## ## Residuals: ## Min 1Q Median 3Q Max ## -8.703 -6.508 -1.478 5.277 17.464 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 7.5912 1.0914 6.955 8.61e-09 *** ## X_quadratic 2.0220 0.3948 5.122 5.32e-06 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 7.709 on 48 degrees of freedom ## Multiple R-squared: 0.3534, Adjusted R-squared: 0.3399 ## F-statistic: 26.23 on 1 and 48 DF, p-value: 5.321e-06 #Plot it ggplot(df2, aes(x = X_quadratic, y = Y_quadratic)) + geom_point(shape = 20, size = 3) + geom_smooth(method = &quot;lm&quot;, formula = y ~ poly(x, 2), color = &quot;red&quot;, se = FALSE,) + scale_x_continuous(&quot;X&quot;, breaks = seq(-5,5,1), limits = c(-5,5)) + ylab(&quot;Y&quot;) + theme_bw() Well that looks better and is the proper way to deal with quadratic relationships in linear regression. Well, data can take on not only a quadratic form, it could also take on a form of a square-root function. I will show the most classical example of such a functional form. The gapminder data is loaded. It contains data about the average life expectancy (lifeExp)and the GDP per capita (gdpPercap) of countries in different years. Let us look if the GDP per Capita is correlated with Life Expectancy: #checking the data head(gapminder) ## # A tibble: 6 × 6 ## country continent year lifeExp pop gdpPercap ## &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 Afghanistan Asia 1952 28.8 8425333 779. ## 2 Afghanistan Asia 1957 30.3 9240934 821. ## 3 Afghanistan Asia 1962 32.0 10267083 853. ## 4 Afghanistan Asia 1967 34.0 11537966 836. ## 5 Afghanistan Asia 1972 36.1 13079460 740. ## 6 Afghanistan Asia 1977 38.4 14880372 786. #Plotting it ggplot(gapminder, aes(gdpPercap, lifeExp)) + geom_point() + geom_smooth(method = &quot;loess&quot;, se = FALSE) + scale_y_continuous(&quot;Life Expectancy&quot;, seq(30, 80, 10), limits = c(30, 80)) + theme_bw() ## `geom_smooth()` using formula = &#39;y ~ x&#39; Well, that looks terrible. What can we do? I already mentioned that the fitted line looks like a square-root function (\\(y = \\beta{\\sqrt{x}}\\) ). When you take the logarithm of square root, you neutralize the square root and only x remains \\(\\log{(\\sqrt{x})} = x\\). When you do that the functional form changes to \\(y = \\beta{x}\\). Well, that is exactly the systematic component we are after: ggplot(gapminder, aes(log(gdpPercap), lifeExp)) + geom_point() + geom_smooth(method = &quot;lm&quot;, se = FALSE) + scale_y_continuous(&quot;Life Expectancy&quot;, seq(30, 80, 10), limits = c(30, 80)) + xlab(&quot;GDP per Capita&quot;) + theme_bw() ## `geom_smooth()` using formula = &#39;y ~ x&#39; This looks more like what we want to achieve. What can we see in that plot? When we run a model we want to take the logarithm of the independent variable, when we expect the following: If the most observations of a variable are low, but some observations are extremely high such functional forms can occur. In our example, most of the countries have a low GDP per Capita, but some countries such as the Western European countries or the USA have such a a high level of GDP per Capita, they change the functional form of the fitted line. These are influential outliers, but too much to delete them. It could bias the representativeness of the sample, therefore we can deal with them by taking the logarithm. 7.2.1.5 Independent Observation Another assumption of the linear regression model is the independent, identically distributed (i.i.d) assumption. That sounds complicated but it really is not. Consider following plot: # Getting the Data # set seed set.seed(123) # generate a date vector date &lt;- seq(as.Date(&quot;1960/1/1&quot;), as.Date(&quot;2020/1/1&quot;), &quot;years&quot;) # initialize the employment vector y_time &lt;- c(5000, rep(NA, length(date)-1)) # generate time series observations with random influences for (i in 2:length(date)) { y_time[i] &lt;- -50 + 0.98 * y_time[i-1] + rnorm(n = 1, sd = 200) } # Plot it df_time_series &lt;- data.frame(y_time, date) ggplot(df_time_series, aes(date, y_time)) + geom_line() + ylab(&quot;Y&quot;) + xlab(&quot;Year&quot;) + theme_bw() If you look at the plot, can we assume that the observation Year = 2000 is independent from the Years before? No, the observation in the years are correlated to each other, thus the assumption is violated. This is basically the huge problem of working with longitudinal data (time-series cross-sectional or panel). If you face such problems there are plenty of other methods to use: Interrupted time series, Difference-in-Difference Designs, Panel-Matching, Fixed-Effects Models etc. 7.2.2 Model Fit: Multivariate Regression 7.2.2.1 Model Fit: Adjusted R-squared The last important aspect of Multivariate Regression is the Adjusted R-squared measure. Reconsider, the calculation of the classical R-squared: TSS (Variation in the DV): \\(TSS =\\sum(y_i - \\bar{y})^2\\) , we just subtract our actual values (df$y) from its mean and square it to avoid negative numbers. This gives us the total variation of our dependent variable. ESS (Variation we explain in the DV): \\(ESS = \\sum(\\hat{y_i} - \\bar{y})^2\\) , now we use our predicted values (df$y_hat) instead of our actual values. That gives us the variation in the dependent variable, we can explain with our model. \\(R^2\\) (The Variation we can predict from our model): \\(R^2 = \\frac{ESS}{TSS}\\) , well to get the proportion we just divide the variation we can explain from our DV from the actual variation through the total variation in the DV. If these two values are the same, thus our model predicts all the variation in our dependent variable and this \\(R^2\\) is 1. The problem with the classical R-squared is, that if you would add useless independent variables to it, the classical R-squared would decrease, although your model did not increase in explanatory power. This is called overfitting. However, adjusted R-squared will account for that problem by introducing a “penalty” for every additional variable. Mathematically, it looks like this: \\[ Adj.R^2 = 1 - \\frac{(1 - R^2)*(N - 1)}{N - k - 1} \\] where \\(R^2\\) is our classical R-squared calculated (\\(\\frac{TSS}{ESS}\\)) \\(N\\) is the number of observations in our sample \\(k\\) is the number of independent variables In R, we can extract the adjusted R-squared simply from our model in chunk, multivariate regression: #running multivariate model multivariate_model &lt;- lm(y ~ x + categorical_variable, data = df) #Getting summary summary(multivariate_model) ## ## Call: ## lm(formula = y ~ x + categorical_variable, data = df) ## ## Residuals: ## Min 1Q Median 3Q Max ## -4.5183 -1.3840 -0.5014 1.2393 5.5808 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1.9799 1.0679 1.854 0.0747 ## x 1.5557 0.1621 9.596 3.42e-10 ## categorical_variable1 -1.0667 0.9637 -1.107 0.2781 ## ## (Intercept) . ## x *** ## categorical_variable1 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 2.533 on 27 degrees of freedom ## Multiple R-squared: 0.7734, Adjusted R-squared: 0.7566 ## F-statistic: 46.07 on 2 and 27 DF, p-value: 1.982e-09 #Extract Adjusted R-squared summary(multivariate_model)$adj.r.squared ## [1] 0.7565721 #Calculating by hand adj_r_squared &lt;- 1 - (((1-summary(multivariate_model)$r.squared) * (nrow(df) - 1))/(nrow(df) - 2 - 1)) #printing it print(adj_r_squared) ## [1] 0.7565721 7.2.2.2 Omitted Variable Bias I already mentioned one (abstract) reason why we should include other variables in our model. But there is more to it: You could find effects between two variables X and Y, but it could be that in Reality there is not an association. For example, let us say you collect data about ice cream and shark attacks. Ice cream sales is your independent variable and you want to explain the number of shark attacks, here is your data: # Set seed for reproducibility set.seed(0) # Number of data points n &lt;- 100 # Simulate diet data (assuming a normal distribution) temperature &lt;- rnorm(n, mean = 1500, sd = 200) # Simulate exercise data (assuming a normal distribution) ice_cream_sales &lt;- rnorm(n, mean = 3, sd = 1) # Simulate weight loss data violence_crime_true &lt;- 0.2 * temperature - 0.5 * ice_cream_sales + rnorm(n, mean = 0, sd = 5) # Create a data frame data &lt;- data.frame(temperature = temperature, ice_cream_sales = ice_cream_sales, violence_crime_true = violence_crime_true) # Fit a model without including the diet variable model_without_temperature &lt;- lm(violence_crime_true ~ ice_cream_sales, data = data) #Fit a model with only the temperature variable model_with_only_temperature &lt;- lm(violence_crime_true ~temperature, data = data) # Fit a model including both diet and exercise variables model_with_temperature &lt;- lm(violence_crime_true ~ ice_cream_sales + temperature, data = data) # Output the summary of both models summary(model_without_temperature) ## ## Call: ## lm(formula = violence_crime_true ~ ice_cream_sales, data = data) ## ## Residuals: ## Min 1Q Median 3Q Max ## -94.788 -23.736 -2.348 22.264 98.754 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 287.681 11.623 24.751 &lt;2e-16 *** ## ice_cream_sales 4.090 3.741 1.093 0.277 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 35.94 on 98 degrees of freedom ## Multiple R-squared: 0.01205, Adjusted R-squared: 0.001969 ## F-statistic: 1.195 on 1 and 98 DF, p-value: 0.2769 summary(model_with_only_temperature) ## ## Call: ## lm(formula = violence_crime_true ~ temperature, data = data) ## ## Residuals: ## Min 1Q Median 3Q Max ## -13.9969 -3.8454 0.1718 2.9121 11.7502 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -3.535495 4.576293 -0.773 0.442 ## temperature 0.201591 0.003021 66.727 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 5.307 on 98 degrees of freedom ## Multiple R-squared: 0.9785, Adjusted R-squared: 0.9782 ## F-statistic: 4452 on 1 and 98 DF, p-value: &lt; 2.2e-16 summary(model_with_temperature) ## ## Call: ## lm(formula = violence_crime_true ~ ice_cream_sales + temperature, ## data = data) ## ## Residuals: ## Min 1Q Median 3Q Max ## -14.4770 -3.6734 -0.0914 2.9449 12.1843 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -2.396657 4.694786 -0.510 0.611 ## ice_cream_sales -0.596143 0.556470 -1.071 0.287 ## temperature 0.202005 0.003043 66.373 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 5.303 on 97 degrees of freedom ## Multiple R-squared: 0.9787, Adjusted R-squared: 0.9783 ## F-statistic: 2230 on 2 and 97 DF, p-value: &lt; 2.2e-16 #Let us display both models next to each other #EDIT: I created this function specifically, the code for the function is at the top. table_ovb(model_without_temperature, model_with_temperature) Model without Temperature Model with Temperature Intercept 287.680876 -2.3966572 Ice Cream Sales 4.090356 -0.5961434 Temperature NA 0.2020051 We can see that the coefficient changes dramatically. What happened? Well, one important assumption of linear regression is that the error term captures all variance not explained by our model and is not correlated with the independent variable(s) nor the dependent variable. But if there is unexplained variation in our model that is correlated with our independent variable, then this assumption is violated. In our example, we can see that ice cream sales coefficient in the first model is biased, because ice cream sales and is correlated to temperature. The warmer it gets, the more ice cream is sold. But, the warmer it gets, the more violent people get, therefore we have an omitted variable and that is temperature. When we include temperature in the model, we see the problem of omitted variable bias: It biases our coefficients, by either overestimating (like in our example) or by underestimating it. What we should do in such a case, is to delete the omitted variable, which is the drastically changing variable (ice cream sales in our case). This is also the reason, why people talk about additional variables as control variables in a multiple linear model. This way you can control if an association between two variables is due to omitted variable bias or other variables, which can explain the variation better. 7.2.2.3 Multicollinearity Another, and I promise, the last OLS assumption, which has to tested is that there is no Multicollinearity. The concept is simple: The independent variables should not be correlated. In our previous example, ice cream sales and temperature were correlated. This would have hurt these assumption. In strong cases, multicollinearity can bias our estimates, so that they gain statistical significance and lead us to wrong conclusions. Let us look at an obvious example. You want to find out how the grades of children is affected by different factors. You choose 2 factors: The time they spent on doing their homework (learning time) and the time they spent on playing video games (gaming time). The systematic component looks like this: \\[ Grades_i = \\beta_0 + \\beta_1*\\text{learning time}_i + \\beta_2 *\\text{gaming time}_i + \\epsilon_i \\] Let us compute them: # Set seed for reproducibility set.seed(42) # Number of samples n &lt;- 100 # True coefficients beta_0 &lt;- 80 beta_1 &lt;- 1.5 beta_2 &lt;- 1.5 # Generate independent variables learning_time &lt;- runif(n, 1, 10) gaming_time &lt;- 0.7 * learning_time + sqrt(1 - 0.7^2) * rnorm(n, sd = 1) #generate error term epsilon &lt;- rnorm(n, 0, 3) # Generate grades grades &lt;- beta_0 + beta_1 * learning_time + beta_2 * gaming_time + epsilon # Create a data frame df_grades &lt;- data.frame(learning_time, gaming_time, grades) # Display first few rows of the data frame grades_model &lt;- lm(grades ~ learning_time + gaming_time, data = df_grades) #Getting the summary summary(grades_model) ## ## Call: ## lm(formula = grades ~ learning_time + gaming_time, data = df_grades) ## ## Residuals: ## Min 1Q Median 3Q Max ## -6.2868 -1.9842 -0.0991 1.9482 6.2446 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 80.1718 0.6668 120.231 &lt; 2e-16 *** ## learning_time 1.2665 0.3335 3.798 0.000255 *** ## gaming_time 1.7860 0.4312 4.142 7.36e-05 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 2.822 on 97 degrees of freedom ## Multiple R-squared: 0.8661, Adjusted R-squared: 0.8634 ## F-statistic: 313.8 on 2 and 97 DF, p-value: &lt; 2.2e-16 By looking at the model, we could conclude that the more a student learns, the better its grades on average, holding all else constant. So far, so clear, but the same goes for gaming time. Why is that the case? Because if we think that a student has per day 3 hours, which the student can assign to either learning or gaming, than both are correlated, because assigning 2 hours to learning means 1 hour for gaming, 0.5 hours for learning means 2.5 hours for gaming and so forth. This means both coefficients are explaining each other and bias each other. How to detect them? 7.2.2.3.1 Testing Correlations to each other The first technique is to check the correlations of the variables to each other beforehand. You can do that two-ways: Just print out a correlation table: #First store the variables you need in a seperate data frame cormatrix_data &lt;- df_grades %&gt;% select(learning_time, gaming_time) #Second, calculate the table, the 2 at the end are the dimensions cormatrix &lt;- cor(cormatrix_data) #Calculate the correlations round(cormatrix, 2) #round it to the second digit and display it ## learning_time gaming_time ## learning_time 1.00 0.95 ## gaming_time 0.95 1.00 #We could have also done this code in one step #df_grades %&gt;% # select(learning_time, gaming_time) %&gt;% # cor() %&gt;% # round(2) We can see that the correlation between both variables is way to high. Now, with only two variables the table works fine, but what if we have, let us say, 20 variables? It could get messy, therefore you could also use the correlation matrix, which I introduced in the chapter before. In this case, it does not make sense, since it would just print out one block. But keep it nevertheless in mind for the future. 7.2.2.3.2 Variance of Inflation (VIF) Another measure for multicolinearity and probably the most famous one, is the Variance-of-Inflation (VIF) factor. Intuitively spoken, this measure fits models with multiple variables, by calculating the variance of each variable. Then it fits a model with only one independent variable and calculates the variance of it. The result is a measure that displays high values if the variance of a variable increases, when other variables are added. That is exactly what this measure does.. The formula of it is really simple: \\[ VIF_i = \\frac{1}{1 - R^2_i} \\] where, the Variance of inflation (VIF) of variable i is calculated by 1 divided by 1 - the R-squared (\\(R^2_i\\)) of the regression with only that variable. In R, we can use the VIF() function from the car - package to do it. #We only have to use the function VIF() on our model vif(grades_model) ## learning_time gaming_time ## 10.21358 10.21358 The rule is that if a value exceeds 10, it is considered critical. We should always test for multicolinearity, and if we detect it, run the regression separately without the correlated variables. "],["solutions-exercises.html", "Chapter 8 Solutions Exercises 8.1 Chapter 1: Fundamentals 8.2 Chapter 2: Data Manipulation 8.3 Chapter 3: Data Visualisation 8.4 Chapter 4: Exploratory Data Analysis 8.5 Chapter 5: Data Analysis 8.6 Chapter 6: Loops and Functions", " Chapter 8 Solutions Exercises pacman::p_load(&quot;tidyverse&quot;, &quot;gapminder&quot;, &quot;babynames&quot;, &quot;sjPlot&quot;, &quot;ggridges&quot;) 8.1 Chapter 1: Fundamentals 8.1.1 Exercise 1: Making your first Vector Create a vector called my_vector with the values 1,2,3 and check is class. #create the vector my_vector &lt;- c(1,2,3) #check the class class(my_vector) 8.1.2 Exercise 2: Making your first matrix Create a Matrix called student. This should contain information about the name, age and major. Make three vectors with three entries and bind them together to a the matrix student. Print the matrix. Choose the three names, age, and major by yourself: #Create the vectors name &lt;- c(&quot;James&quot;, &quot;Elif&quot;, &quot;Jonas&quot;) age &lt;- c(&quot;19&quot;, &quot;17&quot;, &quot;24&quot;) major &lt;- c(&quot;Political Science&quot;, &quot;Data Science&quot;, &quot;Physics&quot;) #Create the matrix student &lt;- cbind(name, age, major) #Or with the matrix command student &lt;- matrix(name, age, major) #Print the matrix print(student) 8.1.3 Exercise 3: ifelse function Write an ifelse statement that checks if a given number is positive or negative. If the number is positive or 0, print “Number is positive”, otherwise print “Number is negative”. Feel free to decide if you want to use the ifelse function or the ifelse condition. #Assigning the number to the object &quot;number&quot; number &lt;- 0 #With ifelse() function ifelse(number &gt;= 0, &quot;Number is positive&quot;, &quot;Number is negative&quot;) #With ifelse condition if (number &gt; 0) { print(&quot;Positive Number&quot;) } else { print(&quot;Negative Number&quot;) } 8.1.4 Exercise 4: ifelse ladders Write an if-else ladder that categorizes a student’s grade based on their score. The grading criteria are as follows: Score &gt;= 90: “A” Score &gt;= 80 and &lt; 90: “B” Score &gt;= 70 and &lt; 80: “C” Score &gt;= 60 and &lt; 70: “D” Score &lt; 60: “F”. #Defining the vector score Score &lt;- 90 #Defing ifelse ladder with ifelse() function ifelse(Score &gt;= 90, &quot;A&quot;, ifelse(Score &gt;= 80 &amp; Score &lt; 90, &quot;B&quot;, ifelse(Score &gt;= 70 &amp; Score &lt; 80, &quot;C&quot;, ifelse(Score &gt;= 60 &amp; Score &lt; 70, &quot;D&quot;, ifelse(Score &lt; 60, &quot;F&quot;))))) #Defining it with a ifelse condition if (score &gt;= 90) { print(&quot;A&quot;) } else if (score &gt;= 80 &amp; score &lt; 90) { print(&quot;B&quot;) } else if (score &gt;= 70 &amp; score &lt; 80) { print(&quot;C&quot;) } else if (score &gt;= 60 &amp; score &lt; 70) { print(&quot;D&quot;) } else if (score &lt; 60) { print(&quot;F&quot;) } 8.2 Chapter 2: Data Manipulation pacman::p_load(&quot;tidyverse&quot;) 8.2.1 Exercise 1: Let’s wrangle kid You are interested in discrimination and the perception of the judicial. More specifically, you want to know if people, who fell discriminated evaluate courts differently. Below you see a table with all variables you want to include in your analysis: Variable Description Scales idnt Respondent’s identification number unique number from 1-9000 year The year when the survey was conducted only 2020 cntry Country BE, BG, CH, CZ, EE, FI, FR,GB, GR, HR, HU, IE, IS, IT, LT,NL, NO, PT, SI, SK agea Age of the Respondent, calculated Number of Age = 15-90 999 = Not available gndr Gender 1 = Male; 2 = Female; 9 = No answer happy How happy are you 0 (Extremly unhappy) - 10 (Extremly happy); 77 = Refusal; 88 = Don’t Know; 99 = No answer eisced Highest level of education, ES - ISCED 0 = Not possible to harmonise into ES-ISCED; 1 (ES-ISCED I , less than lower secondary) - 7 (ES-ISCED V2, higher tertiary education, =&gt; MA level; 55 = Other; 77 = Refusal; 88 = Don’t know; 99 = No answer netusoft Internet use, how often 1 (Never) - 5 (Every day); 7 = Refusal; 8 = Don’t know; 9 = No answer trstprl Most people can be trusted or you can’t be too careful 0 (You can’t be too careful) - 10 (Most people can be trusted); 77 = Refusal; 88 = Don’t Know; 99 = No answer lrscale Left-Right Placement 0 (Left) - 10 (Right); 77 = Refusal; 88 = Don’t know; 99 = No answer Wrangle the data, and assign it to an object called ess. Select the variables you need Filter for Austria, Belgium, Denmark, Georgia, Iceland and the Russian Federation Have a look at the codebook and code all irrelevant values as missing. If you have binary variables recode them from 1, 2 to 0 to 1 You want to build an extremism variable: You do so by subtracting 5 from the from the variable and squaring it afterwards. Call it extremism Rename the variables to more intuitive names, don’t forget to name binary varaibles after the category which is on 1 drop all missing values Check out your new dataset ess &lt;- d1 %&gt;% select(cntry, dscrgrp, cttresa, agea, gndr, eisced, lrscale) %&gt;% #a filter(cntry %in% c(&quot;AT&quot;, &quot;BE&quot;, &quot;DK&quot;, &quot;GE&quot;, &quot;IS&quot;,&quot;RU&quot;)) %&gt;% #b mutate(dscrgrp = case_when( #c dscrgrp == 1 ~ 0, dscrgrp == 2 ~ 1, dscrgrp %in% c(7, 8, 9) ~ NA_real_, TRUE ~ dscrgrp), cttresa = case_when( cttresa %in% c(77, 88, 99) ~ NA_real_, TRUE ~ cttresa), agea = case_when( agea == 999 ~ NA_real_, TRUE ~ agea), gndr = case_when( gndr == 1 ~ 0, gndr == 2 ~ 1, gndr == 9 ~ NA_real_ ), eisced = case_when( eisced %in% c(55, 77, 88, 99) ~ NA_real_, TRUE ~ eisced), lrscale = case_when( lrscale %in% c(77, 88, 99) ~ NA_real_, TRUE ~ lrscale) #d, you could do this step also in a separate mutate function if you think that is more intuitive ) %&gt;% mutate(extremism = (lrscale - 5)^2) %&gt;% rename(discriminated = dscrgrp, court = cttresa, age = agea, female = gndr, education = eisced, lrscale = lrscale, extremism = extremism ) %&gt;% drop_na() #Checking the dataset head(ess) 8.3 Chapter 3: Data Visualisation In this exercise Section, we will work with the babynames and the iris package. This is a classic built-in package in R, which contains data from the Ronald Fisher’s 1936 Study “The use of multiple measurements in taxonomic problems”. It contains three plant species and four measured features for each species. Let us get an overview of the package: summary(iris) ## Sepal.Length Sepal.Width Petal.Length ## Min. :4.300 Min. :2.000 Min. :1.000 ## 1st Qu.:5.100 1st Qu.:2.800 1st Qu.:1.600 ## Median :5.800 Median :3.000 Median :4.350 ## Mean :5.843 Mean :3.057 Mean :3.758 ## 3rd Qu.:6.400 3rd Qu.:3.300 3rd Qu.:5.100 ## Max. :7.900 Max. :4.400 Max. :6.900 ## Petal.Width Species ## Min. :0.100 setosa :50 ## 1st Qu.:0.300 versicolor:50 ## Median :1.300 virginica :50 ## Mean :1.199 ## 3rd Qu.:1.800 ## Max. :2.500 8.3.1 Exercise 1: Distributions a. Plot a Chart, which shows the distribution of Sepal.Length over the setosa Species. Choose the type of distribution chart for yourself. HINT: Prepare the data first and then plot it. #Filtering for setosa d1 &lt;- iris %&gt;% filter(Species == &quot;setosa&quot;) #Histogram ggplot(iris, aes(x = Sepal.Length)) + geom_histogram() ## `stat_bin()` using `bins = 30`. Pick better value with ## `binwidth`. #Density Plot ggplot(iris, aes(x = Sepal.Length)) + geom_density() #Boxplot ggplot(iris, aes(x = Sepal.Length)) + geom_boxplot() ###EDIT: You could also do everythin in one step as shown below #Histogram #iris %&gt;% # filter(Species == &quot;setosa&quot;) %&gt;% # ggplot(aes(x = Sepal.Length)) + # geom_histogram() #Density Plot #iris %&gt;% # filter(Species == &quot;setosa&quot;) %&gt;% # ggplot(aes(x = Sepal.Length)) + # geom_density() #Boxplot #iris %&gt;% # filter(Species == &quot;setosa&quot;) %&gt;% # ggplot(aes(x = Sepal.Length)) + # geom_boxplot() b. Now I want you to add the two other Species to the Plot. Make Sure, that every Species has a unique color. #Histogram ggplot(iris, aes(x = Sepal.Length, fill = Species)) + geom_histogram() ## `stat_bin()` using `bins = 30`. Pick better value with ## `binwidth`. #Density Plot ggplot(iris, aes(x = Sepal.Length, fill = Species)) + geom_density() #Boxplot ggplot(iris, aes(x = Sepal.Length, fill = Species)) + geom_boxplot() #Violin Plot ggplot(iris, aes(x = Species, y = Sepal.Length, fill = Species)) + geom_violin() #Ridgeline Plot ggplot(iris, aes(x = Sepal.Length, y = Species, fill = Species)) + geom_density_ridges() ## Picking joint bandwidth of 0.181 c. Make a nice Plot! Give the Plot a meaningful title, meaningful labels for the x-axis and the y-axis and play around with the colors. #Histogram ggplot(iris, aes(x = Sepal.Length, fill = Species)) + geom_histogram() + labs( x = &quot;Species&quot;, y = &quot;Sepal Length&quot;, title = &quot;Distribution of Sepal Length across Species&quot; ) + theme_bw() + theme( legend.position = &quot;none&quot; ) + scale_fill_brewer(palette = &quot;Dark2&quot;) ## `stat_bin()` using `bins = 30`. Pick better value with ## `binwidth`. #Density Plot ggplot(iris, aes(x = Sepal.Length, fill = Species)) + geom_density() + labs( x = &quot;Species&quot;, y = &quot;Sepal Length&quot;, title = &quot;Distribution of Sepal Length across Species&quot; ) + theme_bw() + theme( legend.position = &quot;none&quot; ) + scale_fill_brewer(palette = &quot;Dark2&quot;) #Boxplot ggplot(iris, aes(x = Sepal.Length, fill = Species)) + geom_boxplot() + labs( x = &quot;Species&quot;, y = &quot;Sepal Length&quot;, title = &quot;Distribution of Sepal Length across Species&quot; ) + theme_bw() + theme( legend.position = &quot;none&quot; ) + scale_fill_brewer(palette = &quot;Dark2&quot;) #Ridgeline Plot ggplot(iris, aes(x = Species, y = Sepal.Length, fill = Species)) + geom_violin() + labs( x = &quot;Species&quot;, y = &quot;Sepal Length&quot;, title = &quot;Distribution of Sepal Length across Species&quot; ) + theme_bw() + theme( legend.position = &quot;none&quot; ) + scale_fill_brewer(palette = &quot;Dark2&quot;) #Ridgeline Plot ggplot(iris, aes(x = Sepal.Length, y = Species, fill = Species)) + geom_density_ridges() + labs( x = &quot;Species&quot;, y = &quot;Sepal Length&quot;, title = &quot;Distribution of Sepal Length across Species&quot; ) + theme_bw() + theme( legend.position = &quot;none&quot; ) + scale_fill_brewer(palette = &quot;Dark2&quot;) ## Picking joint bandwidth of 0.181 d. Interpret the Plot! 8.3.2 Exercise 2: Rankings a. Calculate the average Petal.Length and plot it for every Species in a nice Barplot. HINT: You have to prepare the data again before you plot it. Decide for yourself if you want to do it vertically or horizontally. #Prepare the data d1 &lt;- iris %&gt;% group_by(Species) %&gt;% dplyr::summarize(PL_average = mean(Petal.Length)) #Check it head(d1) ## # A tibble: 3 × 2 ## Species PL_average ## &lt;fct&gt; &lt;dbl&gt; ## 1 setosa 1.46 ## 2 versicolor 4.26 ## 3 virginica 5.55 #Plotting a horizontal barplot ggplot(d1, aes(x = Species, y = PL_average, fill = Species)) + geom_bar(stat = &quot;identity&quot;) #Plotting a vertical barplot ggplot(d1, aes(x = PL_average, y = Species, fill = Species)) + geom_bar(stat = &quot;identity&quot;) b. Make a nice Plot! Give the Plot a meaningful title, meaningful labels for the x-axis and the y-axis and play around with the colors. #Plotting a horizontal barplot ggplot(d1, aes(x = Species, y = PL_average, fill = Species)) + geom_bar(stat = &quot;identity&quot;) + labs( x = &quot;Species&quot;, y = &quot;Average Sepal Length&quot;, title = &quot;Average Sepal Length across Species&quot; ) + theme_bw() + theme( legend.position = &quot;none&quot; ) + scale_fill_brewer(palette = &quot;Set1&quot;) #Plotting a vertical barplot ggplot(d1, aes(x = PL_average, y = Species, fill = Species)) + geom_bar(stat = &quot;identity&quot;) + labs( x = &quot;Average Sepal Length&quot;, y = &quot;Species&quot;, title = &quot;Average Sepal Length across Species&quot; ) + theme_bw() + theme( legend.position = &quot;none&quot; ) + scale_fill_brewer(palette = &quot;Accent&quot;) 8.3.3 Exercise 3: Correlation a. Make a scatter plot where you plot Sepal.length on the x-axis and Sepal.width on the y-axis. Make the plot for the species virginica #Preparing the data d1 &lt;- iris %&gt;% filter(Species == &quot;virginica&quot;) #Making the plot ggplot(d1, aes(x = Sepal.Length, y = Sepal.Width)) + geom_point() b. Now I want you to add the species versicolor to the plot. The dots of this species should have a different color AND a different form. #Preparing the Data d1 &lt;- iris %&gt;% filter(Species %in% c(&quot;virginica&quot;, &quot;versicolor&quot;)) #Making the Plot ggplot(d1, aes(x = Sepal.Length, y = Sepal.Width, shape = Species, color = Species)) + geom_point() c. Make a nice plot! Add a theme, labels, and a nice title, increase the size of the forms and make play around with the colors. #Making the Plot ggplot(d1, aes(x = Sepal.Length, y = Sepal.Width, shape = Species, color = Species)) + geom_point() + labs( title = &quot;Relationship between Sepal Length and Sepal Width&quot;, x = &quot;Sepal Length&quot;, y = &quot;Sepal Width&quot; ) + scale_color_brewer(palette = &quot;Set1&quot;) + theme_classic() 8.4 Chapter 4: Exploratory Data Analysis 8.4.1 Exercise 1: Standard Descriptive Statistics In this exercise we will work with the built-in iris package in R: a. Calculate the mode, mean and the median for the iris$Sepal.Length variable mean(iris$Sepal.Length) #Calculating the mean ## [1] 5.843333 median(iris$Sepal.Length) # Calculating the median ## [1] 5.8 #Calculating the mode uniq_vals &lt;- unique(iris$Sepal.Length) freqs &lt;- tabulate(iris$Sepal.Length) uniq_vals[which.max(freqs)] ## [1] 5 # You could also define a function for the mode #mode &lt;- function (x) { # uniq_vals &lt;- unique(x, na.rm = TRUE) # freqs &lt;- tabulate(match(x, uniq_vals)) # uniq_vals[which.max(freqs)] #} #mode(iris$Sepal.Length) b. Calculate the interquartile range, variance and the standard deviation for iris$Sepal.Length IQR(iris$Sepal.Length) ## [1] 1.3 var(iris$Sepal.Length) #Variance ## [1] 0.6856935 sd(iris$Sepal.Length) #Standard deviatio ## [1] 0.8280661 #Or we take just the squareroot of the variance to get the sd #var(iris$Sepal.Length) %&gt;% # sqrt() c. Calculate all five measures at once by using a function that does so (Choose by yourself, which one you want to use) psych::describe(iris$Sepal.Length) #psych package ## vars n mean sd median trimmed mad min max range skew ## X1 1 150 5.84 0.83 5.8 5.81 1.04 4.3 7.9 3.6 0.31 ## kurtosis se ## X1 -0.61 0.07 skimr::skim(iris$Sepal.Length) #skim package (#tab:chapter 4 exercise 1c)Data summary Name iris$Sepal.Length Number of rows 150 Number of columns 1 _______________________ Column type frequency: numeric 1 ________________________ Group variables None Variable type: numeric skim_variable n_missing complete_rate mean sd p0 p25 p50 p75 p100 hist data 0 1 5.84 0.83 4.3 5.1 5.8 6.4 7.9 ▆▇▇▅▂ summarytools::descr(iris$Sepal.Length) #summarytools package ## Descriptive Statistics ## iris$Sepal.Length ## N: 150 ## ## Sepal.Length ## ----------------- -------------- ## Mean 5.84 ## Std.Dev 0.83 ## Min 4.30 ## Q1 5.10 ## Median 5.80 ## Q3 6.40 ## Max 7.90 ## MAD 1.04 ## IQR 1.30 ## CV 0.14 ## Skewness 0.31 ## SE.Skewness 0.20 ## Kurtosis -0.61 ## N.Valid 150.00 ## N 150.00 ## Pct.Valid 100.00 8.4.2 Exercise 2: Contingency Tables and Correlations a. Make a Contingency Table for esoph$agegp and esoph$alcgp table(esoph$agegp, esoph$alcgp) #with base R ## ## 0-39g/day 40-79 80-119 120+ ## 25-34 4 4 3 4 ## 35-44 4 4 4 3 ## 45-54 4 4 4 4 ## 55-64 4 4 4 4 ## 65-74 4 3 4 4 ## 75+ 3 4 2 2 summarytools::ctable(esoph$agegp, esoph$alcgp) #with summarytools ## Cross-Tabulation, Row Proportions ## agegp * alcgp ## Data Frame: esoph ## ## ------- ------- ------------ ------------ ------------ ------------ ------------- ## alcgp 0-39g/day 40-79 80-119 120+ Total ## agegp ## 25-34 4 (26.7%) 4 (26.7%) 3 (20.0%) 4 (26.7%) 15 (100.0%) ## 35-44 4 (26.7%) 4 (26.7%) 4 (26.7%) 3 (20.0%) 15 (100.0%) ## 45-54 4 (25.0%) 4 (25.0%) 4 (25.0%) 4 (25.0%) 16 (100.0%) ## 55-64 4 (25.0%) 4 (25.0%) 4 (25.0%) 4 (25.0%) 16 (100.0%) ## 65-74 4 (26.7%) 3 (20.0%) 4 (26.7%) 4 (26.7%) 15 (100.0%) ## 75+ 3 (27.3%) 4 (36.4%) 2 (18.2%) 2 (18.2%) 11 (100.0%) ## Total 23 (26.1%) 23 (26.1%) 21 (23.9%) 21 (23.9%) 88 (100.0%) ## ------- ------- ------------ ------------ ------------ ------------ ------------- gtsummary::tbl_cross(data = esoph, row = agegp, col = alcgp) #with gtsummary #sybfixkfen table { font-family: system-ui, 'Segoe UI', Roboto, Helvetica, Arial, sans-serif, 'Apple Color Emoji', 'Segoe UI Emoji', 'Segoe UI Symbol', 'Noto Color Emoji'; -webkit-font-smoothing: antialiased; -moz-osx-font-smoothing: grayscale; } #sybfixkfen thead, #sybfixkfen tbody, #sybfixkfen tfoot, #sybfixkfen tr, #sybfixkfen td, #sybfixkfen th { border-style: none; } #sybfixkfen p { margin: 0; padding: 0; } #sybfixkfen .gt_table { display: table; border-collapse: collapse; line-height: normal; margin-left: auto; margin-right: auto; color: #333333; font-size: 16px; font-weight: normal; font-style: normal; background-color: #FFFFFF; width: auto; border-top-style: solid; border-top-width: 2px; border-top-color: #A8A8A8; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #A8A8A8; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; } #sybfixkfen .gt_caption { padding-top: 4px; padding-bottom: 4px; } #sybfixkfen .gt_title { color: #333333; font-size: 125%; font-weight: initial; padding-top: 4px; padding-bottom: 4px; padding-left: 5px; padding-right: 5px; border-bottom-color: #FFFFFF; border-bottom-width: 0; } #sybfixkfen .gt_subtitle { color: #333333; font-size: 85%; font-weight: initial; padding-top: 3px; padding-bottom: 5px; padding-left: 5px; padding-right: 5px; border-top-color: #FFFFFF; border-top-width: 0; } #sybfixkfen .gt_heading { background-color: #FFFFFF; text-align: center; border-bottom-color: #FFFFFF; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #sybfixkfen .gt_bottom_border { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #sybfixkfen .gt_col_headings { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #sybfixkfen .gt_col_heading { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 6px; padding-left: 5px; padding-right: 5px; overflow-x: hidden; } #sybfixkfen .gt_column_spanner_outer { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; padding-top: 0; padding-bottom: 0; padding-left: 4px; padding-right: 4px; } #sybfixkfen .gt_column_spanner_outer:first-child { padding-left: 0; } #sybfixkfen .gt_column_spanner_outer:last-child { padding-right: 0; } #sybfixkfen .gt_column_spanner { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 5px; overflow-x: hidden; display: inline-block; width: 100%; } #sybfixkfen .gt_spanner_row { border-bottom-style: hidden; } #sybfixkfen .gt_group_heading { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; text-align: left; } #sybfixkfen .gt_empty_group_heading { padding: 0.5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: middle; } #sybfixkfen .gt_from_md > :first-child { margin-top: 0; } #sybfixkfen .gt_from_md > :last-child { margin-bottom: 0; } #sybfixkfen .gt_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; margin: 10px; border-top-style: solid; border-top-width: 1px; border-top-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; overflow-x: hidden; } #sybfixkfen .gt_stub { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 5px; padding-right: 5px; } #sybfixkfen .gt_stub_row_group { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 5px; padding-right: 5px; vertical-align: top; } #sybfixkfen .gt_row_group_first td { border-top-width: 2px; } #sybfixkfen .gt_row_group_first th { border-top-width: 2px; } #sybfixkfen .gt_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #sybfixkfen .gt_first_summary_row { border-top-style: solid; border-top-color: #D3D3D3; } #sybfixkfen .gt_first_summary_row.thick { border-top-width: 2px; } #sybfixkfen .gt_last_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #sybfixkfen .gt_grand_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #sybfixkfen .gt_first_grand_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-top-style: double; border-top-width: 6px; border-top-color: #D3D3D3; } #sybfixkfen .gt_last_grand_summary_row_top { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-bottom-style: double; border-bottom-width: 6px; border-bottom-color: #D3D3D3; } #sybfixkfen .gt_striped { background-color: rgba(128, 128, 128, 0.05); } #sybfixkfen .gt_table_body { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #sybfixkfen .gt_footnotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #sybfixkfen .gt_footnote { margin: 0px; font-size: 90%; padding-top: 4px; padding-bottom: 4px; padding-left: 5px; padding-right: 5px; } #sybfixkfen .gt_sourcenotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #sybfixkfen .gt_sourcenote { font-size: 90%; padding-top: 4px; padding-bottom: 4px; padding-left: 5px; padding-right: 5px; } #sybfixkfen .gt_left { text-align: left; } #sybfixkfen .gt_center { text-align: center; } #sybfixkfen .gt_right { text-align: right; font-variant-numeric: tabular-nums; } #sybfixkfen .gt_font_normal { font-weight: normal; } #sybfixkfen .gt_font_bold { font-weight: bold; } #sybfixkfen .gt_font_italic { font-style: italic; } #sybfixkfen .gt_super { font-size: 65%; } #sybfixkfen .gt_footnote_marks { font-size: 75%; vertical-align: 0.4em; position: initial; } #sybfixkfen .gt_asterisk { font-size: 100%; vertical-align: 0; } #sybfixkfen .gt_indent_1 { text-indent: 5px; } #sybfixkfen .gt_indent_2 { text-indent: 10px; } #sybfixkfen .gt_indent_3 { text-indent: 15px; } #sybfixkfen .gt_indent_4 { text-indent: 20px; } #sybfixkfen .gt_indent_5 { text-indent: 25px; } #sybfixkfen .katex-display { display: inline-flex !important; margin-bottom: 0.75em !important; } #sybfixkfen div.Reactable > div.rt-table > div.rt-thead > div.rt-tr.rt-tr-group-header > div.rt-th-group:after { height: 0px !important; } alcgp Total 0-39g/day 40-79 80-119 120+ agegp     25-34 4 4 3 4 15     35-44 4 4 4 3 15     45-54 4 4 4 4 16     55-64 4 4 4 4 16     65-74 4 3 4 4 15     75+ 3 4 2 2 11 Total 23 23 21 21 88 b. Cut down the iris dataset to Sepal.Length, Sepal.Width, Petal.Length and Petal.Width and save it in an object called iris_numeric. iris_numeric &lt;- iris %&gt;% select(Sepal.Length, Sepal.Width, Petal.Length, Petal.Width) c. Make a correlation matrix with iris_numeric # With corrplot package cor_iris &lt;- cor(iris_numeric) corrplot::corrplot(cor_iris, method = &quot;color&quot;) # With DataExplorer DataExplorer::plot_correlation(iris_numeric) d. Make the correlation matrix pretty # With Corrplot corrplot::corrplot(cor_iris, method = &quot;circle&quot;, type = &quot;upper&quot;, tl.col = &quot;black&quot;, tl.srt = 45) # With DataExplorer DataExplorer::plot_correlation( iris_numeric, ggtheme = theme_minimal(base_size = 14), title = &quot;Correlation Matrix of Iris Numeric Variables&quot; ) 8.4.3 Exercise 3: Working with packages a. Use a function to get an overview of the dataset mtcars skimr::skim(mtcars) (#tab:chapter 4 exercise 3a)Data summary Name mtcars Number of rows 32 Number of columns 11 _______________________ Column type frequency: numeric 11 ________________________ Group variables None Variable type: numeric skim_variable n_missing complete_rate mean sd p0 p25 p50 p75 p100 hist mpg 0 1 20.09 6.03 10.40 15.43 19.20 22.80 33.90 ▃▇▅▁▂ cyl 0 1 6.19 1.79 4.00 4.00 6.00 8.00 8.00 ▆▁▃▁▇ disp 0 1 230.72 123.94 71.10 120.83 196.30 326.00 472.00 ▇▃▃▃▂ hp 0 1 146.69 68.56 52.00 96.50 123.00 180.00 335.00 ▇▇▆▃▁ drat 0 1 3.60 0.53 2.76 3.08 3.70 3.92 4.93 ▇▃▇▅▁ wt 0 1 3.22 0.98 1.51 2.58 3.33 3.61 5.42 ▃▃▇▁▂ qsec 0 1 17.85 1.79 14.50 16.89 17.71 18.90 22.90 ▃▇▇▂▁ vs 0 1 0.44 0.50 0.00 0.00 0.00 1.00 1.00 ▇▁▁▁▆ am 0 1 0.41 0.50 0.00 0.00 0.00 1.00 1.00 ▇▁▁▁▆ gear 0 1 3.69 0.74 3.00 3.00 4.00 4.00 5.00 ▇▁▆▁▂ carb 0 1 2.81 1.62 1.00 2.00 2.00 4.00 8.00 ▇▂▅▁▁ summarytools::dfSummary(mtcars) ## Data Frame Summary ## mtcars ## Dimensions: 32 x 11 ## Duplicates: 0 ## ## ---------------------------------------------------------------------------------------------------------- ## No Variable Stats / Values Freqs (% of Valid) Graph Valid Missing ## ---- ----------- --------------------------- -------------------- ------------------- ---------- --------- ## 1 mpg Mean (sd) : 20.1 (6) 25 distinct values : 32 0 ## [numeric] min &lt; med &lt; max: : . (100.0%) (0.0%) ## 10.4 &lt; 19.2 &lt; 33.9 . : : ## IQR (CV) : 7.4 (0.3) : : : . ## : : : : : ## ## 2 cyl Mean (sd) : 6.2 (1.8) 4 : 11 (34.4%) IIIIII 32 0 ## [numeric] min &lt; med &lt; max: 6 : 7 (21.9%) IIII (100.0%) (0.0%) ## 4 &lt; 6 &lt; 8 8 : 14 (43.8%) IIIIIIII ## IQR (CV) : 4 (0.3) ## ## 3 disp Mean (sd) : 230.7 (123.9) 27 distinct values : 32 0 ## [numeric] min &lt; med &lt; max: . : (100.0%) (0.0%) ## 71.1 &lt; 196.3 &lt; 472 : : : : : : ## IQR (CV) : 205.2 (0.5) : : : : : : . ## : : : . : : : . : ## ## 4 hp Mean (sd) : 146.7 (68.6) 22 distinct values . : 32 0 ## [numeric] min &lt; med &lt; max: : : (100.0%) (0.0%) ## 52 &lt; 123 &lt; 335 : : : . ## IQR (CV) : 83.5 (0.5) : : : : ## : : : : . . ## ## 5 drat Mean (sd) : 3.6 (0.5) 22 distinct values : 32 0 ## [numeric] min &lt; med &lt; max: : : (100.0%) (0.0%) ## 2.8 &lt; 3.7 &lt; 4.9 : : . ## IQR (CV) : 0.8 (0.1) . : : : ## : : : : . ## ## 6 wt Mean (sd) : 3.2 (1) 29 distinct values : 32 0 ## [numeric] min &lt; med &lt; max: : : (100.0%) (0.0%) ## 1.5 &lt; 3.3 &lt; 5.4 : : ## IQR (CV) : 1 (0.3) : : : : : . ## : : : : : . : ## ## 7 qsec Mean (sd) : 17.8 (1.8) 30 distinct values : 32 0 ## [numeric] min &lt; med &lt; max: : (100.0%) (0.0%) ## 14.5 &lt; 17.7 &lt; 22.9 : : ## IQR (CV) : 2 (0.1) . : : : : ## : : : : : : : . ## ## 8 vs Min : 0 0 : 18 (56.2%) IIIIIIIIIII 32 0 ## [numeric] Mean : 0.4 1 : 14 (43.8%) IIIIIIII (100.0%) (0.0%) ## Max : 1 ## ## 9 am Min : 0 0 : 19 (59.4%) IIIIIIIIIII 32 0 ## [numeric] Mean : 0.4 1 : 13 (40.6%) IIIIIIII (100.0%) (0.0%) ## Max : 1 ## ## 10 gear Mean (sd) : 3.7 (0.7) 3 : 15 (46.9%) IIIIIIIII 32 0 ## [numeric] min &lt; med &lt; max: 4 : 12 (37.5%) IIIIIII (100.0%) (0.0%) ## 3 &lt; 4 &lt; 5 5 : 5 (15.6%) III ## IQR (CV) : 1 (0.2) ## ## 11 carb Mean (sd) : 2.8 (1.6) 1 : 7 (21.9%) IIII 32 0 ## [numeric] min &lt; med &lt; max: 2 : 10 (31.2%) IIIIII (100.0%) (0.0%) ## 1 &lt; 2 &lt; 8 3 : 3 ( 9.4%) I ## IQR (CV) : 2 (0.6) 4 : 10 (31.2%) IIIIII ## 6 : 1 ( 3.1%) ## 8 : 1 ( 3.1%) ## ---------------------------------------------------------------------------------------------------------- gtsummary::tbl_summary(mtcars) #lctzyhtwna table { font-family: system-ui, 'Segoe UI', Roboto, Helvetica, Arial, sans-serif, 'Apple Color Emoji', 'Segoe UI Emoji', 'Segoe UI Symbol', 'Noto Color Emoji'; -webkit-font-smoothing: antialiased; -moz-osx-font-smoothing: grayscale; } #lctzyhtwna thead, #lctzyhtwna tbody, #lctzyhtwna tfoot, #lctzyhtwna tr, #lctzyhtwna td, #lctzyhtwna th { border-style: none; } #lctzyhtwna p { margin: 0; padding: 0; } #lctzyhtwna .gt_table { display: table; border-collapse: collapse; line-height: normal; margin-left: auto; margin-right: auto; color: #333333; font-size: 16px; font-weight: normal; font-style: normal; background-color: #FFFFFF; width: auto; border-top-style: solid; border-top-width: 2px; border-top-color: #A8A8A8; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #A8A8A8; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; } #lctzyhtwna .gt_caption { padding-top: 4px; padding-bottom: 4px; } #lctzyhtwna .gt_title { color: #333333; font-size: 125%; font-weight: initial; padding-top: 4px; padding-bottom: 4px; padding-left: 5px; padding-right: 5px; border-bottom-color: #FFFFFF; border-bottom-width: 0; } #lctzyhtwna .gt_subtitle { color: #333333; font-size: 85%; font-weight: initial; padding-top: 3px; padding-bottom: 5px; padding-left: 5px; padding-right: 5px; border-top-color: #FFFFFF; border-top-width: 0; } #lctzyhtwna .gt_heading { background-color: #FFFFFF; text-align: center; border-bottom-color: #FFFFFF; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #lctzyhtwna .gt_bottom_border { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #lctzyhtwna .gt_col_headings { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #lctzyhtwna .gt_col_heading { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 6px; padding-left: 5px; padding-right: 5px; overflow-x: hidden; } #lctzyhtwna .gt_column_spanner_outer { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; padding-top: 0; padding-bottom: 0; padding-left: 4px; padding-right: 4px; } #lctzyhtwna .gt_column_spanner_outer:first-child { padding-left: 0; } #lctzyhtwna .gt_column_spanner_outer:last-child { padding-right: 0; } #lctzyhtwna .gt_column_spanner { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 5px; overflow-x: hidden; display: inline-block; width: 100%; } #lctzyhtwna .gt_spanner_row { border-bottom-style: hidden; } #lctzyhtwna .gt_group_heading { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; text-align: left; } #lctzyhtwna .gt_empty_group_heading { padding: 0.5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: middle; } #lctzyhtwna .gt_from_md > :first-child { margin-top: 0; } #lctzyhtwna .gt_from_md > :last-child { margin-bottom: 0; } #lctzyhtwna .gt_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; margin: 10px; border-top-style: solid; border-top-width: 1px; border-top-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; overflow-x: hidden; } #lctzyhtwna .gt_stub { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 5px; padding-right: 5px; } #lctzyhtwna .gt_stub_row_group { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 5px; padding-right: 5px; vertical-align: top; } #lctzyhtwna .gt_row_group_first td { border-top-width: 2px; } #lctzyhtwna .gt_row_group_first th { border-top-width: 2px; } #lctzyhtwna .gt_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #lctzyhtwna .gt_first_summary_row { border-top-style: solid; border-top-color: #D3D3D3; } #lctzyhtwna .gt_first_summary_row.thick { border-top-width: 2px; } #lctzyhtwna .gt_last_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #lctzyhtwna .gt_grand_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #lctzyhtwna .gt_first_grand_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-top-style: double; border-top-width: 6px; border-top-color: #D3D3D3; } #lctzyhtwna .gt_last_grand_summary_row_top { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-bottom-style: double; border-bottom-width: 6px; border-bottom-color: #D3D3D3; } #lctzyhtwna .gt_striped { background-color: rgba(128, 128, 128, 0.05); } #lctzyhtwna .gt_table_body { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #lctzyhtwna .gt_footnotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #lctzyhtwna .gt_footnote { margin: 0px; font-size: 90%; padding-top: 4px; padding-bottom: 4px; padding-left: 5px; padding-right: 5px; } #lctzyhtwna .gt_sourcenotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #lctzyhtwna .gt_sourcenote { font-size: 90%; padding-top: 4px; padding-bottom: 4px; padding-left: 5px; padding-right: 5px; } #lctzyhtwna .gt_left { text-align: left; } #lctzyhtwna .gt_center { text-align: center; } #lctzyhtwna .gt_right { text-align: right; font-variant-numeric: tabular-nums; } #lctzyhtwna .gt_font_normal { font-weight: normal; } #lctzyhtwna .gt_font_bold { font-weight: bold; } #lctzyhtwna .gt_font_italic { font-style: italic; } #lctzyhtwna .gt_super { font-size: 65%; } #lctzyhtwna .gt_footnote_marks { font-size: 75%; vertical-align: 0.4em; position: initial; } #lctzyhtwna .gt_asterisk { font-size: 100%; vertical-align: 0; } #lctzyhtwna .gt_indent_1 { text-indent: 5px; } #lctzyhtwna .gt_indent_2 { text-indent: 10px; } #lctzyhtwna .gt_indent_3 { text-indent: 15px; } #lctzyhtwna .gt_indent_4 { text-indent: 20px; } #lctzyhtwna .gt_indent_5 { text-indent: 25px; } #lctzyhtwna .katex-display { display: inline-flex !important; margin-bottom: 0.75em !important; } #lctzyhtwna div.Reactable > div.rt-table > div.rt-thead > div.rt-tr.rt-tr-group-header > div.rt-th-group:after { height: 0px !important; } Characteristic N = 321 mpg 19.2 (15.4, 22.8) cyl     4 11 (34%)     6 7 (22%)     8 14 (44%) disp 196 (121, 334) hp 123 (96, 180) drat 3.70 (3.08, 3.92) wt 3.33 (2.54, 3.65) qsec 17.71 (16.89, 18.90) vs 14 (44%) am 13 (41%) gear     3 15 (47%)     4 12 (38%)     5 5 (16%) carb     1 7 (22%)     2 10 (31%)     3 3 (9.4%)     4 10 (31%)     6 1 (3.1%)     8 1 (3.1%) 1 Median (Q1, Q3); n (%) b. Have a look at the structure of the missing values in mtcars # With a table naniar::miss_var_summary(mtcars) ## # A tibble: 11 × 3 ## variable n_miss pct_miss ## &lt;chr&gt; &lt;int&gt; &lt;num&gt; ## 1 mpg 0 0 ## 2 cyl 0 0 ## 3 disp 0 0 ## 4 hp 0 0 ## 5 drat 0 0 ## 6 wt 0 0 ## 7 qsec 0 0 ## 8 vs 0 0 ## 9 am 0 0 ## 10 gear 0 0 ## 11 carb 0 0 DataExplorer::introduce(mtcars) ## rows columns discrete_columns continuous_columns ## 1 32 11 0 11 ## all_missing_columns total_missing_values complete_rows ## 1 0 0 32 ## total_observations memory_usage ## 1 352 5928 SmartEDA::ExpData(mtcars, type = 1) ## Descriptions ## 1 Sample size (nrow) ## 2 No. of variables (ncol) ## 3 No. of numeric/interger variables ## 4 No. of factor variables ## 5 No. of text variables ## 6 No. of logical variables ## 7 No. of identifier variables ## 8 No. of date variables ## 9 No. of zero variance variables (uniform) ## 10 %. of variables having complete cases ## 11 %. of variables having &gt;0% and &lt;50% missing cases ## 12 %. of variables having &gt;=50% and &lt;90% missing cases ## 13 %. of variables having &gt;=90% missing cases ## Value ## 1 32 ## 2 11 ## 3 11 ## 4 0 ## 5 0 ## 6 0 ## 7 0 ## 8 0 ## 9 0 ## 10 100% (11) ## 11 0% (0) ## 12 0% (0) ## 13 0% (0) # Graphically naniar::vis_miss(mtcars) DataExplorer::plot_missing(mtcars) c. Make an automized EDA report for mtcars! dlookr::describe(mtcars) DataExplorer::create_reports(mtcars) 8.5 Chapter 5: Data Analysis To understand linear regression, we do not have to load any complicated data. Let us assume, you are a market analyst and your customer is the production company of a series called “Breaking Thrones”. The production company wants to know how the viewers of the series judge the final episode. You conduct a survey and ask people on how satisfied they were with the season final and some social demographics. Here is your codebook: Variable Description id The id of the respondent satisfaction The answer to the question “How satisfied were you with the final episode of Breaking Throne?”, where 0 is completely dissatisfied and 10 completely satisfied age The age of the respondent female The Gender of the respondent, where 0 = Male, 1 = Female Let us generate the data: #setting seed for reproduciability set.seed(123) #Generating the data final_BT &lt;- data.frame( id = c(1:10), satisfaction = round(rnorm(10, mean = 6, sd = 2.5)), age = round(rnorm(10, mean = 25, sd = 5)), female = rbinom(10, 1, 0.5) ) #Print the Data Frame print(final_BT) ## id satisfaction age female ## 1 1 5 31 0 ## 2 2 5 27 0 ## 3 3 10 27 0 ## 4 4 6 26 0 ## 5 5 6 22 0 ## 6 6 10 34 0 ## 7 7 7 27 0 ## 8 8 3 15 0 ## 9 9 4 29 0 ## 10 10 5 23 1 8.5.1 Exercise 1: Linear Regression with two variables You want to know if age has an impact on the satisfaction with the last episode. You want to conduct a linear regression. a. Calculate \\(\\beta_0\\) and \\(\\beta_1\\) by hand #Calculating Covariance cov &lt;- sum((final_BT$age - mean(final_BT$age)) * (final_BT$satisfaction - mean(final_BT$satisfaction))) #Calculating Variance var &lt;- (sum((final_BT$age - mean(final_BT$age))^2)) #Calculating beta_1 beta_1 &lt;- cov/var #printing beta_1 print(beta_1) ## [1] 0.2466586 #calculating beta 1 beta_0 &lt;- mean(final_BT$satisfaction) - (beta_1 * mean(final_BT$age)) #Print both print(beta_0) ## [1] -0.3377886 print(beta_1) ## [1] 0.2466586 b. Calculate \\(\\beta_0\\) and \\(\\beta_1\\) automatically with R #Calculating it automatically summary(m1 &lt;- lm(satisfaction ~ age, data = final_BT)) ## ## Call: ## lm(formula = satisfaction ~ age, data = final_BT) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.8153 -1.0820 -0.2053 0.8530 3.6780 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -0.3378 3.4796 -0.097 0.9251 ## age 0.2467 0.1310 1.883 0.0964 . ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 2.058 on 8 degrees of freedom ## Multiple R-squared: 0.3072, Adjusted R-squared: 0.2206 ## F-statistic: 3.547 on 1 and 8 DF, p-value: 0.0964 c. Interpret all quantities of your result: Standard Error, t-statistic, p-value, confidence intervals and the \\(R^2\\). d. Check for influential outliers #Cooks Distance can be calculated with a built-in function final_BT$cooks_distance &lt;- cooks.distance(m1) #Plotting it ggplot(final_BT, aes(x = age, y = cooks_distance)) + geom_point(colour = &quot;darkgreen&quot;, size = 3, alpha = 0.5) + labs(y = &quot;Cook&#39;s Distance&quot;, x = &quot;Independent Variable&quot;) + geom_hline(yintercept = 1, linetype = &quot;dashed&quot;) + theme_bw() 8.5.2 Exercise 2: Multivariate Regression a. Add the variable female to your regression summary(m1 &lt;- lm(satisfaction ~ age + female, data = final_BT)) ## ## Call: ## lm(formula = satisfaction ~ age + female, data = final_BT) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.8401 -1.1312 -0.0574 0.8001 3.6435 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -0.1712 3.8481 -0.044 0.966 ## age 0.2418 0.1429 1.692 0.134 ## female -0.3895 2.3662 -0.165 0.874 ## ## Residual standard error: 2.196 on 7 degrees of freedom ## Multiple R-squared: 0.3099, Adjusted R-squared: 0.1127 ## F-statistic: 1.571 on 2 and 7 DF, p-value: 0.2731 b. Interpret the Output. What has changed? What stood the same? c. Make an interaction effect between age and female and interpret it! summary(m2 &lt;- lm(satisfaction ~ age + female + age:female, data = final_BT)) ## ## Call: ## lm(formula = satisfaction ~ age + female + age:female, data = final_BT) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.8401 -1.1312 -0.0574 0.8001 3.6435 ## ## Coefficients: (1 not defined because of singularities) ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -0.1712 3.8481 -0.044 0.966 ## age 0.2418 0.1429 1.692 0.134 ## female -0.3895 2.3662 -0.165 0.874 ## age:female NA NA NA NA ## ## Residual standard error: 2.196 on 7 degrees of freedom ## Multiple R-squared: 0.3099, Adjusted R-squared: 0.1127 ## F-statistic: 1.571 on 2 and 7 DF, p-value: 0.2731 d. Plot the interaction and make the plot nice plot_model(m2, type = &quot;int&quot;) + scale_x_continuous(breaks = seq(0,60, 1)) + labs(title = &quot;Relationship of Age and Gender on Satisfaction with BT.&quot;, x = &quot;Age&quot;, y = &quot;Satisfaction&quot;) + scale_color_manual( values = c(&quot;red&quot;, &quot;blue&quot;), labels = c(&quot;Female&quot;, &quot;Male&quot;) ) + theme_sjplot() + theme(legend.position = &quot;bottom&quot;, legend.title = element_blank()) ## Warning in predict.lm(model, newdata = data_grid, type = ## &quot;response&quot;, se.fit = se, : prediction from rank-deficient ## fit; attr(*, &quot;non-estim&quot;) has doubtful cases ## Scale for colour is already present. ## Adding another scale for colour, which will replace the ## existing scale. 8.6 Chapter 6: Loops and Functions 8.6.1 Exercise 1: Writing a loop Write a for loop that prints the square of each number from 1 to 10 #Assigning an object for a better workflow number &lt;- 10 8.6.2 Exercise 2: Writing a function Write a function that takes the input x and squares it: #Defining a function for squaring sq &lt;- function (x) { x_squared &lt;- x^2 return(x_squared) } #Defining a vector containing a vector from 1 to 10 numbers &lt;- c(1:10) #Applying the number sq(numbers) 8.6.3 Exercise 3: The midnight Formula This is the midnight formula separated in two equations: \\(x_{1,2} = \\frac{-b \\pm \\sqrt{b^2 - 4ac}}{2a}\\) Make one function for the midnight formula, so the output are \\(x_1\\) a d \\(x_2\\). Test it with a = 2, b = -6, c = -8 Hint: You need two split up the formula into two equations with two outputs. mnf &lt;- function(a, b, c) { x_1 &lt;- (-b + sqrt(b^2 - 4*a*c))/(2*a) print(x_1) x_2 &lt;- (-b - sqrt(b^2 - 4*a*c))/(2*a) print(x_2) } #Test it with these numbers (other numbers might throw an error if the number under the square root is negative) mnf(2, 20, 8) "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
